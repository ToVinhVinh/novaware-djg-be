import streamlit as st
import pandas as pd
import numpy as np
import pickle
import os
import sys
import plotly.graph_objects as go
import plotly.express as px
from typing import Dict, List
import time
import re

current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

apps_utils_path = os.path.join(current_dir, 'apps', 'utils')
if apps_utils_path not in sys.path:
    sys.path.insert(0, apps_utils_path)

_train_import_error = None
try:
    import train_recommendation
except ImportError as e:
    train_recommendation = None
    _train_import_error = str(e)

_export_import_error = None
try:
    from apps.utils.export_data import export_all_data, ensure_export_directory
except ImportError as e:
    export_all_data = None
    ensure_export_directory = None
    _export_import_error = str(e)

st.set_page_config(
    page_title="Fashion Recommendation System",
    page_icon="üëî",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color:
        margin-bottom: 2rem;
    }
    .sub-header {
        font-size: 1.5rem;
        font-weight: bold;
        color:
        margin-top: 2rem;
        margin-bottom: 1rem;
    }
    .step-header {
        font-size: 1.2rem;
        font-weight: bold;
        color:
        margin-top: 1rem;
        background-color:
        padding: 0.5rem;
        border-radius: 5px;
    }
    .formula-box {
        background-color:
        padding: 1rem;
        border-radius: 5px;
        border-left: 5px solid
        margin: 1rem 0;
    }
</style>

    try:
        with open('recommendation_system/data/preprocessor.pkl', 'rb') as f:
            preprocessor = pickle.load(f)

        with open('recommendation_system/models/content_based_model.pkl', 'rb') as f:
            cb_model = pickle.load(f)

        with open('recommendation_system/models/gnn_model.pkl', 'rb') as f:
            gnn_model = pickle.load(f)

        with open('recommendation_system/models/hybrid_model.pkl', 'rb') as f:
            hybrid_model = pickle.load(f)

        return preprocessor, cb_model, gnn_model, hybrid_model
    except Exception as e:
        return None, None, None, None

@st.cache_data
def load_comparison_results():
    try:
        df = pd.read_csv('recommendation_system/evaluation/comparison_results.csv')
        return df
    except:
        return None

def compute_sparsity(df: pd.DataFrame) -> pd.Series:
    if df.empty:
        return pd.Series(dtype=float)
    non_null_counts = df.count()
    sparsity = 1 - (non_null_counts / len(df))
    return sparsity.sort_values(ascending=False)

def render_sparsity_chart(df: pd.DataFrame, title: str, key: str):
    sparsity = compute_sparsity(df)
    if sparsity.empty:
        st.info("Kh√¥ng ƒë·ªß d·ªØ li·ªáu ƒë·ªÉ t√≠nh ƒë·ªô th∆∞a.")
        return
    sparsity_df = sparsity.reset_index()
    sparsity_df.columns = ['Column', 'Sparsity']
    fig = px.bar(
        sparsity_df,
        x='Column',
        y='Sparsity',
        title=title,
        labels={'Column': 'C·ªôt', 'Sparsity': 'ƒê·ªô th∆∞a (t·ªâ l·ªá null)'}
    )
    st.plotly_chart(fig, use_container_width=True)

def render_distribution_chart(df: pd.DataFrame, dataset_key: str):
    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()
    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
    available_cols = categorical_cols + numeric_cols
    if not available_cols:
        st.info("Kh√¥ng c√≥ c·ªôt ph√π h·ª£p ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì t·ªâ l·ªá.")
        return
    selected_col = st.selectbox(
        "Ch·ªçn c·ªôt ƒë·ªÉ hi·ªÉn th·ªã bi·ªÉu ƒë·ªì t·ªâ l·ªá",
        available_cols,
        key=f"{dataset_key}_distribution_column"
    )
    if selected_col in categorical_cols:
        value_counts = df[selected_col].fillna("N/A").value_counts().head(10)
        fig = px.pie(
            values=value_counts.values,
            names=value_counts.index,
            title=f"T·ªâ l·ªá ph√¢n b·ªë c·ªßa '{selected_col}'"
        )
    else:
        numeric_series = df[selected_col].dropna()
        if numeric_series.empty:
            st.info("C·ªôt ƒë√£ ch·ªçn kh√¥ng c√≥ d·ªØ li·ªáu ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì.")
            return
        hist_data = pd.cut(numeric_series, bins=10).value_counts().sort_index()
        hist_df = hist_data.reset_index()
        hist_df.columns = ['Range', 'Count']
        hist_df['Range'] = hist_df['Range'].astype(str)
        fig = px.bar(
            hist_df,
            x='Range',
            y='Count',
            title=f"Ph√¢n b·ªë gi√° tr·ªã c·ªßa '{selected_col}'",
            labels={'Range': 'Kho·∫£ng gi√° tr·ªã', 'Count': 'S·ªë l∆∞·ª£ng'}
        )
    st.plotly_chart(fig, use_container_width=True)

def render_data_statistics(df: pd.DataFrame):
    if df.empty:
        st.info("Dataset tr·ªëng, kh√¥ng th·ªÉ th·ªëng k√™.")
        return
    numeric_df = df.select_dtypes(include=[np.number])
    if numeric_df.empty:
        st.info("Kh√¥ng c√≥ c·ªôt s·ªë ƒë·ªÉ th·ªëng k√™.")
        return
    stats_df = numeric_df.describe().T
    st.dataframe(stats_df, use_container_width=True)

def render_dataset_upload_section(
    dataset_key: str,
    display_name: str,
    purpose_text: str
):
    st.markdown(f"
    st.write(purpose_text)
    uploaded_file = st.file_uploader(
        f"T·∫£i l√™n {display_name}",
        type=['csv'],
        key=f"{dataset_key}_file_uploader"
    )
    if uploaded_file is None:
        st.info("Ch∆∞a c√≥ file ƒë∆∞·ª£c t·∫£i l√™n.")
        return
    try:
        df = pd.read_csv(uploaded_file)
    except Exception as exc:
        st.error(f"L·ªói khi ƒë·ªçc file CSV: {exc}")
        return
    st.success(f"ƒê√£ t·∫£i {display_name}: {len(df)} rows √ó {len(df.columns)} columns")
    col_rows, col_cols = st.columns(2)
    with col_rows:
        st.metric("S·ªë d√≤ng (rows)", len(df))
    with col_cols:
        st.metric("S·ªë c·ªôt (columns)", len(df.columns))
    st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
    st.dataframe(df.head(100), use_container_width=True)
    st.markdown("**üìâ Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (t·ªâ l·ªá gi√° tr·ªã null tr√™n m·ªói c·ªôt):**")
    render_sparsity_chart(df, f"ƒê·ªô th∆∞a - {display_name}", dataset_key)
    st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
    render_distribution_chart(df, dataset_key)
    st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu (count, mean, std, min, 25%, 50%, 75%, max):**")
    render_data_statistics(df)

def display_product_info(product_info: Dict, score: float = None):
    col1, col2 = st.columns([1, 3])

    with col1:
        if score is not None:
            st.metric("Score", f"{score:.4f}")

    with col2:
        st.markdown(f"**{product_info.get('productDisplayName', 'N/A')}**")
        st.write(f"üè∑Ô∏è **Category**: {product_info.get('masterCategory', 'N/A')} > {product_info.get('subCategory', 'N/A')} > {product_info.get('articleType', 'N/A')}")
        st.write(f"üë§ **Gender**: {product_info.get('gender', 'N/A')}")
        st.write(f"üé® **Color**: {product_info.get('baseColour', 'N/A')}")

def render_metrics_table(df, highlight_model=None):
    if df is None:
        st.warning("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y t√≠nh to√°n tr∆∞·ªõc.")
        return

    st.markdown("

    required_cols = ['model_name', 'recall@10', 'recall@20', 'ndcg@10', 'ndcg@20',
                     'precision@10', 'precision@20', 'training_time', 'avg_inference_time',
                     'coverage@10', 'diversity@10']

    display_df = df.copy()
    available_cols = [col for col in required_cols if col in display_df.columns]
    display_df = display_df[available_cols]

    column_mapping = {
        'model_name': 'Model',
        'recall@10': 'Recall@10',
        'recall@20': 'Recall@20',
        'ndcg@10': 'NDCG@10',
        'ndcg@20': 'NDCG@20',
        'precision@10': 'Precision@10',
        'precision@20': 'Precision@20',
        'training_time': 'Training Time (s)',
        'avg_inference_time': 'Inference Time (s)',
        'coverage@10': 'Coverage@10',
        'diversity@10': 'Diversity@10'
    }
    display_df = display_df.rename(columns=column_mapping)

    numeric_cols = display_df.select_dtypes(include=[np.number]).columns
    display_df[numeric_cols] = display_df[numeric_cols].round(4)

    def highlight_row(row):
        model_name = row.get('Model', '')
        if model_name == highlight_model:
            return ['background-color:
        return [''] * len(row)

    st.dataframe(display_df.style.apply(highlight_row, axis=1), use_container_width=True)

def slugify_model_name(model_name: str) -> str:
    return re.sub(r'[^a-z0-9]+', '_', model_name.lower()).strip('_')

def apply_5core_pruning(interactions_df: pd.DataFrame, min_interactions: int = 5) -> Dict:

    if interactions_df.empty:
        return {
            'pruned_interactions': pd.DataFrame(),
            'removed_users': 0,
            'removed_products': 0,
            'iterations': 0,
            'stats': []
        }

    df = interactions_df.copy()

    if 'user_id' not in df.columns or 'product_id' not in df.columns:
        raise ValueError("DataFrame ph·∫£i c√≥ columns 'user_id' v√† 'product_id'")

    original_users = df['user_id'].nunique()
    original_products = df['product_id'].nunique()
    original_interactions = len(df)

    stats = [{
        'iteration': 0,
        'users': original_users,
        'products': original_products,
        'interactions': original_interactions,
        'removed_users': 0,
        'removed_products': 0
    }]

    iteration = 0
    changed = True

    while changed:
        iteration += 1
        changed = False

        user_counts = df['user_id'].value_counts()
        users_to_keep = user_counts[user_counts >= min_interactions].index

        product_counts = df['product_id'].value_counts()
        products_to_keep = product_counts[product_counts >= min_interactions].index

        before_len = len(df)
        df = df[df['user_id'].isin(users_to_keep) & df['product_id'].isin(products_to_keep)]
        after_len = len(df)

        if before_len != after_len:
            changed = True

        removed_users = original_users - df['user_id'].nunique()
        removed_products = original_products - df['product_id'].nunique()

        stats.append({
            'iteration': iteration,
            'users': df['user_id'].nunique(),
            'products': df['product_id'].nunique(),
            'interactions': len(df),
            'removed_users': removed_users,
            'removed_products': removed_products
        })

        if iteration >= 100:
            break

    total_removed_users = original_users - df['user_id'].nunique()
    total_removed_products = original_products - df['product_id'].nunique()

    return {
        'pruned_interactions': df,
        'removed_users': total_removed_users,
        'removed_products': total_removed_products,
        'iterations': iteration,
        'stats': stats,
        'original_users': original_users,
        'original_products': original_products,
        'original_interactions': original_interactions
    }

def apply_feature_encoding(products_df: pd.DataFrame, features: List[str] = None) -> Dict:

    if products_df.empty:
        return {
            'encoded_matrix': np.array([]),
            'feature_mapping': {},
            'feature_dims': {},
            'total_dims': 0,
            'feature_names': []
        }

    if features is None:
        features = ['masterCategory', 'subCategory', 'articleType', 'baseColour', 'usage']

    available_features = [f for f in features if f in products_df.columns]

    if not available_features:
        return {
            'encoded_matrix': np.array([]),
            'feature_mapping': {},
            'feature_dims': {},
            'total_dims': 0,
            'feature_names': []
        }

    feature_mapping = {}
    feature_dims = {}
    encoded_parts = []
    feature_names = []
    start_idx = 0

    for feat in available_features:
        unique_values = sorted(products_df[feat].dropna().unique())
        n_values = len(unique_values)

        value_to_idx = {val: idx for idx, val in enumerate(unique_values)}
        feature_mapping[feat] = {
            'value_to_idx': value_to_idx,
            'idx_to_value': {idx: val for val, idx in value_to_idx.items()},
            'start_idx': start_idx,
            'end_idx': start_idx + n_values
        }

        one_hot = np.zeros((len(products_df), n_values))
        for i, val in enumerate(products_df[feat]):
            if pd.notna(val) and val in value_to_idx:
                one_hot[i, value_to_idx[val]] = 1

        encoded_parts.append(one_hot)
        feature_dims[feat] = n_values

        for val in unique_values:
            feature_names.append(f"{feat}_{val}")

        start_idx += n_values

    if encoded_parts:
        encoded_matrix = np.hstack(encoded_parts)
    else:
        encoded_matrix = np.array([])

    return {
        'encoded_matrix': encoded_matrix,
        'feature_mapping': feature_mapping,
        'feature_dims': feature_dims,
        'total_dims': encoded_matrix.shape[1] if len(encoded_matrix.shape) > 1 else 0,
        'feature_names': feature_names,
        'product_ids': products_df.index.tolist() if hasattr(products_df.index, 'tolist') else list(range(len(products_df)))
    }

def load_evaluation_log(model_name: str):
    slug = slugify_model_name(model_name)
    log_path = os.path.join('recommendation_system', 'evaluation', 'logs', f'{slug}.log')
    if os.path.exists(log_path):
        with open(log_path, 'r', encoding='utf-8') as f:
            return slug, f.read()
    return slug, None

def parse_evaluation_log(log_text: str) -> Dict:

    if not log_text:
        return {'metrics': {}, 'examples': {}, 'formulas': {}}

    metrics = {}
    examples = {}
    formulas = {}

    lines = log_text.split('\n')
    i = 0
    current_metric = None

    while i < len(lines):
        line = lines[i].strip()

        if not line or line.startswith('===') or line.startswith('[') or 'EVALUATING' in line or 'RESULTS FOR' in line:
            i += 1
            continue

        if ':' in line and not line.startswith('üìê') and not line.startswith('üßÆ'):
            parts = line.split(':', 1)
            if len(parts) == 2:
                metric_name = parts[0].strip()
                value_str = parts[1].strip()

                value_str = value_str.split()[0] if value_str.split() else value_str

                try:
                    value = float(value_str)
                    metrics[metric_name] = value
                    current_metric = metric_name
                except ValueError:
                    pass

        if 'üìê C√¥ng th·ª©c:' in line:
            formula = line.split('üìê C√¥ng th·ª©c:', 1)[1].strip()
            if current_metric:
                formulas[current_metric] = formula

        if 'V√≠ d·ª• √°p d·ª•ng:' in line:
            example = line.split('V√≠ d·ª• √°p d·ª•ng:', 1)[1].strip()
            if current_metric:
                examples[current_metric] = example

        i += 1

    return {
        'metrics': metrics,
        'examples': examples,
        'formulas': formulas
    }

def render_metrics_in_step(
    metrics_data,
    metric_keys: List[str],
    step_title: str,
    key_suffix: str,
    model_name: str = None
):

    if metrics_data is None:
        st.info("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y train & evaluate tr∆∞·ªõc.")
        return
    elif isinstance(metrics_data, pd.Series):
        if metrics_data.empty:
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y train & evaluate tr∆∞·ªõc.")
            return
    elif isinstance(metrics_data, dict):
        if not metrics_data or (isinstance(metrics_data, dict) and 'metrics' in metrics_data and not metrics_data['metrics']):
            st.info("Ch∆∞a c√≥ d·ªØ li·ªáu metrics. Vui l√≤ng ch·∫°y train & evaluate tr∆∞·ªõc.")
            return

    parsed_log = None
    if model_name:
        _, log_text = load_evaluation_log(model_name)
        if log_text:
            parsed_log = parse_evaluation_log(log_text)

    n_cols = 2
    cols = st.columns(n_cols)

    for idx, metric_key in enumerate(metric_keys):
        col_idx = idx % n_cols
        with cols[col_idx]:
            value = None
            formula = ''
            example = ''

            if isinstance(metrics_data, dict) and 'metrics' in metrics_data:
                value = metrics_data['metrics'].get(metric_key, None)
                formula = metrics_data['formulas'].get(metric_key, '')
                example = metrics_data['examples'].get(metric_key, '')
            elif isinstance(metrics_data, pd.Series):
                value = metrics_data.get(metric_key, None)
                if parsed_log:
                    formula = parsed_log['formulas'].get(metric_key, '')
                    example = parsed_log['examples'].get(metric_key, '')

            if value is not None:
                display_name = metric_key.replace('@', '@').replace('_', ' ').title()

                st.metric(display_name, f"{value:.4f}")

                with st.expander(f"Chi ti·∫øt {display_name}", expanded=False):
                    if formula:
                        st.markdown(f"**C√¥ng th·ª©c:** {formula}")

                    if example:
                        if "| Trung b√¨nh" in example:
                            parts = example.split(" | ")
                            user_examples = []
                            avg_formula = None

                            for part in parts:
                                if "Trung b√¨nh" in part:
                                    avg_formula = part
                                else:
                                    user_examples.append(part)

                            st.markdown("
                            for i, user_ex in enumerate(user_examples, 1):
                                st.markdown(f"**{i}. {user_ex}**")

                            if avg_formula:
                                st.markdown("

                                if "=" in avg_formula:
                                    formula_parts = avg_formula.split("=")
                                    if len(formula_parts) >= 2:
                                        left_side = formula_parts[0].strip()
                                        right_side = "=".join(formula_parts[1:]).strip()

                                        import re
                                        n_users_match = re.search(r'user(\d+)', right_side)
                                        n_users = n_users_match.group(1) if n_users_match else "N"

                                        metric_var = display_name.replace(" ", "_").lower()

                                        st.markdown(f"""
                                        **C√¥ng th·ª©c:**
                                        $$\\text{{Trung b√¨nh}} = \\frac{{\\sum_{{u=1}}^{{{n_users}}} {display_name}_u}}{{{n_users}}}$$

                                        **D·∫°ng m·ªü r·ªông:**
                                        $$\\text{{Trung b√¨nh}} = \\frac{{{display_name}_{{user1}} + {display_name}_{{user2}} + \\ldots + {display_name}_{{user{n_users}}}}}{{{n_users}}}$$

                                                **T√≠nh to√°n:**
                                                $$\\text{{Trung b√¨nh}} = \\frac{{{formula_example} + {display_name}_{{user{n_users}}}}}{{{n_users}}} = {value:.4f}$$

    slug, log_text = load_evaluation_log(model_name)
    with st.expander("üìú Evaluation Log (Raw)", expanded=False):
        if log_text:
            st.text_area(
                "Chi ti·∫øt log t√≠nh to√°n",
                log_text,
                height=320,
                key=f"log_text_{key_suffix}"
            )
            st.download_button(
                "‚¨áÔ∏è T·∫£i log",
                log_text,
                file_name=f"{slug}.log",
                mime="text/plain",
                key=f"log_download_{key_suffix}"
            )
        else:
            st.info("Ch∆∞a c√≥ log evaluation. H√£y ch·∫°y train & evaluate ƒë·ªÉ t·∫°o log.")

def run_training(model_type: str):
    import io
    from contextlib import redirect_stdout

    model_names = {
        "all": "T·∫•t C·∫£ Models",
        "content_based": "Content-Based Filtering",
        "gnn": "GNN (GraphSAGE)",
        "hybrid": "Hybrid (GNN + Content-Based)"
    }

    model_name = model_names.get(model_type, model_type)

    with st.status(f"ƒêang train {model_name}...", expanded=True) as status:
        st.write(f"üöÄ B·∫Øt ƒë·∫ßu training {model_name}...")
        try:
            f = io.StringIO()
            with redirect_stdout(f):
                if model_type == "all":
                    train_recommendation.train_and_evaluate()
                elif model_type == "content_based":
                    train_recommendation.train_content_based(evaluate=True)
                elif model_type == "gnn":
                    train_recommendation.train_gnn(evaluate=True)
                elif model_type == "hybrid":
                    train_recommendation.train_hybrid(evaluate=True)
                else:
                    raise ValueError(f"Unknown model type: {model_type}")

            output_log = f.getvalue()
            st.text_area("Logs", output_log, height=300)

            st.cache_resource.clear()
            st.cache_data.clear()
            preprocessor, cb_model, gnn_model, hybrid_model = load_models()
            comparison_df = load_comparison_results()

            status.update(label=f"‚úÖ Ho√†n th√†nh training {model_name}!", state="complete", expanded=False)
            st.success(f"‚úÖ ƒê√£ ho√†n th√†nh training {model_name} v√† c·∫≠p nh·∫≠t s·ªë li·ªáu!")
        except Exception as e:
            status.update(label=f"‚ùå L·ªói khi train {model_name}", state="error")
            st.error(f"L·ªói: {str(e)}")
            import traceback
            st.code(traceback.format_exc())

def main():

    st.markdown('<div class="main-header">üëî Fashion Recommendation System</div>', unsafe_allow_html=True)

    st.sidebar.title("‚öôÔ∏è Menu")

    page = st.sidebar.radio(
        "Ch·ªçn ch·ª©c nƒÉng",
        ["üìö Algorithms & Steps", "üìä Model Comparison", "üéØ Personalized Recommendations", "üëó Outfit Recommendations"]
    )

    preprocessor, cb_model, gnn_model, hybrid_model = load_models()
    comparison_df = load_comparison_results()

    if page == "üìö Algorithms & Steps":
        st.markdown("

        with st.expander("B∆∞·ªõc 0: Xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB th√†nh CSV", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** Xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB (products, users, interactions) th√†nh c√°c file CSV ƒë·ªÉ s·ª≠ d·ª•ng cho training v√† evaluation.")

            if export_all_data is None:
                st.error(f"‚ùå Kh√¥ng th·ªÉ import export_data module: {_export_import_error}")
                st.info("Vui l√≤ng ƒë·∫£m b·∫£o file apps/utils/export_data.py t·ªìn t·∫°i v√† c√≥ th·ªÉ import ƒë∆∞·ª£c.")
            else:
                col_export1, col_export2 = st.columns([2, 1])
                with col_export1:
                    st.write("**C√°c file s·∫Ω ƒë∆∞·ª£c xu·∫•t:**")
                    st.write("- `products.csv`: id, gender, masterCategory, subCategory, articleType, baseColour, season, year, usage, productDisplayName, images")
                    st.write("- `users.csv`: id, name, email, age, gender, interaction_history")
                    st.write("- `interactions.csv`: user_id, product_id, interaction_type, timestamp")
                    st.write("**V·ªã tr√≠ l∆∞u:** `apps/exports/`")

                with col_export2:
                    export_button_clicked = st.button("üì• Xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB", type="primary", use_container_width=True)

                if export_button_clicked:
                    with st.spinner("ƒêang xu·∫•t d·ªØ li·ªáu t·ª´ MongoDB..."):
                        try:
                            result = export_all_data()

                            if result['success']:
                                st.success(f"‚úÖ {result['message']}")

                                st.markdown("
                                col_res1, col_res2, col_res3 = st.columns(3)

                                with col_res1:
                                    products_result = result['results']['products']
                                    if products_result['success']:
                                        st.success(f"‚úÖ Products: {products_result['count']} records")
                                    else:
                                        st.error(f"‚ùå Products: {products_result.get('error', 'L·ªói')}")

                                with col_res2:
                                    users_result = result['results']['users']
                                    if users_result['success']:
                                        st.success(f"‚úÖ Users: {users_result['count']} records")
                                    else:
                                        st.error(f"‚ùå Users: {users_result.get('error', 'L·ªói')}")

                                with col_res3:
                                    interactions_result = result['results']['interactions']
                                    if interactions_result['success']:
                                        st.success(f"‚úÖ Interactions: {interactions_result['count']} records")
                                    else:
                                        st.error(f"‚ùå Interactions: {interactions_result.get('error', 'L·ªói')}")

                                st.markdown("---")
                                st.markdown("

                                export_dir = ensure_export_directory()

                                products_path = export_dir / 'products.csv'
                                if products_path.exists() and products_result['success']:
                                    st.markdown("
                                    try:
                                        products_df = pd.read_csv(products_path)
                                        st.success(f"‚úÖ ƒê√£ t·∫£i products.csv: {len(products_df)} rows √ó {len(products_df.columns)} columns")

                                        col_p1, col_p2 = st.columns(2)
                                        with col_p1:
                                            st.metric("S·ªë d√≤ng (rows)", len(products_df))
                                        with col_p2:
                                            st.metric("S·ªë c·ªôt (columns)", len(products_df.columns))

                                        st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
                                        st.dataframe(products_df.head(100), use_container_width=True)

                                        st.markdown("**üìâ Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (t·ªâ l·ªá gi√° tr·ªã null tr√™n m·ªói c·ªôt):**")
                                        render_sparsity_chart(products_df, "ƒê·ªô th∆∞a - Products", "products_export")

                                        st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
                                        render_distribution_chart(products_df, "products_export")

                                        st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu:**")
                                        render_data_statistics(products_df)
                                    except Exception as e:
                                        st.error(f"L·ªói khi ƒë·ªçc products.csv: {str(e)}")

                                st.markdown("---")

                                users_path = export_dir / 'users.csv'
                                if users_path.exists() and users_result['success']:
                                    st.markdown("
                                    try:
                                        users_df = pd.read_csv(users_path)
                                        st.success(f"‚úÖ ƒê√£ t·∫£i users.csv: {len(users_df)} rows √ó {len(users_df.columns)} columns")

                                        col_u1, col_u2 = st.columns(2)
                                        with col_u1:
                                            st.metric("S·ªë d√≤ng (rows)", len(users_df))
                                        with col_u2:
                                            st.metric("S·ªë c·ªôt (columns)", len(users_df.columns))

                                        st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
                                        st.dataframe(users_df.head(100), use_container_width=True)

                                        st.markdown("**üìâ Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (t·ªâ l·ªá gi√° tr·ªã null tr√™n m·ªói c·ªôt):**")
                                        render_sparsity_chart(users_df, "ƒê·ªô th∆∞a - Users", "users_export")

                                        st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
                                        render_distribution_chart(users_df, "users_export")

                                        st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu:**")
                                        render_data_statistics(users_df)
                                    except Exception as e:
                                        st.error(f"L·ªói khi ƒë·ªçc users.csv: {str(e)}")

                                st.markdown("---")

                                interactions_path = export_dir / 'interactions.csv'
                                if interactions_path.exists() and interactions_result['success']:
                                    st.markdown("
                                    try:
                                        interactions_df = pd.read_csv(interactions_path)
                                        st.success(f"‚úÖ ƒê√£ t·∫£i interactions.csv: {len(interactions_df)} rows √ó {len(interactions_df.columns)} columns")

                                        col_i1, col_i2 = st.columns(2)
                                        with col_i1:
                                            st.metric("S·ªë d√≤ng (rows)", len(interactions_df))
                                        with col_i2:
                                            st.metric("S·ªë c·ªôt (columns)", len(interactions_df.columns))

                                        st.markdown("**üëÄ Xem tr∆∞·ªõc d·ªØ li·ªáu (t·ªëi ƒëa 100 d√≤ng ƒë·∫ßu):**")
                                        st.dataframe(interactions_df.head(100), use_container_width=True)

                                        st.markdown("**üìâ Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (t·ªâ l·ªá gi√° tr·ªã null tr√™n m·ªói c·ªôt):**")
                                        render_sparsity_chart(interactions_df, "ƒê·ªô th∆∞a - Interactions", "interactions_export")

                                        st.markdown("**üìä Bi·ªÉu ƒë·ªì t·ªâ l·ªá / ph√¢n b·ªë:**")
                                        render_distribution_chart(interactions_df, "interactions_export")

                                        st.markdown("**üìà B·∫£ng th·ªëng k√™ d·ªØ li·ªáu:**")
                                        render_data_statistics(interactions_df)
                                    except Exception as e:
                                        st.error(f"L·ªói khi ƒë·ªçc interactions.csv: {str(e)}")

                                st.session_state['exported_data'] = {
                                    'products_path': str(products_path) if products_path.exists() else None,
                                    'users_path': str(users_path) if users_path.exists() else None,
                                    'interactions_path': str(interactions_path) if interactions_path.exists() else None,
                                    'export_dir': str(export_dir)
                                }

                            else:
                                st.error(f"‚ùå C√≥ l·ªói x·∫£y ra khi xu·∫•t d·ªØ li·ªáu")
                                for key, res in result['results'].items():
                                    if not res['success']:
                                        st.error(f"‚ùå {key}: {res.get('error', 'L·ªói kh√¥ng x√°c ƒë·ªãnh')}")

                        except Exception as e:
                            st.error(f"‚ùå L·ªói: {str(e)}")
                            import traceback
                            st.code(traceback.format_exc())

                export_dir = ensure_export_directory() if ensure_export_directory else None
                if export_dir:
                    st.info(f"üí° **L∆∞u √Ω:** C√°c file CSV s·∫Ω ƒë∆∞·ª£c l∆∞u t·∫°i: `{export_dir}`")

        st.markdown("---")

        with st.expander("B∆∞·ªõc 1.1: L√†m s·∫°ch v√† L·ªçc D·ªØ li·ªáu (Pruning & Sparsity Handling)", expanded=True):
            st.write("**N·ªôi dung th·ª±c hi·ªán:** √Åp d·ª•ng k·ªπ thu·∫≠t 5-Core Pruning ƒë·ªÉ lo·∫°i b·ªè ƒë·ªá quy c√°c ng∆∞·ªùi d√πng v√† s·∫£n ph·∫©m c√≥ d∆∞·ªõi 5 t∆∞∆°ng t√°c nh·∫±m gi·∫£m ƒë·ªô th∆∞a th·ªõt c·ªßa d·ªØ li·ªáu.")
            st.write("**D·ªØ li·ªáu s·ª≠ d·ª•ng:** `interactions.csv`")

            st.markdown("""
            **Thu·∫≠t to√°n 5-Core Pruning:**

            1. **Kh·ªüi t·∫°o:** ƒê·∫øm s·ªë l∆∞·ª£ng t∆∞∆°ng t√°c cho m·ªói user v√† m·ªói product
            2. **L·∫∑p ƒë·ªá quy:**
               - Lo·∫°i b·ªè t·∫•t c·∫£ users c√≥ < 5 interactions
               - Lo·∫°i b·ªè t·∫•t c·∫£ products c√≥ < 5 interactions
               - C·∫≠p nh·∫≠t l·∫°i s·ªë l∆∞·ª£ng interactions c·ªßa c√°c users/products c√≤n l·∫°i
               - L·∫∑p l·∫°i cho ƒë·∫øn khi kh√¥ng c√≤n user/product n√†o b·ªã lo·∫°i b·ªè
            3. **K·∫øt qu·∫£:** Ma tr·∫≠n t∆∞∆°ng t√°c $R$ ƒë∆∞·ª£c l√†m s·∫°ch, ch·ªâ gi·ªØ l·∫°i c√°c users v√† products c√≥ ƒë·ªß d·ªØ li·ªáu

            **C√¥ng th·ª©c:**
            $$R_{pruned} = \\{(u, i) \\in R : |I_u| \\geq 5 \\land |U_i| \\geq 5\\}$$

            Trong ƒë√≥:
            - $R$: Ma tr·∫≠n t∆∞∆°ng t√°c g·ªëc
            - $I_u$: T·∫≠p s·∫£n ph·∫©m m√† user $u$ ƒë√£ t∆∞∆°ng t√°c
            - $U_i$: T·∫≠p users ƒë√£ t∆∞∆°ng t√°c v·ªõi s·∫£n ph·∫©m $i$
            - $R_{pruned}$: Ma tr·∫≠n sau khi pruning

                        ‚ùå **K·∫øt qu·∫£:** T·∫•t c·∫£ d·ªØ li·ªáu ƒë√£ b·ªã lo·∫°i b·ªè!

                        **Nguy√™n nh√¢n:**
                        - V·ªõi min_interactions = {min_interactions_used}, t·∫•t c·∫£ users v√†/ho·∫∑c products ƒë·ªÅu c√≥ √≠t h∆°n {min_interactions_used} interactions
                        - ƒêi·ªÅu n√†y t·∫°o ra hi·ªáu ·ª©ng cascade: khi lo·∫°i b·ªè users/products, c√°c interactions li√™n quan c≈©ng b·ªã lo·∫°i b·ªè, khi·∫øn c√°c users/products kh√°c c≈©ng kh√¥ng ƒë·ªß ƒëi·ªÅu ki·ªán

                        **Gi·∫£i ph√°p:**
                        1. Gi·∫£m min_interactions xu·ªëng (v√≠ d·ª•: 3 ho·∫∑c 2)
                        2. Thu th·∫≠p th√™m d·ªØ li·ªáu interactions
                        3. Ch·∫•p nh·∫≠n d·ªØ li·ªáu th∆∞a th·ªõt v√† kh√¥ng √°p d·ª•ng pruning

                    ‚úÖ Ma tr·∫≠n t∆∞∆°ng t√°c th∆∞a th·ªõt $R$ ƒë∆∞·ª£c l√†m s·∫°ch, gi·∫£m nhi·ªÖu (noise) do t∆∞∆°ng t√°c ng·∫´u nhi√™n ho·∫∑c kh√¥ng ƒë·ªß d·ªØ li·ªáu

                    ‚úÖ TƒÉng m·∫≠t ƒë·ªô d·ªØ li·ªáu t∆∞∆°ng t√°c cho c√°c thu·∫≠t to√°n c·ªông t√°c (GNN)

                    ‚úÖ Lo·∫°i b·ªè c√°c users v√† products c√≥ qu√° √≠t t∆∞∆°ng t√°c, gi√∫p model h·ªçc ƒë∆∞·ª£c patterns r√µ r√†ng h∆°n

            **Ph∆∞∆°ng ph√°p m√£ h√≥a:**

            **1. One-Hot Encoding:**
            - M·ªói gi√° tr·ªã ph√¢n lo·∫°i ƒë∆∞·ª£c chuy·ªÉn th√†nh m·ªôt vector nh·ªã ph√¢n
            - V√≠ d·ª•: masterCategory c√≥ 3 gi√° tr·ªã ‚Üí 3 chi·ªÅu binary vector
            - T·ªïng s·ªë chi·ªÅu = t·ªïng s·ªë gi√° tr·ªã unique c·ªßa t·∫•t c·∫£ c√°c features

            **2. Categorical Embedding (Alternative):**
            - S·ª≠ d·ª•ng embedding layer ƒë·ªÉ h·ªçc vector ƒë·∫°i di·ªán
            - K√≠ch th∆∞·ªõc nh·ªè g·ªçn h∆°n One-Hot
            - C√≥ th·ªÉ h·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá gi·ªØa c√°c categories

            **C√¥ng th·ª©c:**
            $$\\mathbf{v}_i = [\\text{OneHot}(\\text{masterCategory}_i), \\text{OneHot}(\\text{subCategory}_i), \\text{OneHot}(\\text{articleType}_i), \\text{OneHot}(\\text{baseColour}_i), \\text{OneHot}(\\text{usage}_i)]$$

            Trong ƒë√≥:
            - $\\mathbf{v}_i$: Item Profile Vector c·ªßa s·∫£n ph·∫©m $i$
            - $\\text{OneHot}(x)$: Vector one-hot encoding c·ªßa gi√° tr·ªã $x$
            - K·∫øt qu·∫£: Vector concatenation c·ªßa t·∫•t c·∫£ c√°c features

            **Ma tr·∫≠n ƒë·∫∑c tr∆∞ng:**
            $$P \\in \\mathbb{R}^{|I| \\times d_c}$$

            Trong ƒë√≥:
            - $|I|$: S·ªë l∆∞·ª£ng s·∫£n ph·∫©m
            - $d_c$: T·ªïng s·ªë chi·ªÅu ƒë·∫∑c tr∆∞ng n·ªôi dung (t·ªïng s·ªë gi√° tr·ªã unique c·ªßa t·∫•t c·∫£ features)

                    ‚úÖ Vector $\\mathbf{v}_i$ cho m·ªói s·∫£n ph·∫©m $i$ trong h·ªá th·ªëng, ƒë·∫°i di·ªán cho thu·ªôc t√≠nh n·ªôi dung c·ªßa n√≥

                    ‚úÖ Ma tr·∫≠n ƒë·∫∑c tr∆∞ng $P \\in \\mathbb{R}^{|I| \\times d_c}$ ƒë∆∞·ª£c t·∫°o th√†nh

                    ‚úÖ C√°c vector n√†y l√† ƒë·∫ßu v√†o c∆° s·ªü cho CBF (Content-Based Filtering) v√† Diversity (ILD) metric

                    ‚úÖ M·ªói s·∫£n ph·∫©m ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng vector s·ªë h·ªçc, c√≥ th·ªÉ t√≠nh to√°n similarity v√† distance

                **C√¥ng th·ª©c √°p d·ª•ng:**
                $$Text(P_i) = [Gender] + [MasterCategory] + [SubCategory] \\times 2 + [ArticleType] \\times 3 + [BaseColour] + [Usage]$$

                **Gi·∫£i th√≠ch:** C√°c features ƒë∆∞·ª£c k·∫øt h·ª£p th√†nh chu·ªói vƒÉn b·∫£n, trong ƒë√≥:
                - `ArticleType` ƒë∆∞·ª£c l·∫∑p l·∫°i **3 l·∫ßn** (tr·ªçng s·ªë cao nh·∫•t - quan tr·ªçng nh·∫•t)
                - `SubCategory` ƒë∆∞·ª£c l·∫∑p l·∫°i **2 l·∫ßn** (tr·ªçng s·ªë trung b√¨nh)
                - C√°c features kh√°c (Gender, MasterCategory, BaseColour, Usage) xu·∫•t hi·ªán **1 l·∫ßn**

                **L√Ω do:** Vi·ªác l·∫∑p l·∫°i gi√∫p TF-IDF coi tr·ªçng c√°c features quan tr·ªçng h∆°n khi t√≠nh to√°n similarity.

                **C√¥ng th·ª©c TF-IDF:**
                $$TF(t, d) = \\frac{count(t, d)}{len(d)}, \\quad IDF(t) = \\log(\\frac{N}{df(t)}), \\quad TF\\text{-}IDF = TF \\times IDF$$

                Trong ƒë√≥:
                - $TF(t, d)$: T·∫ßn su·∫•t t·ª´ $t$ trong document $d$
                - $IDF(t)$: Ngh·ªãch ƒë·∫£o t·∫ßn su·∫•t document, ƒëo ƒë·ªô hi·∫øm c·ªßa t·ª´ $t$
                - $N$: T·ªïng s·ªë documents (s·∫£n ph·∫©m)
                - $df(t)$: S·ªë documents ch·ª©a t·ª´ $t$

                **C√¥ng th·ª©c Cosine Similarity:**
                $$Cosine(A, B) = \\frac{A \\cdot B}{||A|| \\times ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$

                Trong ƒë√≥:
                - $A, B$: Hai vector TF-IDF c·ªßa 2 s·∫£n ph·∫©m
                - $A_i, B_i$: Gi√° tr·ªã TF-IDF c·ªßa feature th·ª© $i$
                - K·∫øt qu·∫£: Gi√° tr·ªã t·ª´ 0 ƒë·∫øn 1 (1 = gi·ªëng nhau ho√†n to√†n, 0 = kh√°c bi·ªát ho√†n to√†n)

User 1 <---[weight=1.0]---> Product A
User 2 <---[weight=0.7]---> Product A
User 1 <---[weight=0.5]---> Product B

                **C√¥ng th·ª©c GraphSAGE (Mean Aggregator):**

                **B∆∞·ªõc 1 - Aggregate (T·ªïng h·ª£p th√¥ng tin t·ª´ neighbors):**
                $$h_{N(v)}^{(k)} = \\frac{1}{|N(v)|} \\sum_{u \\in N(v)} h_u^{(k-1)}$$

                **B∆∞·ªõc 2 - Update (C·∫≠p nh·∫≠t embedding):**
                $$h_v^{(k)} = \\sigma\\left(W^{(k)} \\cdot \\text{CONCAT}(h_v^{(k-1)}, h_{N(v)}^{(k)})\\right)$$

                Trong ƒë√≥:
                - $h_v^{(k)}$: Embedding c·ªßa node $v$ ·ªü layer $k$
                - $N(v)$: T·∫≠p neighbors c·ªßa node $v$
                - $W^{(k)}$: Ma tr·∫≠n tr·ªçng s·ªë ·ªü layer $k$
                - $\\sigma$: H√†m activation (ReLU)
                - $\\text{CONCAT}$: N·ªëi vector hi·ªán t·∫°i v·ªõi vector t·ªïng h·ª£p t·ª´ neighbors

                **C√¥ng th·ª©c BPR Loss (Bayesian Personalized Ranking):**
                $$L = -\\frac{1}{|D|} \\sum_{(u,i,j) \\in D} w_{ui} \\cdot \\ln \\sigma(\\hat{x}_{ui} - \\hat{x}_{uj})$$

                Trong ƒë√≥:
                - $D$: T·∫≠p c√°c triplets $(u, i, j)$
                - $u$: User
                - $i$: Item d∆∞∆°ng (user ƒë√£ t∆∞∆°ng t√°c)
                - $j$: Item √¢m (user ch∆∞a t∆∞∆°ng t√°c, negative sample)
                - $w_{ui}$: Tr·ªçng s·ªë c·ªßa interaction (1.0 cho purchase, 0.7 cho cart, 0.5 cho like)
                - $\\hat{x}_{ui} = h_u \\cdot h_i$: ƒêi·ªÉm d·ª± ƒëo√°n (dot product c·ªßa embeddings)
                - $\\sigma$: Sigmoid function

                **√ù nghƒ©a:** Loss c√†ng nh·ªè nghƒ©a l√† model c√†ng ph√¢n bi·ªát t·ªët gi·ªØa items user th√≠ch v√† kh√¥ng th√≠ch. Weighted loss gi√∫p model coi tr·ªçng c√°c t∆∞∆°ng t√°c m·∫°nh h∆°n (purchase > cart > like).

                **C√¥ng th·ª©c t√≠nh ƒëi·ªÉm:**
                $$\\hat{x}_{ui} = h_u \\cdot h_i$$

                Trong ƒë√≥:
                - $h_u$: User embedding (t·ª´ node_embeddings)
                - $h_i$: Product embedding (t·ª´ node_embeddings)
                - $\\hat{x}_{ui}$: ƒêi·ªÉm d·ª± ƒëo√°n user $u$ s·∫Ω th√≠ch product $i$

                **Quy tr√¨nh:**
                1. V·ªõi m·ªói user trong test-set, t√≠nh ƒëi·ªÉm v·ªõi t·∫•t c·∫£ products
                2. S·∫Øp x·∫øp products theo ƒëi·ªÉm gi·∫£m d·∫ßn
                3. L·∫•y Top-K products l√†m recommendations
                4. So s√°nh v·ªõi ground truth (products user th·ª±c t·∫ø ƒë√£ t∆∞∆°ng t√°c)
                5. T√≠nh c√°c metrics: Recall, Precision, NDCG, Coverage, Diversity

                    $$Recall@K = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|R_u \\cap T_u|}{|T_u|}$$

                    $$Precision@K = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|R_u \\cap T_u|}{K}$$

                    $$NDCG@K = \\frac{DCG@K}{IDCG@K}, \\quad DCG@K = \\sum_{i=1}^{K} \\frac{rel_i}{\\log_2(i+1)}$$

                    $$Coverage@K = \\frac{|\\bigcup_{u \\in U} R_u|}{|P|}$$

                    $$Diversity@K = \\frac{1}{|U|} \\sum_{u \\in U} \\frac{|\\text{unique categories in } R_u|}{K}$$

                **C√¥ng th·ª©c Min-Max Scaling:**
                $$Score_{norm} = \\frac{Score - \\min(Scores)}{\\max(Scores) - \\min(Scores)}$$

                Trong ƒë√≥:
                - $Score$: ƒêi·ªÉm s·ªë g·ªëc (t·ª´ GNN ho·∫∑c CB)
                - $\\min(Scores)$: ƒêi·ªÉm s·ªë th·∫•p nh·∫•t trong t·∫≠p
                - $\\max(Scores)$: ƒêi·ªÉm s·ªë cao nh·∫•t trong t·∫≠p
                - $Score_{norm}$: ƒêi·ªÉm s·ªë sau khi chu·∫©n h√≥a (0-1)

                **L√Ω do:** GNN v√† CB c√≥ thang ƒëi·ªÉm kh√°c nhau, c·∫ßn chu·∫©n h√≥a ƒë·ªÉ k·∫øt h·ª£p c√¥ng b·∫±ng.

                **C√¥ng th·ª©c Late Fusion:**
                $$Score_{final} = \\alpha \\times Score_{GNN\\_norm} + (1 - \\alpha) \\times Score_{CB\\_norm}$$

                Trong ƒë√≥:
                - $\\alpha$: Tr·ªçng s·ªë cho GNN (m·∫∑c ƒë·ªãnh 0.5 = c√¢n b·∫±ng khi kh·ªüi t·∫°o model)
                - $Score_{GNN\\_norm}$: ƒêi·ªÉm s·ªë ƒë√£ chu·∫©n h√≥a t·ª´ GNN (Min-Max scaling v·ªÅ [0, 1])
                - $Score_{CB\\_norm}$: ƒêi·ªÉm s·ªë ƒë√£ chu·∫©n h√≥a t·ª´ Content-Based (Min-Max scaling v·ªÅ [0, 1])
                - $Score_{final}$: ƒêi·ªÉm s·ªë cu·ªëi c√πng ƒë·ªÉ ranking

                **L∆∞u √Ω quan tr·ªçng:**
                - Khi kh·ªüi t·∫°o, Hybrid model c√≥ th·ªÉ d√πng $\\alpha = 0.5$ (c√¢n b·∫±ng)
                - **Trong th·ª±c t·∫ø khi recommend**, model s·ª≠ d·ª•ng **dynamic weight** (GNN=0.8, CB=0.2) ƒë·ªÉ ∆∞u ti√™n GNN cao h∆°n v√¨ GNN th∆∞·ªùng cho k·∫øt qu·∫£ t·ªët h∆°n Content-Based
                - C√¥ng th·ª©c th·ª±c t·∫ø: $Score_{final} = 0.8 \\times Score_{GNN\\_norm} + 0.2 \\times Score_{CB\\_norm}$

                **T·∫°i sao Hybrid l√† t·ªëi ∆∞u nh·∫•t?**

                1. **Recall & Precision:** Hybrid ƒë·∫°t ƒë∆∞·ª£c s·ª± c√¢n b·∫±ng t·ªët h∆°n:
                   - GNN gi√∫p tƒÉng Recall (t√¨m ƒë∆∞·ª£c s·∫£n ph·∫©m ti·ªÅm nƒÉng user ch∆∞a t·ª´ng th·∫•y)
                   - CB gi√∫p tƒÉng Precision (ƒë·∫£m b·∫£o s·∫£n ph·∫©m gi·ªëng s·ªü th√≠ch c≈©)

                2. **Coverage & Diversity:**
                   - Coverage c·ªßa Hybrid th∆∞·ªùng cao h∆°n GNN thu·∫ßn t√∫y v√¨ c√≥ th·ªÉ g·ª£i √Ω c·∫£ nh·ªØng s·∫£n ph·∫©m √≠t t∆∞∆°ng t√°c (nh·ªù Content-Based)
                   - Diversity t·ªët h∆°n CB thu·∫ßn t√∫y nh·ªù GNN ƒëa d·∫°ng h√≥a recommendations

                3. **Kh·∫Øc ph·ª•c ƒëi·ªÉm y·∫øu:**
                   - GNN b·ªã y·∫øu khi User m·ªõi (Cold-start) ‚Üí CB b√π ƒë·∫Øp b·∫±ng c√°ch d·ª±a v√†o ƒë·∫∑c tr∆∞ng s·∫£n ph·∫©m
                   - CB b·ªã y·∫øu v·ªÅ ƒë·ªô ƒëa d·∫°ng v√† kh√°m ph√° ‚Üí GNN b√π ƒë·∫Øp b·∫±ng c√°ch h·ªçc t·ª´ t∆∞∆°ng t√°c c·ªßa users kh√°c

                4. **Robustness:** Hybrid √≠t b·ªã ·∫£nh h∆∞·ªüng b·ªüi d·ªØ li·ªáu thi·∫øu ho·∫∑c kh√¥ng c√¢n b·∫±ng h∆°n c√°c model ƒë∆°n l·∫ª

