"""Streamlit dashboard for Novaware product analytics and model APIs."""

from __future__ import annotations

import json
import time
from io import BytesIO
from typing import Any, Dict, Optional

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import requests
import seaborn as sns
import streamlit as st


st.set_page_config(
    page_title="Novaware Product Insights",
    page_icon="ðŸ§¥",
    layout="wide",
)

# Custom CSS for better styling
st.markdown("""
<style>
    .model-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .metric-card {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #FF4B4B;
        margin: 5px 0;
    }
    .step-header {
        background-color: #FF4B4B;
        color: white;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

st.title("ðŸ§¥ Novaware Product Insights & Model Console")
st.caption(
    "Upload CSV data, explore quick analytics, and interact with the "
    "GNN / CBF / Hybrid recommendation APIs."
)

# Quick start guide
with st.expander("ðŸ“‹ HÆ°á»›ng dáº«n sá»­ dá»¥ng (Quick Start Guide)", expanded=False):
    st.markdown("""
    ### ðŸŽ¯ Quy trÃ¬nh táº¡o tÃ i liá»‡u tá»± Ä‘á»™ng:
    
    **BÆ°á»›c 1: Kiá»ƒm tra káº¿t ná»‘i API**
    - Äáº£m báº£o Django server Ä‘ang cháº¡y táº¡i URL trong sidebar
    - Máº·c Ä‘á»‹nh: `http://127.0.0.1:8000/api/v1`
    
    **BÆ°á»›c 2: Train cÃ¡c mÃ´ hÃ¬nh (Section 2)**
    - Click nÃºt "Train GNN" â†’ Chá» training hoÃ n táº¥t
    - Click nÃºt "Train Content-based (CBF)" â†’ Chá» training hoÃ n táº¥t  
    - Click nÃºt "Train Hybrid" â†’ Chá» training hoÃ n táº¥t
    - âœ… Sau khi train, thÃ´ng sá»‘ huáº¥n luyá»‡n sáº½ tá»± Ä‘á»™ng Ä‘iá»n vÃ o tÃ i liá»‡u
    
    **BÆ°á»›c 3: Gá»i API Recommend (Section 3)**
    - Nháº­p User ID vÃ  Product ID (hoáº·c dÃ¹ng giÃ¡ trá»‹ máº·c Ä‘á»‹nh)
    - Click "Recommend GNN" â†’ Láº¥y evaluation metrics
    - Click "Recommend Content-based (CBF)" â†’ Láº¥y evaluation metrics
    - Click "Recommend Hybrid" â†’ Láº¥y evaluation metrics
    - âœ… Sau khi recommend, evaluation metrics sáº½ tá»± Ä‘á»™ng Ä‘iá»n vÃ o tÃ i liá»‡u
    
    **BÆ°á»›c 4: Xem vÃ  copy tÃ i liá»‡u (Section 4)**
    - Chá»n tab tÆ°Æ¡ng á»©ng (GNN, CBF, Hybrid, hoáº·c So sÃ¡nh)
    - Xem sá»‘ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»n
    - Copy markdown code Ä‘á»ƒ dÃ¡n vÃ o bÃ¡o cÃ¡o
    
    **ðŸ’¡ Máº¹o**: 
    - Sá»­ dá»¥ng section "ðŸ” Test API & Xem Response" Ä‘á»ƒ kiá»ƒm tra response cá»§a API
    - Táº¥t cáº£ sá»‘ liá»‡u Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»n, khÃ´ng cáº§n nháº­p thá»§ cÃ´ng
    """)


@st.cache_data(show_spinner=False)
def load_csv(file_buffer: BytesIO) -> pd.DataFrame:
    """Load CSV with error handling for malformed data."""
    try:
        # Try standard read first
        return pd.read_csv(file_buffer)
    except pd.errors.ParserError:
        # Reset buffer position
        file_buffer.seek(0)
        # Try with error handling options
        try:
            # Option 1: Skip bad lines
            return pd.read_csv(file_buffer, on_bad_lines='skip', engine='python')
        except Exception:
            # Reset buffer position again
            file_buffer.seek(0)
            # Option 2: Use more lenient parsing
            return pd.read_csv(
                file_buffer,
                on_bad_lines='skip',
                quoting=1,  # QUOTE_ALL
                escapechar='\\',
                engine='python'
            )


def describe_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Generate descriptive statistics for all columns in the dataframe."""
    numeric_stats = df.describe(
        percentiles=[0.25, 0.5, 0.75],
        include="all",
    ).transpose()
    # Select only available columns (some may not exist for non-numeric data)
    available_cols = [col for col in ["count", "mean", "std", "min", "25%", "50%", "75%", "max"] 
                      if col in numeric_stats.columns]
    numeric_stats = numeric_stats[available_cols].dropna(how="all")
    return numeric_stats


def plot_sparsity(df: pd.DataFrame) -> None:
    """Plot missing data ratio using KDE (Kernel Density Estimation)."""
    sparsity = df.isna().sum() / len(df) if len(df) else df.isna().sum()
    sparsity_values = sparsity.values
    
    # Create KDE plot
    fig, ax = plt.subplots(figsize=(10, 4))
    
    if len(sparsity_values) > 0 and sparsity_values.max() > 0:
        # KDE plot
        sns.kdeplot(data=sparsity_values, fill=True, color='#FF4B4B', ax=ax)
        ax.set_xlabel('Missing Ratio', fontsize=12)
        ax.set_ylabel('Density', fontsize=12)
        ax.set_title('Distribution of Missing Data (KDE)', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # Add statistics text
        mean_val = sparsity_values.mean()
        median_val = pd.Series(sparsity_values).median()
        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2%}')
        ax.axvline(median_val, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_val:.2%}')
        ax.legend()
    else:
        ax.text(0.5, 0.5, 'No missing data', ha='center', va='center', fontsize=14)
        ax.set_xlim(0, 1)
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close()
    
    # Show detailed table
    with st.expander("ðŸ“Š Chi tiáº¿t Missing Ratio theo cá»™t"):
        sparsity_df = (
            sparsity.rename("Missing Ratio")
            .reset_index()
            .rename(columns={"index": "Column"})
            .sort_values("Missing Ratio", ascending=False)
        )
        sparsity_df["Missing Ratio"] = sparsity_df["Missing Ratio"].apply(lambda x: f"{x:.2%}")
        st.dataframe(sparsity_df, use_container_width=True, hide_index=True)


def plot_ratio(df: pd.DataFrame, column: str) -> None:
    """Plot value distribution for a categorical column."""
    value_counts = (
        df[column]
        .fillna("Unknown")
        .astype(str)
        .value_counts(normalize=True)
        .mul(100)
    )
    
    # Create DataFrame with proper column names
    value_ratio = pd.DataFrame({
        column: value_counts.index,
        "Percentage": value_counts.values
    })
    
    st.bar_chart(
        value_ratio,
        x=column,
        y="Percentage",
        use_container_width=True,
    )


def call_api(
    base_url: str,
    endpoint: str,
    payload: Optional[Dict[str, Any]] = None,
    method: str = "post",
) -> Dict[str, Any]:
    url = f"{base_url.rstrip('/')}/{endpoint.lstrip('/')}"
    try:
        response = requests.request(method, url, json=payload, timeout=600)
        response.raise_for_status()
        return {
            "success": True,
            "data": response.json(),
        }
    except requests.RequestException as exc:
        return {
            "success": False,
            "error": str(exc),
            "response": getattr(exc, "response", None)
            and getattr(exc.response, "text", None),
        }
    except json.JSONDecodeError:
        return {"success": True, "data": {"message": "Completed", "raw": response.text}}


BASE_URL = st.sidebar.text_input(
    "API base URL",
    value="http://127.0.0.1:8000/api/v1",
    help="Äáº·t URL backend Django (vÃ­ dá»¥ http://localhost:8000/api/v1).",
)
st.sidebar.markdown("---")
st.sidebar.write("User_ID cá»‘ Ä‘á»‹nh: `690bf0f2d0c3753df0ecbdd6`")
st.sidebar.write("Product_ID thá»­ nghiá»‡m: `10068`")


# Store training results in session state
if "training_results" not in st.session_state:
    st.session_state.training_results = {
        "gnn": None,
        "cbf": None,
        "hybrid": None,
    }

if "recommendation_results" not in st.session_state:
    st.session_state.recommendation_results = {
        "gnn": None,
        "cbf": None,
        "hybrid": None,
    }


def extract_training_metrics(result_data: Dict[str, Any], model_type: str) -> Dict[str, Any]:
    """Extract training metrics from API response.
    
    This extracts metrics from /train API response which includes:
    - Training parameters: num_users, num_products, epochs, batch_size, etc.
    - Training time: time taken to train the model
    """
    metrics = {
        "num_users": "N/A",
        "num_products": "N/A",
        "num_interactions": "N/A",
        "num_training_samples": "N/A",
        "epochs": "N/A",
        "batch_size": "N/A",
        "embed_dim": "N/A",
        "learning_rate": "N/A",
        "test_size": 0.2,
        "training_time": "N/A",
    }
    
    if not result_data:
        return metrics
    
    # Try to extract from different possible response structures
    if isinstance(result_data, dict):
        # Training time - extract from result
        for key in ["training_time", "time"]:
            if key in result_data:
                value = result_data[key]
                # Convert to string if numeric
                if isinstance(value, (int, float)):
                    metrics["training_time"] = str(value)
                else:
                    metrics["training_time"] = value
        
        # Training info nested structure
        if "training_info" in result_data:
            info = result_data["training_info"]
            # Map API keys to metric keys
            info_key_mapping = {
                "embedding_dim": "embed_dim",
            }
            for key in ["num_users", "num_products", "num_interactions", "num_training_samples",
                       "epochs", "batch_size", "embed_dim", "embedding_dim", "learning_rate"]:
                if key in info:
                    value = info[key]
                    target_key = info_key_mapping.get(key, key)
                    metrics[target_key] = str(value) if value is not None else "N/A"
        
        # Direct keys at root level (from /train API)
        # Map API keys to metric keys
        key_mapping = {
            "embedding_dim": "embed_dim",  # API returns embedding_dim, we need embed_dim
            "training_time": "time",
        }
        
        for key in ["num_users", "num_products", "num_interactions", "num_training_samples",
                   "epochs", "batch_size", "embed_dim", "embedding_dim", 
                   "learning_rate", "training_time", "time", "test_size"]:
            if key in result_data:
                value = result_data[key]
                # Use mapping if exists, otherwise use key as-is
                target_key = key_mapping.get(key, key)
                if isinstance(value, (int, float)):
                    metrics[target_key] = str(value)
                else:
                    metrics[target_key] = value if value is not None else "N/A"
        
        # Try nested structures (e.g., metrics.evaluation, stats, etc.)
        for nested_key in ["metrics", "evaluation", "stats", "results"]:
            if nested_key in result_data and isinstance(result_data[nested_key], dict):
                nested = result_data[nested_key]
                # Extract training time if available
                for key in ["training_time", "time"]:
                    if key in nested:
                        value = nested[key]
                        if isinstance(value, (int, float)):
                            metrics["training_time"] = str(value)
                        else:
                            metrics["training_time"] = value
        
        # Try to extract from summary or message
        if "summary" in result_data:
            summary = result_data["summary"]
            if isinstance(summary, dict):
                for key in summary:
                    if key in metrics:
                        value = summary[key]
                        metrics[key] = str(value) if isinstance(value, (int, float)) else value
    
    return metrics


def extract_recommend_metrics(result_data: Dict[str, Any], model_type: str) -> Dict[str, Any]:
    """Extract evaluation metrics from /recommend API response.
    
    The /recommend API returns evaluation_metrics with:
    - Recall@10, Recall@20, NDCG@10, NDCG@20, inference_time
    """
    metrics = {
        "recall_at_10": "N/A",
        "recall_at_20": "N/A",
        "ndcg_at_10": "N/A",
        "ndcg_at_20": "N/A",
        "inference_time": "N/A",
    }
    
    if not result_data or not isinstance(result_data, dict):
        return metrics
    
    # Extract from evaluation_metrics (from /recommend API)
    if "evaluation_metrics" in result_data:
        eval_metrics = result_data["evaluation_metrics"]
        if isinstance(eval_metrics, dict):
            for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20"]:
                if key in eval_metrics:
                    value = eval_metrics[key]
                    if isinstance(value, (int, float)):
                        metrics[key] = str(value)
                    else:
                        metrics[key] = value
            
            # Inference time (in milliseconds)
            if "inference_time" in eval_metrics:
                value = eval_metrics["inference_time"]
                metrics["inference_time"] = str(value) if isinstance(value, (int, float)) else value
            elif "time" in eval_metrics:
                value = eval_metrics["time"]
                # Convert seconds to milliseconds if needed
                if isinstance(value, (int, float)):
                    if value < 1000:  # Likely in seconds, convert to ms
                        metrics["inference_time"] = str(value * 1000)
                    else:
                        metrics["inference_time"] = str(value)
                else:
                    metrics["inference_time"] = value
    
    return metrics


def auto_fill_metrics_to_session_state(slug: str, metrics: Dict[str, Any]) -> None:
    """Auto-fill extracted metrics to session state for input fields."""
    # Map of metric keys to session state keys
    field_mapping = {
        "num_users": f"{slug}_num_users",
        "num_products": f"{slug}_num_products",
        "num_interactions": f"{slug}_num_interactions",
        "num_training_samples": f"{slug}_num_samples",
        "epochs": f"{slug}_epochs",
        "batch_size": f"{slug}_batch",
        "embed_dim": f"{slug}_embed",
        "learning_rate": f"{slug}_lr",
        "test_size": f"{slug}_test_size",
        "training_time": f"{slug}_training_time",
        "recall_at_10": f"{slug}_recall_at_10",
        "recall_at_20": f"{slug}_recall_at_20",
        "ndcg_at_10": f"{slug}_ndcg_at_10",
        "ndcg_at_20": f"{slug}_ndcg_at_20",
        "inference_time": f"{slug}_inference_time",
    }
    
    # Update session state with extracted metrics
    for metric_key, state_key in field_mapping.items():
        if metric_key in metrics and metrics[metric_key] != "N/A":
            value = metrics[metric_key]
            # Convert to appropriate type
            if metric_key == "test_size":
                try:
                    st.session_state[state_key] = float(value) if value != "N/A" else 0.2
                except (ValueError, TypeError):
                    st.session_state[state_key] = 0.2
            else:
                st.session_state[state_key] = str(value)


st.header("1. Upload & Preview CSV")

# Táº¡o 2 tabs cho sáº£n pháº©m vÃ  ngÆ°á»i dÃ¹ng
tab_product, tab_user = st.tabs(["ðŸ“¦ Dá»¯ liá»‡u Sáº£n pháº©m", "ðŸ‘¤ Dá»¯ liá»‡u NgÆ°á»i dÃ¹ng"])

# Tab 1: Dá»¯ liá»‡u sáº£n pháº©m
with tab_product:
    uploaded_file = st.file_uploader("Táº£i file CSV sáº£n pháº©m", type=["csv"], key="product_csv")

    df: Optional[pd.DataFrame] = None
    if uploaded_file is not None:
        with st.spinner("Äang Ä‘á»c dá»¯ liá»‡u sáº£n pháº©m..."):
            df = load_csv(uploaded_file)
        st.success(f"ÄÃ£ táº£i {len(df):,} dÃ²ng, {len(df.columns)} cá»™t.")
        st.dataframe(df.head(100), use_container_width=True)

        st.subheader("Thá»‘ng kÃª dá»¯ liá»‡u sáº£n pháº©m")
        stats_df = describe_dataframe(df)
        st.dataframe(stats_df, use_container_width=True)

        st.subheader("Biá»ƒu Ä‘á»“ Ä‘á»™ thÆ°a (Missing Ratio)")
        plot_sparsity(df)

        st.subheader("Biá»ƒu Ä‘á»“ tá»· lá»‡ (Value Ratio)")
        ratio_col = st.selectbox(
            "Chá»n cá»™t Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ tá»· lá»‡",
            options=df.columns.tolist(),
            key="product_ratio_col",
        )
        if ratio_col:
            plot_ratio(df, ratio_col)
    else:
        st.info("Vui lÃ²ng táº£i lÃªn file CSV sáº£n pháº©m Ä‘á»ƒ báº¯t Ä‘áº§u.")

# Tab 2: Dá»¯ liá»‡u ngÆ°á»i dÃ¹ng
with tab_user:
    uploaded_user_file = st.file_uploader("Táº£i file CSV ngÆ°á»i dÃ¹ng", type=["csv"], key="user_csv")

    df_user: Optional[pd.DataFrame] = None
    if uploaded_user_file is not None:
        with st.spinner("Äang Ä‘á»c dá»¯ liá»‡u ngÆ°á»i dÃ¹ng..."):
            df_user = load_csv(uploaded_user_file)
        st.success(f"ÄÃ£ táº£i {len(df_user):,} ngÆ°á»i dÃ¹ng, {len(df_user.columns)} cá»™t.")
        st.dataframe(df_user.head(100), use_container_width=True)

        st.subheader("Thá»‘ng kÃª dá»¯ liá»‡u ngÆ°á»i dÃ¹ng")
        stats_user_df = describe_dataframe(df_user)
        st.dataframe(stats_user_df, use_container_width=True)

        # PhÃ¢n tÃ­ch Ä‘áº·c biá»‡t cho dá»¯ liá»‡u ngÆ°á»i dÃ¹ng
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("PhÃ¢n bá»‘ Giá»›i tÃ­nh")
            if "Gender" in df_user.columns:
                gender_counts = df_user["Gender"].value_counts()
                st.bar_chart(gender_counts)
                
                # Hiá»ƒn thá»‹ sá»‘ liá»‡u
                for gender, count in gender_counts.items():
                    percentage = (count / len(df_user)) * 100
                    st.metric(
                        label=f"{gender}",
                        value=f"{count:,}",
                        delta=f"{percentage:.1f}%"
                    )
            else:
                st.warning("KhÃ´ng tÃ¬m tháº¥y cá»™t 'Gender' trong dá»¯ liá»‡u.")

        with col2:
            st.subheader("PhÃ¢n bá»‘ Äá»™ tuá»•i")
            if "Age" in df_user.columns:
                # Táº¡o nhÃ³m tuá»•i
                df_user_copy = df_user.copy()
                df_user_copy["age_group"] = pd.cut(
                    df_user_copy["Age"],
                    bins=[0, 12, 18, 25, 35, 50, 100],
                    labels=["Kids (0-12)", "Teens (13-18)", "Young Adults (19-25)", 
                            "Adults (26-35)", "Middle Age (36-50)", "Senior (50+)"]
                )
                age_group_counts = df_user_copy["age_group"].value_counts().sort_index()
                st.bar_chart(age_group_counts)
                
                # Thá»‘ng kÃª Ä‘á»™ tuá»•i
                st.write(f"**Äá»™ tuá»•i trung bÃ¬nh:** {df_user['Age'].mean():.1f}")
                st.write(f"**Äá»™ tuá»•i nhá» nháº¥t:** {df_user['Age'].min()}")
                st.write(f"**Äá»™ tuá»•i lá»›n nháº¥t:** {df_user['Age'].max()}")
            else:
                st.warning("KhÃ´ng tÃ¬m tháº¥y cá»™t 'Age' trong dá»¯ liá»‡u.")

        st.subheader("Biá»ƒu Ä‘á»“ Ä‘á»™ thÆ°a (Missing Ratio)")
        plot_sparsity(df_user)

        st.subheader("Biá»ƒu Ä‘á»“ tá»· lá»‡ (Value Ratio)")
        user_ratio_col = st.selectbox(
            "Chá»n cá»™t Ä‘á»ƒ váº½ biá»ƒu Ä‘á»“ tá»· lá»‡",
            options=df_user.columns.tolist(),
            key="user_ratio_col",
        )
        if user_ratio_col:
            plot_ratio(df_user, user_ratio_col)
    else:
        st.info("Vui lÃ²ng táº£i lÃªn file CSV ngÆ°á»i dÃ¹ng Ä‘á»ƒ phÃ¢n tÃ­ch.")


st.header("2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh")
models = {
    "GNN": "gnn",
    "Content-based (CBF)": "cbf",
    "Hybrid": "hybrid",
}

def poll_task_status(
    base_url: str, 
    endpoint: str, 
    task_id: str, 
    max_wait_time: int = 600,
    status_placeholder=None,
    progress_bar=None
) -> Dict[str, Any]:
    """Poll task status until completion or timeout."""
    start_time = time.time()
    poll_interval = 2  # Poll every 2 seconds
    last_progress = 0
    
    while time.time() - start_time < max_wait_time:
        result = call_api(base_url, endpoint, payload={"task_id": task_id}, method="post")
        
        if not result["success"]:
            return result
        
        data = result["data"]
        status = data.get("status", "unknown")
        
        if status == "success":
            # Success! Return the result with all metrics
            return result
        elif status == "failure":
            return {
                "success": False,
                "error": data.get("error", "Training failed"),
                "data": data,
            }
        elif status in ["pending", "running"]:
            # Update progress if available
            current_progress = data.get("progress", last_progress)
            if current_progress > last_progress:
                last_progress = current_progress
                # Progress from 30% to 90% during polling
                if progress_bar:
                    progress_bar.progress(30 + int(current_progress * 0.6))
                if status_placeholder:
                    message = data.get("message", f"Training in progress... {current_progress}%")
                    current_step = data.get("current_step", "")
                    if current_step:
                        message += f" - {current_step}"
                    status_placeholder.info(message)
            time.sleep(poll_interval)
            continue
        else:
            # Unknown status, wait and retry
            time.sleep(poll_interval)
            continue
    
    return {
        "success": False,
        "error": f"Training timeout after {max_wait_time} seconds",
        "data": {"status": "timeout", "task_id": task_id},
    }


train_cols = st.columns(len(models))
for col, (label, slug) in zip(train_cols, models.items()):
    with col:
        if st.button(f"Train {label}", key=f"train_{slug}"):
            status_placeholder = st.empty()
            progress = st.progress(0)
            status_placeholder.info("Báº¯t Ä‘áº§u gá»i API train...")
            progress.progress(10)
            start_time = time.time()
            
            # Use sync mode to get results immediately
            with st.spinner(f"Äang huáº¥n luyá»‡n {label}..."):
                # Try sync mode first (sends sync: true in payload)
                result = call_api(BASE_URL, f"{slug}/train", payload={"sync": True}, method="post")
                
                # If async response (has task_id), poll for results
                if result["success"] and isinstance(result["data"], dict):
                    data = result["data"]
                    if "task_id" in data and data.get("status") in ["pending", "running"]:
                        task_id = data["task_id"]
                        status_placeholder.info(f"Training Ä‘ang cháº¡y (task_id: {task_id[:8]}...). Äang chá» káº¿t quáº£...")
                        progress.progress(30)
                        
                        # Poll for completion with progress updates
                        result = poll_task_status(
                            BASE_URL, 
                            f"{slug}/train", 
                            task_id, 
                            max_wait_time=600,
                            status_placeholder=status_placeholder,
                            progress_bar=progress
                        )
            
            elapsed_time = time.time() - start_time
            progress.progress(100)
            
            if result["success"]:
                status_placeholder.success(f"Train {label} hoÃ n táº¥t.")
                # Store result in session state for documentation
                result_data = result["data"]
                st.session_state.training_results[slug] = result_data
                
                # Add training time if not present
                if isinstance(result_data, dict):
                    if "training_time" not in result_data and "time" not in result_data:
                        result_data["training_time"] = f"{elapsed_time:.2f}s"
                    
                    # Auto-fill metrics to session state for input fields
                    extracted_metrics = extract_training_metrics(result_data, slug)
                    auto_fill_metrics_to_session_state(slug, extracted_metrics)
                
                st.json(result_data)
                st.success(f"âœ… Sá»‘ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»n vÃ o pháº§n tÃ i liá»‡u!")
                
                # Tá»± Ä‘á»™ng gá»i API recommend Ä‘á»ƒ láº¥y evaluation metrics
                st.info("ðŸ”„ Äang tá»± Ä‘á»™ng gá»i API recommend Ä‘á»ƒ láº¥y evaluation metrics...")
                default_user_id = "690bf0f2d0c3753df0ecbdd6"
                
                # Try to get user's interaction history to test with multiple products
                product_ids_to_test = ["10068"]  # Default
                try:
                    user_url = f"{BASE_URL.rstrip('/')}/users/{default_user_id}"
                    user_response = requests.get(user_url, timeout=10)
                    if user_response.status_code == 200:
                        user_data = user_response.json()
                        if isinstance(user_data, dict) and "data" in user_data:
                            user_info = user_data["data"].get("user", {})
                            interaction_history = user_info.get("interaction_history", [])
                            if interaction_history:
                                # Get product IDs from interaction history
                                history_products = [str(interaction.get("product_id")) for interaction in interaction_history[:5] if interaction.get("product_id")]
                                if history_products:
                                    product_ids_to_test = history_products + ["10068"]  # Add default
                                    product_ids_to_test = list(dict.fromkeys(product_ids_to_test))  # Remove duplicates
                except:
                    pass
                
                # Test with multiple products and find the best result
                best_result = None
                best_metrics = None
                best_product_id = None
                recommended_products_to_try = []  # Collect recommended products to test
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                total_tests = min(len(product_ids_to_test), 5)
                
                # First pass: Test with products from interaction history
                for idx, product_id in enumerate(product_ids_to_test[:5]):  # Test up to 5 products
                    status_text.info(f"Äang test vá»›i product_id: {product_id} ({idx+1}/{total_tests})...")
                    progress_bar.progress((idx + 1) / (total_tests * 2))  # Reserve half for recommended products
                    
                    recommend_payload = {"user_id": default_user_id, "current_product_id": product_id}
                    recommend_result = call_api(BASE_URL, f"{slug}/recommend", payload=recommend_payload)
                    
                    if recommend_result["success"]:
                        data = recommend_result["data"]
                        eval_metrics = data.get("evaluation_metrics", {})
                        
                        # Collect recommended products for second pass
                        personalized = data.get("personalized", [])
                        for rec in personalized[:3]:  # Get first 3 recommendations
                            rec_product = rec.get("product", {})
                            if isinstance(rec_product, dict):
                                rec_id = rec_product.get("id")
                            else:
                                rec_id = rec.get("id") or rec.get("product_id")
                            if rec_id and str(rec_id) not in recommended_products_to_try and str(rec_id) not in product_ids_to_test:
                                recommended_products_to_try.append(str(rec_id))
                        
                        # Check if this result is better (has non-zero/non-null metrics)
                        if eval_metrics:
                            recall_at_10 = eval_metrics.get("recall_at_10", 0)
                            recall_at_20 = eval_metrics.get("recall_at_20", 0)
                            ndcg_at_10 = eval_metrics.get("ndcg_at_10", 0)
                            ndcg_at_20 = eval_metrics.get("ndcg_at_20", 0)
                            
                            # Check if this is a valid result (at least one metric is non-zero/non-null)
                            is_valid = (
                                recall_at_10 != 0 or recall_at_20 != 0 or 
                                ndcg_at_10 != 0 or ndcg_at_20 != 0
                            )
                            
                            if is_valid:
                                # Found valid metrics, use this result
                                best_result = recommend_result
                                best_metrics = eval_metrics
                                best_product_id = product_id
                                break
                            elif best_result is None:
                                # Keep first result as fallback
                                best_result = recommend_result
                                best_metrics = eval_metrics
                                best_product_id = product_id
                
                # Second pass: Test with recommended products if no valid metrics found
                if best_metrics and not any([
                    best_metrics.get("recall_at_10", 0) != 0,
                    best_metrics.get("recall_at_20", 0) != 0,
                    best_metrics.get("ndcg_at_10", 0) != 0,
                    best_metrics.get("ndcg_at_20", 0) != 0
                ]) and recommended_products_to_try:
                    status_text.info(f"KhÃ´ng tÃ¬m tháº¥y metrics há»£p lá»‡. Äang test vá»›i {len(recommended_products_to_try[:5])} recommended products...")
                    
                    for idx, rec_product_id in enumerate(recommended_products_to_try[:5]):
                        status_text.info(f"Äang test vá»›i recommended product_id: {rec_product_id} ({idx+1}/{min(len(recommended_products_to_try), 5)})...")
                        progress_bar.progress((total_tests + idx + 1) / (total_tests * 2))
                        
                        recommend_payload = {"user_id": default_user_id, "current_product_id": rec_product_id}
                        recommend_result = call_api(BASE_URL, f"{slug}/recommend", payload=recommend_payload)
                        
                        if recommend_result["success"]:
                            data = recommend_result["data"]
                            eval_metrics = data.get("evaluation_metrics", {})
                            
                            if eval_metrics:
                                recall_at_10 = eval_metrics.get("recall_at_10", 0)
                                recall_at_20 = eval_metrics.get("recall_at_20", 0)
                                ndcg_at_10 = eval_metrics.get("ndcg_at_10", 0)
                                ndcg_at_20 = eval_metrics.get("ndcg_at_20", 0)
                                
                                is_valid = (
                                    recall_at_10 != 0 or recall_at_20 != 0 or 
                                    ndcg_at_10 != 0 or ndcg_at_20 != 0
                                )
                                
                                if is_valid:
                                    # Found valid metrics, use this result
                                    best_result = recommend_result
                                    best_metrics = eval_metrics
                                    best_product_id = rec_product_id
                                    break
                
                progress_bar.progress(1.0)
                status_text.empty()
                
                if best_result and best_result["success"]:
                    has_valid_metrics = best_metrics and any([
                        best_metrics.get("recall_at_10", 0) != 0,
                        best_metrics.get("recall_at_20", 0) != 0,
                        best_metrics.get("ndcg_at_10", 0) != 0,
                        best_metrics.get("ndcg_at_20", 0) != 0
                    ])
                    
                    if has_valid_metrics:
                        st.success(f"âœ… ÄÃ£ tÃ¬m tháº¥y evaluation metrics há»£p lá»‡ vá»›i product_id: {best_product_id}!")
                    else:
                        st.warning(f"âš ï¸ ÄÃ£ test {total_tests + min(len(recommended_products_to_try), 5)} products nhÆ°ng metrics váº«n null/0.")
                        st.info(f"ðŸ“Š Sá»­ dá»¥ng káº¿t quáº£ tá»« product_id: {best_product_id}")
                        
                        # Show debug info to help understand why
                        debug_info = best_metrics.get("_debug", {}) if best_metrics else {}
                        if debug_info:
                            with st.expander("ðŸ” Debug Info - Táº¡i sao metrics = 0?"):
                                st.json(debug_info)
                                
                                # Show diagnosis if available
                                diagnosis = best_metrics.get("_diagnosis", {}) if best_metrics else {}
                                if diagnosis:
                                    st.markdown("#### ðŸ”¬ Cháº©n Ä‘oÃ¡n tá»± Ä‘á»™ng:")
                                    issues = diagnosis.get("issues", [])
                                    if issues:
                                        for issue in issues:
                                            severity = issue.get("severity", "info")
                                            if severity == "error":
                                                st.error(f"âŒ **{issue.get('issue')}**")
                                            elif severity == "warning":
                                                st.warning(f"âš ï¸ **{issue.get('issue')}**")
                                            else:
                                                st.info(f"â„¹ï¸ **{issue.get('issue')}**")
                                            st.markdown(f"- **LÃ½ do**: {issue.get('reason')}")
                                            st.markdown(f"- **CÃ¡ch sá»­a**: {issue.get('fix')}")
                                    else:
                                        st.success("âœ… KhÃ´ng phÃ¡t hiá»‡n váº¥n Ä‘á» trong logic tÃ­nh toÃ¡n")
                                
                                # Show overlap info
                                overlap_found = debug_info.get("overlap_found", False)
                                num_rec = debug_info.get("num_recommendations", 0)
                                num_gt = debug_info.get("num_ground_truth", 0)
                                
                                st.markdown("#### ðŸ“Š TÃ³m táº¯t:")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Recommendations", num_rec)
                                with col2:
                                    st.metric("Ground Truth", num_gt)
                                with col3:
                                    st.metric("Overlap", "âœ… CÃ³" if overlap_found else "âŒ KhÃ´ng")
                                
                                if not overlap_found and num_rec > 0 and num_gt > 0:
                                    st.info("ðŸ’¡ **Giáº£i thÃ­ch**: CBF Ä‘ang recommend cÃ¡c sáº£n pháº©m khÃ¡c vá»›i interaction_history cá»§a user. ÄÃ¢y cÃ³ thá»ƒ lÃ  hÃ nh vi Ä‘Ãºng (recommend sáº£n pháº©m má»›i), nhÆ°ng Ä‘á»ƒ tÃ­nh metrics cáº§n cÃ³ overlap.")
                    
                    # Store recommendation result
                    st.session_state.recommendation_results[slug] = best_result["data"]
                    
                    # Extract evaluation metrics from recommend API and update session state
                    if isinstance(best_result["data"], dict):
                        eval_metrics = extract_recommend_metrics(best_result["data"], slug)
                        # Update session state with evaluation metrics from recommend API
                        for key, value in eval_metrics.items():
                            if value != "N/A":
                                state_key = f"{slug}_{key}"
                                st.session_state[state_key] = value
                                # Also update training_results if exists
                                if st.session_state.training_results.get(slug):
                                    if isinstance(st.session_state.training_results[slug], dict):
                                        st.session_state.training_results[slug][key] = value
                    
                    st.json(best_result["data"].get("evaluation_metrics", {}))
                else:
                    st.warning(f"âš ï¸ KhÃ´ng thá»ƒ tá»± Ä‘á»™ng gá»i API recommend: {best_result.get('error', 'Unknown error') if best_result else 'No valid results found'}")
            else:
                status_placeholder.error(f"Lá»—i train {label}.")
                st.error(result["error"])
                if result.get("data"):
                    st.json(result["data"])
                if result.get("response"):
                    st.code(result["response"])


st.header("3. Recommendation APIs")
default_user_id = "690bf0f2d0c3753df0ecbdd6"
default_product_id = "10068"

user_id = st.text_input("User ID", value=default_user_id)
product_id = st.text_input("Product ID", value=default_product_id)

recommend_cols = st.columns(len(models))
# API expects user_id and current_product_id (not userId and productId)
payload = {"user_id": user_id, "current_product_id": product_id}

for col, (label, slug) in zip(recommend_cols, models.items()):
    with col:
        if st.button(f"Recommend {label}", key=f"recommend_{slug}"):
            status_placeholder = st.empty()
            status_placeholder.info("Äang gá»i API recommend...")
            with st.spinner(f"Äá»£i káº¿t quáº£ {label}..."):
                result = call_api(BASE_URL, f"{slug}/recommend", payload=payload)
            if result["success"]:
                status_placeholder.success(f"Káº¿t quáº£ {label} sáºµn sÃ ng.")
                # Store recommendation result
                st.session_state.recommendation_results[slug] = result["data"]
                
                # Extract evaluation metrics from recommend API and update session state
                if isinstance(result["data"], dict):
                    eval_metrics = extract_recommend_metrics(result["data"], slug)
                    # Update session state with evaluation metrics from recommend API
                    for key, value in eval_metrics.items():
                        if value != "N/A":
                            state_key = f"{slug}_{key}"
                            st.session_state[state_key] = value
                            # Also update training_results if exists
                            if st.session_state.training_results.get(slug):
                                if isinstance(st.session_state.training_results[slug], dict):
                                    st.session_state.training_results[slug][key] = value
                
                st.json(result["data"])
            else:
                status_placeholder.error(f"Lá»—i recommend {label}.")
                st.error(result["error"])
                if result.get("response"):
                    st.code(result["response"])


def generate_gnn_documentation(metrics: Dict[str, Any]) -> str:
    """Generate GNN documentation markdown with metrics."""
    doc = f"""### 2.3.1. GNN (Graph Neural Network - LightGCN)

- **Quy trÃ¬nh thá»±c hiá»‡n**:
  - *Chuáº©n hÃ³a dá»¯ liá»‡u vá»›i Surprise*:  
    Sá»­ dá»¥ng `surprise.Dataset.load_from_df(...)` vÃ  `train_test_split(test_size={metrics['test_size']})` Ä‘á»ƒ chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm thá»­.  
    - Test size: **{metrics['test_size']}** (tá»· lá»‡ dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ kiá»ƒm thá»­, pháº§n cÃ²n láº¡i dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng train: **{metrics['num_users']}** (sá»‘ ngÆ°á»i dÃ¹ng trong táº­p huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng sáº£n pháº©m train: **{metrics['num_products']}** (sá»‘ sáº£n pháº©m trong táº­p huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng tÆ°Æ¡ng tÃ¡c (interactions): **{metrics['num_interactions']}** (tá»•ng sá»‘ lÆ°á»£t tÆ°Æ¡ng tÃ¡c giá»¯a ngÆ°á»i dÃ¹ng vÃ  sáº£n pháº©m)
    - Sá»‘ lÆ°á»£ng training samples (BPR): **{metrics['num_training_samples']}** (sá»‘ máº«u huáº¥n luyá»‡n sau khi táº¡o negative samples cho BPR)
  - *Pipeline 5 bÆ°á»›c*:
    1. **Huáº¥n luyá»‡n mÃ´ hÃ¬nh**: LightGCN vá»›i kiáº¿n trÃºc Graph Convolutional Network.
       - Thuáº­t toÃ¡n: LightGCN (Light Graph Convolution Network) - mÃ´ hÃ¬nh há»c biá»ƒu diá»…n ngÆ°á»i dÃ¹ng vÃ  sáº£n pháº©m dá»±a trÃªn Ä‘á»“ thá»‹ tÆ°Æ¡ng tÃ¡c
       - Framework: PyTorch + PyTorch Geometric
       - Loss function: BPR (Bayesian Personalized Ranking) - tá»‘i Æ°u hÃ³a thá»© háº¡ng sáº£n pháº©m cho tá»«ng ngÆ°á»i dÃ¹ng
       - Negative sampling: 4 negative samples per positive interaction (táº¡o 4 máº«u Ã¢m cho má»—i tÆ°Æ¡ng tÃ¡c tÃ­ch cá»±c Ä‘á»ƒ há»c phÃ¢n biá»‡t)
       - Epochs: **{metrics['epochs']}** (sá»‘ láº§n duyá»‡t toÃ n bá»™ dá»¯ liá»‡u training)
       - Batch size: **{metrics['batch_size']}** (sá»‘ lÆ°á»£ng máº«u xá»­ lÃ½ cÃ¹ng lÃºc trong má»—i bÆ°á»›c cáº­p nháº­t)
       - Embedding dimension: **{metrics['embed_dim']}** (kÃ­ch thÆ°á»›c vector Ä‘áº¡i diá»‡n cho ngÆ°á»i dÃ¹ng/sáº£n pháº©m, cÃ ng lá»›n cÃ ng biá»ƒu diá»…n chi tiáº¿t hÆ¡n)
       - Learning rate: **{metrics['learning_rate']}** (tá»‘c Ä‘á»™ há»c, Ä‘iá»u chá»‰nh Ä‘á»™ lá»›n bÆ°á»›c cáº­p nháº­t tham sá»‘)
       - Optimizer: Adam (thuáº­t toÃ¡n tá»‘i Æ°u hÃ³a tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh learning rate)
       - Model file: `models/gnn_lightgcn.pkl`
    2. **Chuáº©n bá»‹ dá»¯ liá»‡u graph**: 
       - XÃ¢y dá»±ng bipartite graph (Ä‘á»“ thá»‹ hai phÃ­a) tá»« `UserInteraction` collection, má»—i cáº¡nh ná»‘i má»™t ngÆ°á»i dÃ¹ng vá»›i má»™t sáº£n pháº©m
       - Ãp dá»¥ng trá»ng sá»‘ tÆ°Æ¡ng tÃ¡c theo `INTERACTION_WEIGHTS` Ä‘á»ƒ phÃ¢n biá»‡t má»©c Ä‘á»™ quan trá»ng:
         ```python
         INTERACTION_WEIGHTS = {{
             'view': 1.0,        # Xem sáº£n pháº©m (quan tÃ¢m tháº¥p)
             'add_to_cart': 2.0, # ThÃªm vÃ o giá» (quan tÃ¢m trung bÃ¬nh)
             'purchase': 3.0,    # Mua hÃ ng (quan tÃ¢m cao nháº¥t)
             'wishlist': 1.5,    # YÃªu thÃ­ch (quan tÃ¢m trung bÃ¬nh-tháº¥p)
             'rating': 2.5       # ÄÃ¡nh giÃ¡ (quan tÃ¢m cao)
         }}
         ```
       - Táº¡o edge index (danh sÃ¡ch cáº·p user-product) vÃ  edge weights (trá»ng sá»‘ tÆ°Æ¡ng á»©ng)
    3. **Táº¡o ma tráº­n User-Item Interaction**: 
       - Sá»­ dá»¥ng sparse matrix (ma tráº­n thÆ°a) Ä‘á»ƒ biá»ƒu diá»…n tÆ°Æ¡ng tÃ¡c user-product má»™t cÃ¡ch hiá»‡u quáº£
       - TÃ­nh toÃ¡n sparsity (Ä‘á»™ thÆ°a): `sparsity = 1 - ({metrics['num_interactions']} / ({metrics['num_users']} * {metrics['num_products']}))` - tá»· lá»‡ pháº§n trÄƒm cÃ¡c tÆ°Æ¡ng tÃ¡c khÃ´ng xáº£y ra
    4. **TÃ­nh cosine similarity** giá»¯a user embeddings vÃ  product embeddings.  
       - Sau khi training, LightGCN sinh ra:
         - User embeddings: `[{metrics['num_users']}, {metrics['embed_dim']}]` - {metrics['num_users']} vector, má»—i vector {metrics['embed_dim']} chiá»u
         - Product embeddings: `[{metrics['num_products']}, {metrics['embed_dim']}]` - {metrics['num_products']} vector, má»—i vector {metrics['embed_dim']} chiá»u
       - Recommendation score = dot product (tÃ­ch vÃ´ hÆ°á»›ng) giá»¯a user embedding vÃ  product embedding, giÃ¡ trá»‹ cÃ ng cao thÃ¬ sáº£n pháº©m cÃ ng phÃ¹ há»£p vá»›i ngÆ°á»i dÃ¹ng
    5. **TÃ­nh toÃ¡n chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡**: Recall@10, Recall@20, NDCG@10, NDCG@20, thá»i gian train, thá»i gian inference.
       - *Recall@10*: Trong 10 mÃ³n báº¡n gá»£i Ã½, cÃ³ bao nhiÃªu mÃ³n user thá»±c sá»± thÃ­ch (trong test set)? CÃ ng cao cÃ ng tá»‘t (0-1)
       - *Recall@20*: TÆ°Æ¡ng tá»± nhÆ°ng top 20. CÃ ng cao cÃ ng tá»‘t (0-1)
       - *NDCG@10*: Top 10 cá»§a báº¡n khÃ´ng chá»‰ Ä‘Ãºng mÃ  cÃ²n sáº¯p xáº¿p Ä‘Ãºng thá»© tá»± (mÃ³n user thÃ­ch nháº¥t Ä‘á»©ng cao). CÃ ng cao cÃ ng tá»‘t (0-1)
       - *NDCG@20*: TÆ°Æ¡ng tá»± top 20. CÃ ng cao cÃ ng tá»‘t (0-1)
       - *Thá»i gian train*: Máº¥t bao lÃ¢u Ä‘á»ƒ train xong 1 láº§n ({metrics.get('training_time', 'N/A')}) - cÃ ng tháº¥p cÃ ng tá»‘t
       - *Thá»i gian inference/user*: Máº¥t bao lÃ¢u Ä‘á»ƒ tráº£ vá» gá»£i Ã½ cho 1 user ({metrics.get('inference_time', 'N/A')} ms) - cÃ ng tháº¥p cÃ ng tá»‘t (ráº¥t quan trá»ng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Thá»i gian train | Thá»i gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| GNN (LightGCN) | {metrics.get('recall_at_10', 'N/A')} | {metrics.get('recall_at_20', 'N/A')} | {metrics.get('ndcg_at_10', 'N/A')} | {metrics.get('ndcg_at_20', 'N/A')} | {metrics.get('training_time', 'N/A')} | {metrics.get('inference_time', 'N/A')} ms |
"""
    return doc


def generate_cbf_documentation(metrics: Dict[str, Any]) -> str:
    """Generate Content-based Filtering documentation markdown with metrics."""
    doc = f"""### 2.3.2. Content-based Filtering

- **Quy trÃ¬nh thá»±c hiá»‡n**:
  - *Chuáº©n hÃ³a dá»¯ liá»‡u vá»›i Surprise*:  
    Sá»­ dá»¥ng `surprise.Dataset.load_from_df(...)` vÃ  `train_test_split(test_size={metrics['test_size']})` Ä‘á»ƒ chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm thá»­.  
    - Test size: **{metrics['test_size']}** (tá»· lá»‡ dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ kiá»ƒm thá»­, pháº§n cÃ²n láº¡i dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng sáº£n pháº©m train: **{metrics['num_products']}** (sá»‘ sáº£n pháº©m trong táº­p huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng test: **{metrics['num_users']}** (sá»‘ ngÆ°á»i dÃ¹ng trong táº­p kiá»ƒm thá»­)
  - *Pipeline 5 bÆ°á»›c*:
    1. **Huáº¥n luyá»‡n mÃ´ hÃ¬nh**: Sentence-BERT embedding + FAISS index.
       - Model: Sentence-BERT (SBERT) - mÃ´ hÃ¬nh chuyá»ƒn Ä‘á»•i vÄƒn báº£n thÃ nh vector sá»‘, hiá»ƒu Ä‘Æ°á»£c ngá»¯ nghÄ©a cá»§a mÃ´ táº£ sáº£n pháº©m
       - Index: FAISS (Facebook AI Similarity Search) - thÆ° viá»‡n tÃ¬m kiáº¿m tÆ°Æ¡ng tá»± nhanh, cho phÃ©p tÃ¬m sáº£n pháº©m tÆ°Æ¡ng tá»± trong thá»i gian ngáº¯n
       - Embedding dimension: **{metrics['embed_dim']}** (kÃ­ch thÆ°á»›c vector Ä‘áº¡i diá»‡n cho má»—i sáº£n pháº©m, cÃ ng lá»›n cÃ ng biá»ƒu diá»…n chi tiáº¿t hÆ¡n)
    2. **Chuáº©n bá»‹ dá»¯ liá»‡u vÄƒn báº£n**: ghÃ©p cÃ¡c thuá»™c tÃ­nh `category`, `gender`, `color`, `style_tags`, `productDisplayName` thÃ nh má»™t chuá»—i vÄƒn báº£n mÃ´ táº£ Ä‘áº§y Ä‘á»§ sáº£n pháº©m
    3. **Táº¡o ma tráº­n TF-IDF**: sá»­ dá»¥ng `TfidfVectorizer` Ä‘á»ƒ táº¡o ma tráº­n TF-IDF (Term Frequency-Inverse Document Frequency) - Ä‘Ã¡nh giÃ¡ táº§m quan trá»ng cá»§a tá»« trong mÃ´ táº£ sáº£n pháº©m
    4. **TÃ­nh cosine similarity** giá»¯a cÃ¡c sáº£n pháº©m (SBERT embeddings).  
       - Recommendation score = cosine similarity (Ä‘á»™ tÆ°Æ¡ng tá»± cosine) giá»¯a product embeddings, giÃ¡ trá»‹ tá»« 0-1, cÃ ng gáº§n 1 thÃ¬ sáº£n pháº©m cÃ ng giá»‘ng nhau vá» Ä‘áº·c Ä‘iá»ƒm
    5. **TÃ­nh toÃ¡n chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡**: Recall@10, Recall@20, NDCG@10, NDCG@20, thá»i gian train, thá»i gian inference.
       - *Recall@10*: Trong 10 mÃ³n báº¡n gá»£i Ã½, cÃ³ bao nhiÃªu mÃ³n user thá»±c sá»± thÃ­ch (trong test set)? CÃ ng cao cÃ ng tá»‘t (0-1)
       - *Recall@20*: TÆ°Æ¡ng tá»± nhÆ°ng top 20. CÃ ng cao cÃ ng tá»‘t (0-1)
       - *NDCG@10*: Top 10 cá»§a báº¡n khÃ´ng chá»‰ Ä‘Ãºng mÃ  cÃ²n sáº¯p xáº¿p Ä‘Ãºng thá»© tá»± (mÃ³n user thÃ­ch nháº¥t Ä‘á»©ng cao). CÃ ng cao cÃ ng tá»‘t (0-1)
       - *NDCG@20*: TÆ°Æ¡ng tá»± top 20. CÃ ng cao cÃ ng tá»‘t (0-1)
       - *Thá»i gian train*: Máº¥t bao lÃ¢u Ä‘á»ƒ train xong 1 láº§n ({metrics.get('training_time', 'N/A')}) - cÃ ng tháº¥p cÃ ng tá»‘t
       - *Thá»i gian inference/user*: Máº¥t bao lÃ¢u Ä‘á»ƒ tráº£ vá» gá»£i Ã½ cho 1 user ({metrics.get('inference_time', 'N/A')} ms) - cÃ ng tháº¥p cÃ ng tá»‘t (ráº¥t quan trá»ng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Thá»i gian train | Thá»i gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| Content-based Filtering | {metrics.get('recall_at_10', 'N/A')} | {metrics.get('recall_at_20', 'N/A')} | {metrics.get('ndcg_at_10', 'N/A')} | {metrics.get('ndcg_at_20', 'N/A')} | {metrics.get('training_time', 'N/A')} | {metrics.get('inference_time', 'N/A')} ms |
"""
    return doc


def generate_hybrid_documentation(metrics: Dict[str, Any], alpha: float = 0.7) -> str:
    """Generate Hybrid documentation markdown with metrics."""
    doc = f"""### 2.3.3. Hybrid GNN (LightGCN) & Content-based Filtering

- **Quy trÃ¬nh thá»±c hiá»‡n**:
  - *Chuáº©n hÃ³a dá»¯ liá»‡u vá»›i Surprise*:  
    Sá»­ dá»¥ng `surprise.Dataset.load_from_df(...)` vÃ  `train_test_split(test_size={metrics['test_size']})` Ä‘á»ƒ chia dá»¯ liá»‡u thÃ nh táº­p huáº¥n luyá»‡n vÃ  táº­p kiá»ƒm thá»­.  
    - Test size: **{metrics['test_size']}** (tá»· lá»‡ dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ kiá»ƒm thá»­, pháº§n cÃ²n láº¡i dÃ¹ng Ä‘á»ƒ huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng train: **{metrics['num_users']}** (sá»‘ ngÆ°á»i dÃ¹ng trong táº­p huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng sáº£n pháº©m train: **{metrics['num_products']}** (sá»‘ sáº£n pháº©m trong táº­p huáº¥n luyá»‡n)
    - Sá»‘ lÆ°á»£ng tÆ°Æ¡ng tÃ¡c (interactions): **{metrics['num_interactions']}** (tá»•ng sá»‘ lÆ°á»£t tÆ°Æ¡ng tÃ¡c giá»¯a ngÆ°á»i dÃ¹ng vÃ  sáº£n pháº©m)
  - *Pipeline 5 bÆ°á»›c*:
    1. **Huáº¥n luyá»‡n mÃ´ hÃ¬nh**: Káº¿t há»£p GNN (LightGCN) + CBF (Sentence-BERT + FAISS).
       - GNN component: LightGCN vá»›i embedding dimension **{metrics['embed_dim']}** - há»c tá»« hÃ nh vi tÆ°Æ¡ng tÃ¡c cá»§a ngÆ°á»i dÃ¹ng thÃ´ng qua Graph Neural Network
       - CBF component: Sentence-BERT + FAISS index - há»c tá»« Ä‘áº·c Ä‘iá»ƒm ná»™i dung sáº£n pháº©m thÃ´ng qua semantic embeddings
       - Trá»ng sá»‘ káº¿t há»£p: `alpha = {alpha}` (GNN weight = {alpha}, CBF weight = {1-alpha:.1f}) - alpha cÃ ng cao thÃ¬ cÃ ng Æ°u tiÃªn hÃ nh vi ngÆ°á»i dÃ¹ng (GNN), cÃ ng tháº¥p thÃ¬ cÃ ng Æ°u tiÃªn Ä‘áº·c Ä‘iá»ƒm sáº£n pháº©m (CBF)
    2. **Chuáº©n bá»‹ dá»¯ liá»‡u**: 
       - Káº¿t há»£p embedding tá»« GNN (LightGCN) vÃ  Content-based Filtering (Sentence-BERT + FAISS)
       - User embeddings tá»« GNN (LightGCN): `[{metrics['num_users']}, {metrics['embed_dim']}]` - {metrics['num_users']} vector ngÆ°á»i dÃ¹ng, má»—i vector {metrics['embed_dim']} chiá»u, há»c tá»« Ä‘á»“ thá»‹ tÆ°Æ¡ng tÃ¡c
       - Product embeddings tá»« CBF (Sentence-BERT): `[{metrics['num_products']}, {metrics['embed_dim']}]` - {metrics['num_products']} vector sáº£n pháº©m, má»—i vector {metrics['embed_dim']} chiá»u, há»c tá»« mÃ´ táº£ sáº£n pháº©m
    3. **TÃ­nh toÃ¡n similarity**: 
       - GNN similarity: cosine similarity giá»¯a user embedding (LightGCN) vÃ  product embedding (LightGCN) - dá»±a trÃªn hÃ nh vi ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tá»± trong Ä‘á»“ thá»‹ tÆ°Æ¡ng tÃ¡c
       - CBF similarity: cosine similarity giá»¯a product embeddings (Sentence-BERT) - dá»±a trÃªn Ä‘áº·c Ä‘iá»ƒm sáº£n pháº©m tÆ°Æ¡ng tá»± vá» ngá»¯ nghÄ©a
       - Final score = `{alpha} * GNN_score + {1-alpha:.1f} * CBF_score` - káº¿t há»£p hai nguá»“n thÃ´ng tin vá»›i trá»ng sá»‘
    4. **Káº¿t há»£p trá»ng sá»‘**: 
       - Báº£ng similarity tá»« CBF (Sentence-BERT + FAISS) Ä‘Ã¡nh giÃ¡ Ä‘á»™ tÆ°Æ¡ng tá»± ná»™i dung, cá»™ng thÃªm trá»ng sá»‘ GNN (LightGCN) Ä‘Ã¡nh giÃ¡ Ä‘á»™ tÆ°Æ¡ng tá»± hÃ nh vi trong Ä‘á»“ thá»‹
    5. **TÃ­nh toÃ¡n chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡**: Recall@10, Recall@20, NDCG@10, NDCG@20, thá»i gian train, thá»i gian inference.
       - *Recall@10*: Trong 10 mÃ³n báº¡n gá»£i Ã½, cÃ³ bao nhiÃªu mÃ³n user thá»±c sá»± thÃ­ch (trong test set)? CÃ ng cao cÃ ng tá»‘t (0-1)
       - *Recall@20*: TÆ°Æ¡ng tá»± nhÆ°ng top 20. CÃ ng cao cÃ ng tá»‘t (0-1)
       - *NDCG@10*: Top 10 cá»§a báº¡n khÃ´ng chá»‰ Ä‘Ãºng mÃ  cÃ²n sáº¯p xáº¿p Ä‘Ãºng thá»© tá»± (mÃ³n user thÃ­ch nháº¥t Ä‘á»©ng cao). CÃ ng cao cÃ ng tá»‘t (0-1)
       - *NDCG@20*: TÆ°Æ¡ng tá»± top 20. CÃ ng cao cÃ ng tá»‘t (0-1)
       - *Thá»i gian train*: Máº¥t bao lÃ¢u Ä‘á»ƒ train xong 1 láº§n ({metrics.get('training_time', 'N/A')}) - cÃ ng tháº¥p cÃ ng tá»‘t
       - *Thá»i gian inference/user*: Máº¥t bao lÃ¢u Ä‘á»ƒ tráº£ vá» gá»£i Ã½ cho 1 user ({metrics.get('inference_time', 'N/A')} ms) - cÃ ng tháº¥p cÃ ng tá»‘t (ráº¥t quan trá»ng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Thá»i gian train | Thá»i gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| Hybrid GNN+CBF | {metrics.get('recall_at_10', 'N/A')} | {metrics.get('recall_at_20', 'N/A')} | {metrics.get('ndcg_at_10', 'N/A')} | {metrics.get('ndcg_at_20', 'N/A')} | {metrics.get('training_time', 'N/A')} | {metrics.get('inference_time', 'N/A')} ms |
"""
    return doc


def generate_comparison_table(gnn_metrics: Dict[str, Any], cbf_metrics: Dict[str, Any], 
                              hybrid_metrics: Dict[str, Any]) -> str:
    """Generate comparison table for all 3 models."""
    doc = """# 3. ÄÃ¡nh giÃ¡ 3 mÃ´ hÃ¬nh

**Giáº£i thÃ­ch cÃ¡c chá»‰ sá»‘:**
- **Recall@10** (0-1): Trong 10 mÃ³n báº¡n gá»£i Ã½, cÃ³ bao nhiÃªu mÃ³n user thá»±c sá»± thÃ­ch (trong test set)? CÃ ng cao cÃ ng tá»‘t
- **Recall@20** (0-1): TÆ°Æ¡ng tá»± nhÆ°ng top 20. CÃ ng cao cÃ ng tá»‘t
- **NDCG@10** (0-1): Top 10 cá»§a báº¡n khÃ´ng chá»‰ Ä‘Ãºng mÃ  cÃ²n sáº¯p xáº¿p Ä‘Ãºng thá»© tá»± (mÃ³n user thÃ­ch nháº¥t Ä‘á»©ng cao). CÃ ng cao cÃ ng tá»‘t
- **NDCG@20** (0-1): TÆ°Æ¡ng tá»± top 20. CÃ ng cao cÃ ng tá»‘t
- **Thá»i gian train**: Máº¥t bao lÃ¢u Ä‘á»ƒ train xong 1 láº§n (thÆ°á»ng tÃ­nh báº±ng phÃºt/giá») - cÃ ng tháº¥p cÃ ng tá»‘t
- **Thá»i gian inference/user**: Máº¥t bao lÃ¢u Ä‘á»ƒ tráº£ vá» gá»£i Ã½ cho 1 user (thÆ°á»ng tÃ­nh báº±ng ms) - cÃ ng tháº¥p cÃ ng tá»‘t (ráº¥t quan trá»ng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Thá»i gian train | Thá»i gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| GNN (LightGCN) | {gnn_recall_10} | {gnn_recall_20} | {gnn_ndcg_10} | {gnn_ndcg_20} | {gnn_train_time} | {gnn_inference_time} |
| Content-based Filtering | {cbf_recall_10} | {cbf_recall_20} | {cbf_ndcg_10} | {cbf_ndcg_20} | {cbf_train_time} | {cbf_inference_time} |
| Hybrid GNN+CBF | {hybrid_recall_10} | {hybrid_recall_20} | {hybrid_ndcg_10} | {hybrid_ndcg_20} | {hybrid_train_time} | {hybrid_inference_time} |

- **PhÃ¢n tÃ­ch & lá»±a chá»n**:
  - **GNN (LightGCN)**: PhÃ¹ há»£p khi cÃ³ nhiá»u dá»¯ liá»‡u tÆ°Æ¡ng tÃ¡c ngÆ°á»i dÃ¹ng, thÆ°á»ng cho Recall@K vÃ  NDCG@K cao nháº¥t nhá» há»c tá»« hÃ nh vi ngÆ°á»i dÃ¹ng tÆ°Æ¡ng tá»± thÃ´ng qua Graph Neural Network.
  - **Content-based Filtering**: PhÃ¹ há»£p khi cáº§n xá»­ lÃ½ cold-start (ngÆ°á»i dÃ¹ng/sáº£n pháº©m má»›i) hoáº·c catalog phong phÃº, Ä‘áº£m báº£o gá»£i Ã½ há»£p lÃ½ nhá» lá»c theo Ä‘áº·c Ä‘iá»ƒm sáº£n pháº©m (age/gender/style) sá»­ dá»¥ng Sentence-BERT + FAISS.
  - **Hybrid GNN+CBF**: Lá»±a chá»n production máº·c Ä‘á»‹nh vÃ¬ káº¿t há»£p Æ°u Ä‘iá»ƒm cá»§a cáº£ hai phÆ°Æ¡ng phÃ¡p (GNN LightGCN + CBF Sentence-BERT), duy trÃ¬ á»•n Ä‘á»‹nh trong nhiá»u tÃ¬nh huá»‘ng, cÃ³ thá»ƒ tinh chá»‰nh trá»ng sá»‘ `alpha` Ä‘á»ƒ Æ°u tiÃªn hÃ nh vi ngÆ°á»i dÃ¹ng (GNN) hoáº·c Ä‘áº·c Ä‘iá»ƒm sáº£n pháº©m (CBF).
  - **Káº¿t luáº­n**: Hybrid thÆ°á»ng Ä‘áº¡t Recall@K vÃ  NDCG@K cao nháº¥t vÃ  thá»i gian inference cháº¥p nháº­n Ä‘Æ°á»£c, phÃ¹ há»£p cho mÃ´i trÆ°á»ng production.
""".format(
        gnn_recall_10=gnn_metrics.get('recall_at_10', 'N/A'),
        gnn_recall_20=gnn_metrics.get('recall_at_20', 'N/A'),
        gnn_ndcg_10=gnn_metrics.get('ndcg_at_10', 'N/A'),
        gnn_ndcg_20=gnn_metrics.get('ndcg_at_20', 'N/A'),
        gnn_train_time=gnn_metrics.get('training_time', 'N/A'),
        gnn_inference_time=f"{gnn_metrics.get('inference_time', 'N/A')} ms" if gnn_metrics.get('inference_time', 'N/A') != 'N/A' else 'N/A',
        cbf_recall_10=cbf_metrics.get('recall_at_10', 'N/A'),
        cbf_recall_20=cbf_metrics.get('recall_at_20', 'N/A'),
        cbf_ndcg_10=cbf_metrics.get('ndcg_at_10', 'N/A'),
        cbf_ndcg_20=cbf_metrics.get('ndcg_at_20', 'N/A'),
        cbf_train_time=cbf_metrics.get('training_time', 'N/A'),
        cbf_inference_time=f"{cbf_metrics.get('inference_time', 'N/A')} ms" if cbf_metrics.get('inference_time', 'N/A') != 'N/A' else 'N/A',
        hybrid_recall_10=hybrid_metrics.get('recall_at_10', 'N/A'),
        hybrid_recall_20=hybrid_metrics.get('recall_at_20', 'N/A'),
        hybrid_ndcg_10=hybrid_metrics.get('ndcg_at_10', 'N/A'),
        hybrid_ndcg_20=hybrid_metrics.get('ndcg_at_20', 'N/A'),
        hybrid_train_time=hybrid_metrics.get('training_time', 'N/A'),
        hybrid_inference_time=f"{hybrid_metrics.get('inference_time', 'N/A')} ms" if hybrid_metrics.get('inference_time', 'N/A') != 'N/A' else 'N/A',
    )
    return doc


st.header("4. TÃ i liá»‡u mÃ´ hÃ¬nh (Documentation)")

st.markdown("""
**ðŸ“Œ Nguá»“n dá»¯ liá»‡u cho tÃ i liá»‡u:**

- **Tá»« API `/train`**: ThÃ´ng sá»‘ huáº¥n luyá»‡n (num_users, num_products, epochs, batch_size, embed_dim, learning_rate, etc.)
- **Tá»« API `/recommend`**: Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (MAPE, RMSE, Precision, Recall, F1, execution_time) trong `evaluation_metrics`

**ðŸ’¡ LÆ°u Ã½**: Äá»ƒ cÃ³ Ä‘áº§y Ä‘á»§ sá»‘ liá»‡u, báº¡n cáº§n:
1. Train mÃ´ hÃ¬nh qua API `/train` â†’ Láº¥y thÃ´ng sá»‘ huáº¥n luyá»‡n
2. Gá»i API `/recommend` â†’ Láº¥y evaluation metrics
""")

# Test API section
with st.expander("ðŸ” Test API & Xem Response", expanded=False):
    st.subheader("Test API Responses")
    
    test_tabs = st.tabs(["Train API", "Recommend API"])
    
    # Tab 1: Test Train API
    with test_tabs[0]:
        st.markdown("### Test `/train` API Response")
        test_train_cols = st.columns(len(models))
        for col, (label, slug) in zip(test_train_cols, models.items()):
            with col:
                if st.button(f"Test {label} Train", key=f"test_train_{slug}"):
                    with st.spinner(f"Äang gá»i {label} /train API..."):
                        result = call_api(BASE_URL, f"{slug}/train", payload={"sync": True}, method="post")
                    
                    if result["success"]:
                        st.success(f"âœ… {label} Train API Response:")
                        st.json(result["data"])
                        
                        # Store for analysis
                        st.session_state[f"test_train_{slug}"] = result["data"]
                    else:
                        st.error(f"âŒ Lá»—i: {result.get('error', 'Unknown error')}")
                        if result.get("data"):
                            st.json(result["data"])
    
    # Tab 2: Test Recommend API
    with test_tabs[1]:
        st.markdown("### Test `/recommend` API Response")
        test_user_id = st.text_input("User ID (test)", value="690bf0f2d0c3753df0ecbdd6", key="test_user_id")
        test_product_id = st.text_input("Product ID (test)", value="10068", key="test_product_id")
        
        test_recommend_cols = st.columns(len(models))
        for col, (label, slug) in zip(test_recommend_cols, models.items()):
            with col:
                if st.button(f"Test {label} Recommend", key=f"test_recommend_{slug}"):
                    # API expects user_id and current_product_id (not userId and productId)
                    payload = {"user_id": test_user_id, "current_product_id": test_product_id}
                    with st.spinner(f"Äang gá»i {label} /recommend API..."):
                        result = call_api(BASE_URL, f"{slug}/recommend", payload=payload, method="post")
                    
                    if result["success"]:
                        st.success(f"âœ… {label} Recommend API Response:")
                        data = result["data"]
                        
                        # Show evaluation_metrics if available
                        if "evaluation_metrics" in data:
                            st.markdown("**ðŸ“Š Evaluation Metrics:**")
                            st.json(data["evaluation_metrics"])
                            st.markdown("---")
                            st.markdown("**ðŸ“¦ Full Response:**")
                        
                        st.json(data)
                        
                        # Store evaluation metrics for documentation
                        if "evaluation_metrics" in data:
                            eval_metrics = data["evaluation_metrics"]
                            # Update session state with evaluation metrics
                            for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20"]:
                                if key in eval_metrics:
                                    st.session_state[f"{slug}_{key}"] = str(eval_metrics[key])
                            if "inference_time" in eval_metrics:
                                st.session_state[f"{slug}_inference_time"] = str(eval_metrics["inference_time"])
                            elif "execution_time" in eval_metrics:
                                # Convert seconds to milliseconds
                                exec_time = eval_metrics["execution_time"]
                                if isinstance(exec_time, (int, float)):
                                    st.session_state[f"{slug}_inference_time"] = str(exec_time * 1000)
                                else:
                                    st.session_state[f"{slug}_inference_time"] = str(exec_time)
                            st.success(f"âœ… ÄÃ£ cáº­p nháº­t evaluation metrics tá»« {label} recommend API!")
                    else:
                        st.error(f"âŒ Lá»—i: {result.get('error', 'Unknown error')}")
                        if result.get("data"):
                            st.json(result["data"])

st.markdown("---")

# Create tabs for each model
doc_tabs = st.tabs(["ðŸ“Š GNN (LightGCN)", "ðŸ“ Content-based Filtering", "ðŸ”€ Hybrid GNN+CBF", "ðŸ“ˆ So sÃ¡nh 3 mÃ´ hÃ¬nh"])

# Tab 1: GNN Documentation
with doc_tabs[0]:
    st.markdown("### 2.3.1. GNN (Graph Neural Network - LightGCN)")
    
    # Get metrics from training results or session state
    gnn_metrics = extract_training_metrics(
        st.session_state.training_results.get("gnn"), 
        "gnn"
    )
    
    # Get values from session state if available (auto-filled from API)
    def get_value(key: str, default: str) -> str:
        session_key = f"gnn_{key}"
        if session_key in st.session_state:
            return str(st.session_state[session_key])
        return default
    
    def get_test_size() -> float:
        if "gnn_test_size" in st.session_state:
            return st.session_state["gnn_test_size"]
        return gnn_metrics['test_size']
    
    # Display metrics (read-only display, auto-filled from API)
    st.subheader("ThÃ´ng sá»‘ huáº¥n luyá»‡n (tá»± Ä‘á»™ng Ä‘iá»n tá»« API)")
    
    # Show status if data is available
    if st.session_state.training_results.get("gnn"):
        st.info("âœ… Sá»‘ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»n tá»« káº¿t quáº£ training API")
    else:
        st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u tá»« API. Vui lÃ²ng train mÃ´ hÃ¬nh GNN trÆ°á»›c.")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        num_users = get_value("num_users", str(gnn_metrics['num_users']))
        num_products = get_value("num_products", str(gnn_metrics['num_products']))
        st.metric("Sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng train", num_users)
        st.metric("Sá»‘ lÆ°á»£ng sáº£n pháº©m train", num_products)
    with col2:
        num_interactions = get_value("num_interactions", str(gnn_metrics['num_interactions']))
        num_training_samples = get_value("num_samples", str(gnn_metrics['num_training_samples']))
        st.metric("Sá»‘ lÆ°á»£ng tÆ°Æ¡ng tÃ¡c", num_interactions)
        st.metric("Sá»‘ lÆ°á»£ng training samples (BPR)", num_training_samples)
    with col3:
        epochs = get_value("epochs", str(gnn_metrics['epochs']))
        batch_size = get_value("batch", str(gnn_metrics['batch_size']))
        st.metric("Epochs", epochs)
        st.metric("Batch size", batch_size)
    
    col4, col5 = st.columns(2)
    with col4:
        embed_dim = get_value("embed", str(gnn_metrics['embed_dim']))
        learning_rate = get_value("lr", str(gnn_metrics['learning_rate']))
        st.metric("Embedding dimension", embed_dim)
        st.metric("Learning rate", learning_rate)
    with col5:
        test_size = get_test_size()
        st.metric("Test size", test_size)
    
    st.subheader("Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (tá»± Ä‘á»™ng Ä‘iá»n tá»« API /recommend)")
    st.caption("ðŸ’¡ **LÆ°u Ã½**: CÃ¡c chá»‰ sá»‘ nÃ y láº¥y tá»« `evaluation_metrics` trong response cá»§a API `/recommend`. Vui lÃ²ng gá»i API recommend Ä‘á»ƒ cÃ³ sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡.")
    
    # Check if we have recommendation results
    has_recommend_data = st.session_state.recommendation_results.get("gnn") is not None
    if has_recommend_data:
        st.info("âœ… ÄÃ£ cÃ³ dá»¯ liá»‡u tá»« API /recommend")
    else:
        st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u tá»« API /recommend. Vui lÃ²ng gá»i API recommend á»Ÿ section 3 Ä‘á»ƒ láº¥y evaluation metrics.")
    
    eval_col1, eval_col2, eval_col3 = st.columns(3)
    with eval_col1:
        recall_at_10 = get_value("recall_at_10", "N/A")
        recall_at_20 = get_value("recall_at_20", "N/A")
        st.metric("Recall@10", recall_at_10)
        st.metric("Recall@20", recall_at_20)
    with eval_col2:
        ndcg_at_10 = get_value("ndcg_at_10", "N/A")
        ndcg_at_20 = get_value("ndcg_at_20", "N/A")
        st.metric("NDCG@10", ndcg_at_10)
        st.metric("NDCG@20", ndcg_at_20)
    with eval_col3:
        training_time = get_value("training_time", "N/A")
        inference_time = get_value("inference_time", "N/A")
        st.metric("Thá»i gian train", training_time)
        st.metric("Thá»i gian inference/user", f"{inference_time} ms" if inference_time != "N/A" else "N/A")
    
    # Update metrics with current input values
    gnn_metrics_updated = {
        'num_users': num_users,
        'num_products': num_products,
        'num_interactions': num_interactions,
        'num_training_samples': num_training_samples,
        'epochs': epochs,
        'batch_size': batch_size,
        'embed_dim': embed_dim,
        'learning_rate': learning_rate,
        'test_size': test_size,
        'recall_at_10': recall_at_10,
        'recall_at_20': recall_at_20,
        'ndcg_at_10': ndcg_at_10,
        'ndcg_at_20': ndcg_at_20,
        'training_time': training_time,
        'inference_time': inference_time,
    }
    
    # Generate and display documentation
    gnn_doc = generate_gnn_documentation(gnn_metrics_updated)
    
    st.markdown("---")
    st.subheader("ðŸ“„ Ná»™i dung tÃ i liá»‡u (cÃ³ thá»ƒ copy)")
    st.markdown(gnn_doc)
    
    # Copy button
    st.code(gnn_doc, language="markdown")

# Tab 2: CBF Documentation
with doc_tabs[1]:
    st.markdown("### 2.3.2. Content-based Filtering")
    
    # Get metrics from training results or session state
    cbf_metrics = extract_training_metrics(
        st.session_state.training_results.get("cbf"), 
        "cbf"
    )
    
    # Get values from session state if available (auto-filled from API)
    def get_value(key: str, default: str) -> str:
        session_key = f"cbf_{key}"
        if session_key in st.session_state:
            return str(st.session_state[session_key])
        return default
    
    def get_test_size() -> float:
        if "cbf_test_size" in st.session_state:
            return st.session_state["cbf_test_size"]
        return cbf_metrics['test_size']
    
    # Display metrics (read-only display, auto-filled from API)
    st.subheader("ThÃ´ng sá»‘ huáº¥n luyá»‡n (tá»± Ä‘á»™ng Ä‘iá»n tá»« API)")
    
    # Show status if data is available
    if st.session_state.training_results.get("cbf"):
        st.info("âœ… Sá»‘ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»n tá»« káº¿t quáº£ training API")
    else:
        st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u tá»« API. Vui lÃ²ng train mÃ´ hÃ¬nh CBF trÆ°á»›c.")
    
    col1, col2 = st.columns(2)
    with col1:
        num_products = get_value("num_products", str(cbf_metrics['num_products']))
        num_users = get_value("num_users", str(cbf_metrics['num_users']))
        st.metric("Sá»‘ lÆ°á»£ng sáº£n pháº©m train", num_products)
        st.metric("Sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng test", num_users)
    with col2:
        embed_dim = get_value("embed", str(cbf_metrics['embed_dim']))
        test_size = get_test_size()
        st.metric("Embedding dimension", embed_dim)
        st.metric("Test size", test_size)
    
    st.subheader("Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (tá»± Ä‘á»™ng Ä‘iá»n tá»« API /recommend)")
    st.caption("ðŸ’¡ **LÆ°u Ã½**: CÃ¡c chá»‰ sá»‘ nÃ y láº¥y tá»« `evaluation_metrics` trong response cá»§a API `/recommend`. Vui lÃ²ng gá»i API recommend Ä‘á»ƒ cÃ³ sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡.")
    
    # Check if we have recommendation results
    has_recommend_data = st.session_state.recommendation_results.get("cbf") is not None
    if has_recommend_data:
        st.info("âœ… ÄÃ£ cÃ³ dá»¯ liá»‡u tá»« API /recommend")
    else:
        st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u tá»« API /recommend. Vui lÃ²ng gá»i API recommend á»Ÿ section 3 Ä‘á»ƒ láº¥y evaluation metrics.")
    
    eval_col1, eval_col2, eval_col3 = st.columns(3)
    with eval_col1:
        recall_at_10 = get_value("recall_at_10", "N/A")
        recall_at_20 = get_value("recall_at_20", "N/A")
        st.metric("Recall@10", recall_at_10)
        st.metric("Recall@20", recall_at_20)
    with eval_col2:
        ndcg_at_10 = get_value("ndcg_at_10", "N/A")
        ndcg_at_20 = get_value("ndcg_at_20", "N/A")
        st.metric("NDCG@10", ndcg_at_10)
        st.metric("NDCG@20", ndcg_at_20)
    with eval_col3:
        training_time = get_value("training_time", "N/A")
        inference_time = get_value("inference_time", "N/A")
        st.metric("Thá»i gian train", training_time)
        st.metric("Thá»i gian inference/user", f"{inference_time} ms" if inference_time != "N/A" else "N/A")
    
    # Update metrics with current input values
    cbf_metrics_updated = {
        'num_products': num_products,
        'num_users': num_users,
        'embed_dim': embed_dim,
        'test_size': test_size,
        'recall_at_10': recall_at_10,
        'recall_at_20': recall_at_20,
        'ndcg_at_10': ndcg_at_10,
        'ndcg_at_20': ndcg_at_20,
        'training_time': training_time,
        'inference_time': inference_time,
    }
    
    # Generate and display documentation
    cbf_doc = generate_cbf_documentation(cbf_metrics_updated)
    
    st.markdown("---")
    st.subheader("ðŸ“„ Ná»™i dung tÃ i liá»‡u (cÃ³ thá»ƒ copy)")
    st.markdown(cbf_doc)
    
    # Copy button
    st.code(cbf_doc, language="markdown")

# Tab 3: Hybrid Documentation
with doc_tabs[2]:
    st.markdown("### 2.3.3. Hybrid GNN (LightGCN) & Content-based Filtering")
    
    # Get metrics from training results or session state
    hybrid_metrics = extract_training_metrics(
        st.session_state.training_results.get("hybrid"), 
        "hybrid"
    )
    
    # Get values from session state if available (auto-filled from API)
    def get_value(key: str, default: str) -> str:
        session_key = f"hybrid_{key}"
        if session_key in st.session_state:
            return str(st.session_state[session_key])
        return default
    
    def get_test_size() -> float:
        if "hybrid_test_size" in st.session_state:
            return st.session_state["hybrid_test_size"]
        return hybrid_metrics['test_size']
    
    # Alpha parameter (can be from API or default)
    if "hybrid_alpha" in st.session_state:
        default_alpha = st.session_state["hybrid_alpha"]
    else:
        default_alpha = 0.7
    alpha = st.slider("Trá»ng sá»‘ alpha (GNN weight)", min_value=0.0, max_value=1.0, value=default_alpha, step=0.1, key="hybrid_alpha")
    
    # Display metrics (read-only display, auto-filled from API)
    st.subheader("ThÃ´ng sá»‘ huáº¥n luyá»‡n (tá»± Ä‘á»™ng Ä‘iá»n tá»« API)")
    
    # Show status if data is available
    if st.session_state.training_results.get("hybrid"):
        st.info("âœ… Sá»‘ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c tá»± Ä‘á»™ng Ä‘iá»n tá»« káº¿t quáº£ training API")
    else:
        st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u tá»« API. Vui lÃ²ng train mÃ´ hÃ¬nh Hybrid trÆ°á»›c.")
    
    col1, col2 = st.columns(2)
    with col1:
        num_users = get_value("num_users", str(hybrid_metrics['num_users']))
        num_products = get_value("num_products", str(hybrid_metrics['num_products']))
        st.metric("Sá»‘ lÆ°á»£ng ngÆ°á»i dÃ¹ng train", num_users)
        st.metric("Sá»‘ lÆ°á»£ng sáº£n pháº©m train", num_products)
    with col2:
        num_interactions = get_value("num_interactions", str(hybrid_metrics['num_interactions']))
        embed_dim = get_value("embed", str(hybrid_metrics['embed_dim']))
        st.metric("Sá»‘ lÆ°á»£ng tÆ°Æ¡ng tÃ¡c", num_interactions)
        st.metric("Embedding dimension", embed_dim)
    
    test_size = get_test_size()
    st.metric("Test size", test_size)
    
    st.subheader("Chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (tá»± Ä‘á»™ng Ä‘iá»n tá»« API /recommend)")
    st.caption("ðŸ’¡ **LÆ°u Ã½**: CÃ¡c chá»‰ sá»‘ nÃ y láº¥y tá»« `evaluation_metrics` trong response cá»§a API `/recommend`. Vui lÃ²ng gá»i API recommend Ä‘á»ƒ cÃ³ sá»‘ liá»‡u Ä‘Ã¡nh giÃ¡.")
    
    # Check if we have recommendation results
    has_recommend_data = st.session_state.recommendation_results.get("hybrid") is not None
    if has_recommend_data:
        st.info("âœ… ÄÃ£ cÃ³ dá»¯ liá»‡u tá»« API /recommend")
    else:
        st.warning("âš ï¸ ChÆ°a cÃ³ dá»¯ liá»‡u tá»« API /recommend. Vui lÃ²ng gá»i API recommend á»Ÿ section 3 Ä‘á»ƒ láº¥y evaluation metrics.")
    
    eval_col1, eval_col2, eval_col3 = st.columns(3)
    with eval_col1:
        recall_at_10 = get_value("recall_at_10", "N/A")
        recall_at_20 = get_value("recall_at_20", "N/A")
        st.metric("Recall@10", recall_at_10)
        st.metric("Recall@20", recall_at_20)
    with eval_col2:
        ndcg_at_10 = get_value("ndcg_at_10", "N/A")
        ndcg_at_20 = get_value("ndcg_at_20", "N/A")
        st.metric("NDCG@10", ndcg_at_10)
        st.metric("NDCG@20", ndcg_at_20)
    with eval_col3:
        training_time = get_value("training_time", "N/A")
        inference_time = get_value("inference_time", "N/A")
        st.metric("Thá»i gian train", training_time)
        st.metric("Thá»i gian inference/user", f"{inference_time} ms" if inference_time != "N/A" else "N/A")
    
    # Update metrics with current input values
    hybrid_metrics_updated = {
        'num_users': num_users,
        'num_products': num_products,
        'num_interactions': num_interactions,
        'embed_dim': embed_dim,
        'test_size': test_size,
        'recall_at_10': recall_at_10,
        'recall_at_20': recall_at_20,
        'ndcg_at_10': ndcg_at_10,
        'ndcg_at_20': ndcg_at_20,
        'training_time': training_time,
        'inference_time': inference_time,
    }
    
    # Generate and display documentation
    hybrid_doc = generate_hybrid_documentation(hybrid_metrics_updated, alpha)
    
    st.markdown("---")
    st.subheader("ðŸ“„ Ná»™i dung tÃ i liá»‡u (cÃ³ thá»ƒ copy)")
    st.markdown(hybrid_doc)
    
    # Copy button
    st.code(hybrid_doc, language="markdown")

# Tab 4: Comparison
with doc_tabs[3]:
    st.markdown("### So sÃ¡nh 3 mÃ´ hÃ¬nh")
    
    st.info("ðŸ’¡ **LÆ°u Ã½**: Sá»‘ liá»‡u sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c Ä‘iá»n sau khi train cÃ¡c mÃ´ hÃ¬nh qua API. Vui lÃ²ng train cÃ¡c mÃ´ hÃ¬nh trÆ°á»›c khi xem báº£ng so sÃ¡nh.")
    
    # Get all metrics from session state (will be updated by the input fields in other tabs)
    gnn_metrics_final = extract_training_metrics(st.session_state.training_results.get("gnn"), "gnn")
    cbf_metrics_final = extract_training_metrics(st.session_state.training_results.get("cbf"), "cbf")
    hybrid_metrics_final = extract_training_metrics(st.session_state.training_results.get("hybrid"), "hybrid")
    
    # Get values from session state (auto-filled from API)
    def update_metrics_from_session(metrics_dict: Dict[str, Any], prefix: str) -> None:
        """Update metrics from session state with proper key mapping."""
        for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20", 
                   "training_time", "inference_time",
                   "num_users", "num_products", "num_interactions", 
                   "epochs", "embed_dim", "learning_rate"]:
            session_key = f"{prefix}_{key}"
            if session_key in st.session_state:
                metrics_dict[key] = st.session_state[session_key]
        
        # Handle special mappings
        if f"{prefix}_num_samples" in st.session_state:
            metrics_dict["num_training_samples"] = st.session_state[f"{prefix}_num_samples"]
        if f"{prefix}_batch" in st.session_state:
            metrics_dict["batch_size"] = st.session_state[f"{prefix}_batch"]
        if f"{prefix}_embed" in st.session_state:
            metrics_dict["embed_dim"] = st.session_state[f"{prefix}_embed"]
        if f"{prefix}_lr" in st.session_state:
            metrics_dict["learning_rate"] = st.session_state[f"{prefix}_lr"]
    
    update_metrics_from_session(gnn_metrics_final, "gnn")
    update_metrics_from_session(cbf_metrics_final, "cbf")
    update_metrics_from_session(hybrid_metrics_final, "hybrid")
    
    # Also get alpha for hybrid
    if "hybrid_alpha" in st.session_state:
        alpha_final = st.session_state["hybrid_alpha"]
    else:
        alpha_final = 0.7
    
    # Generate comparison table
    comparison_doc = generate_comparison_table(gnn_metrics_final, cbf_metrics_final, hybrid_metrics_final)
    
    st.markdown("---")
    st.subheader("ðŸ“„ Báº£ng so sÃ¡nh (cÃ³ thá»ƒ copy)")
    st.markdown(comparison_doc)
    
    # Copy button
    st.code(comparison_doc, language="markdown")
    
    # Visual comparison
    st.subheader("ðŸ“Š Biá»ƒu Ä‘á»“ so sÃ¡nh")
    comparison_data = {
        "MÃ´ hÃ¬nh": ["GNN (LightGCN)", "Content-based Filtering", "Hybrid GNN+CBF"],
        "Recall@10": [gnn_metrics_final.get('recall_at_10', 'N/A'), cbf_metrics_final.get('recall_at_10', 'N/A'), hybrid_metrics_final.get('recall_at_10', 'N/A')],
        "Recall@20": [gnn_metrics_final.get('recall_at_20', 'N/A'), cbf_metrics_final.get('recall_at_20', 'N/A'), hybrid_metrics_final.get('recall_at_20', 'N/A')],
        "NDCG@10": [gnn_metrics_final.get('ndcg_at_10', 'N/A'), cbf_metrics_final.get('ndcg_at_10', 'N/A'), hybrid_metrics_final.get('ndcg_at_10', 'N/A')],
        "NDCG@20": [gnn_metrics_final.get('ndcg_at_20', 'N/A'), cbf_metrics_final.get('ndcg_at_20', 'N/A'), hybrid_metrics_final.get('ndcg_at_20', 'N/A')],
    }
    
    # Try to convert to numeric for plotting
    try:
        comparison_df = pd.DataFrame(comparison_data)
        for col in ["Recall@10", "Recall@20", "NDCG@10", "NDCG@20"]:
            comparison_df[col] = pd.to_numeric(comparison_df[col], errors='coerce')
        
        st.bar_chart(comparison_df.set_index("MÃ´ hÃ¬nh")[["Recall@10", "Recall@20", "NDCG@10", "NDCG@20"]], use_container_width=True)
    except:
        st.info("Vui lÃ²ng nháº­p sá»‘ liá»‡u Ä‘á»ƒ hiá»ƒn thá»‹ biá»ƒu Ä‘á»“ so sÃ¡nh.")


# Update session state when training completes
st.markdown("---")
st.caption(
    "á»¨ng dá»¥ng Streamlit nÃ y giÃºp kiá»ƒm thá»­ nhanh cÃ¡c API gá»£i Ã½ sáº£n pháº©m cá»§a Novaware vÃ  táº¡o tÃ i liá»‡u tá»± Ä‘á»™ng."
)

