"""Streamlit dashboard for Novaware product analytics and model APIs."""

from __future__ import annotations

import json
import time
from io import BytesIO
from typing import Any, Dict, Optional

import matplotlib.pyplot as plt
import numpy as np
import re
from decimal import Decimal, InvalidOperation, ROUND_DOWN
import os
import pandas as pd
import requests
import seaborn as sns
import streamlit as st
try:
    from dotenv import load_dotenv
except ImportError:
    load_dotenv = None

if load_dotenv:
    load_dotenv()


st.set_page_config(
    page_title="Novaware Product Insights",
    page_icon="üß•",
    layout="wide",
)

# Custom CSS for better styling
st.markdown("""
<style>
    .model-card {
        background-color: #f0f2f6;
        padding: 20px;
        border-radius: 10px;
        margin: 10px 0;
    }
    .metric-card {
        background-color: #ffffff;
        padding: 15px;
        border-radius: 8px;
        border-left: 4px solid #FF4B4B;
        margin: 5px 0;
    }
    .step-header {
        background-color: #FF4B4B;
        color: white;
        padding: 10px;
        border-radius: 5px;
        margin: 10px 0;
    }
</style>
""", unsafe_allow_html=True)

st.title("üß• Novaware Product Insights & Model Console")
st.caption(
    "Upload CSV data, explore quick analytics, and interact with the "
    "GNN / CBF / Hybrid recommendation APIs."
)

@st.cache_data(show_spinner=False)
def load_csv(file_buffer: BytesIO) -> pd.DataFrame:
    """Load CSV with error handling for malformed data."""
    try:
        # Try standard read first
        return pd.read_csv(file_buffer)
    except pd.errors.ParserError:
        # Reset buffer position
        file_buffer.seek(0)
        # Try with error handling options
        try:
            # Option 1: Skip bad lines
            return pd.read_csv(file_buffer, on_bad_lines='skip', engine='python')
        except Exception:
            # Reset buffer position again
            file_buffer.seek(0)
            # Option 2: Use more lenient parsing
            return pd.read_csv(
                file_buffer,
                on_bad_lines='skip',
                quoting=1,  # QUOTE_ALL
                escapechar='\\',
                engine='python'
            )


def describe_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    """Generate descriptive statistics for all columns in the dataframe."""
    numeric_stats = df.describe(
        percentiles=[0.25, 0.5, 0.75],
        include="all",
    ).transpose()
    # Select only available columns (some may not exist for non-numeric data)
    available_cols = [col for col in ["count", "mean", "std", "min", "25%", "50%", "75%", "max"] 
                      if col in numeric_stats.columns]
    numeric_stats = numeric_stats[available_cols].dropna(how="all")
    return numeric_stats


def plot_sparsity(df: pd.DataFrame) -> None:
    """Plot missing data ratio using KDE (Kernel Density Estimation)."""
    sparsity = df.isna().sum() / len(df) if len(df) else df.isna().sum()
    sparsity_values = sparsity.values
    
    # Create KDE plot
    fig, ax = plt.subplots(figsize=(10, 4))
    
    if len(sparsity_values) > 0 and sparsity_values.max() > 0:
        # KDE plot
        sns.kdeplot(data=sparsity_values, fill=True, color='#FF4B4B', ax=ax)
        ax.set_xlabel('Missing Ratio', fontsize=12)
        ax.set_ylabel('Density', fontsize=12)
        ax.set_title('Distribution of Missing Data (KDE)', fontsize=14, fontweight='bold')
        ax.grid(True, alpha=0.3)
        
        # Add statistics text
        mean_val = sparsity_values.mean()
        median_val = pd.Series(sparsity_values).median()
        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2%}')
        ax.axvline(median_val, color='blue', linestyle='--', linewidth=2, label=f'Median: {median_val:.2%}')
        ax.legend()
    else:
        ax.text(0.5, 0.5, 'No missing data', ha='center', va='center', fontsize=14)
        ax.set_xlim(0, 1)
    
    plt.tight_layout()
    st.pyplot(fig)
    plt.close()
    
    # Show detailed table
    with st.expander("üìä Chi ti·∫øt Missing Ratio theo c·ªôt"):
        sparsity_df = (
            sparsity.rename("Missing Ratio")
            .reset_index()
            .rename(columns={"index": "Column"})
            .sort_values("Missing Ratio", ascending=False)
        )
        sparsity_df["Missing Ratio"] = sparsity_df["Missing Ratio"].apply(lambda x: f"{x:.2%}")
        st.dataframe(sparsity_df, use_container_width=True, hide_index=True)


def plot_ratio(df: pd.DataFrame, column: str) -> None:
    """Plot value distribution for a categorical column."""
    value_counts = (
        df[column]
        .fillna("Unknown")
        .astype(str)
        .value_counts(normalize=True)
        .mul(100)
    )
    
    # Create DataFrame with proper column names
    value_ratio = pd.DataFrame({
        column: value_counts.index,
        "Percentage": value_counts.values
    })
    
    st.bar_chart(
        value_ratio,
        x=column,
        y="Percentage",
        use_container_width=True,
    )


def call_api(
    base_url: str,
    endpoint: str,
    payload: Optional[Dict[str, Any]] = None,
    method: str = "post",
) -> Dict[str, Any]:
    url = f"{base_url.rstrip('/')}/{endpoint.lstrip('/')}"
    try:
        response = requests.request(method, url, json=payload, timeout=600)
        response.raise_for_status()
        return {
            "success": True,
            "data": response.json(),
        }
    except requests.RequestException as exc:
        return {
            "success": False,
            "error": str(exc),
            "response": getattr(exc, "response", None)
            and getattr(exc.response, "text", None),
        }
    except json.JSONDecodeError:
        return {"success": True, "data": {"message": "Completed", "raw": response.text}}


BASE_URL = st.sidebar.text_input(
    "API base URL",
    value="http://127.0.0.1:8000/api/v1",
    help="ƒê·∫∑t URL backend Django (v√≠ d·ª• http://localhost:8000/api/v1).",
)
st.sidebar.markdown("---")
st.sidebar.write("User_ID c·ªë ƒë·ªãnh: `690bf0f2d0c3753df0ecbdd6`")
st.sidebar.write("Product_ID th·ª≠ nghi·ªám: `10068`")


# Store training results in session state
if "training_results" not in st.session_state:
    st.session_state.training_results = {
        "gnn": None,
        "cbf": None,
        "hybrid": None,
    }

if "recommendation_results" not in st.session_state:
    st.session_state.recommendation_results = {
        "gnn": None,
        "cbf": None,
        "hybrid": None,
    }

# Store evaluation_support (pairs or ids provided by API) in session state
if "evaluation_support" not in st.session_state:
    st.session_state.evaluation_support = {
        "gnn": None,
        "cbf": None,
        "hybrid": None,
    }


def extract_training_metrics(result_data: Dict[str, Any], model_type: str) -> Dict[str, Any]:
    """Extract training metrics from API response.
    
    This extracts metrics from /train API response which includes:
    - Training parameters: num_users, num_products, epochs, batch_size, etc.
    - Training time: time taken to train the model
    """
    metrics = {
        "num_users": "N/A",
        "num_products": "N/A",
        "num_interactions": "N/A",
        "num_training_samples": "N/A",
        "epochs": "N/A",
        "batch_size": "N/A",
        "embed_dim": "N/A",
        "learning_rate": "N/A",
        "test_size": 0.2,
        "training_time": "N/A",
    }
    
    if not result_data:
        return metrics
    
    # Try to extract from different possible response structures
    if isinstance(result_data, dict):
        # Training time - extract from result
        for key in ["training_time", "time"]:
            if key in result_data:
                value = result_data[key]
                if value is None:
                    continue
                if isinstance(value, (int, float)):
                    metrics["training_time"] = str(value)
                else:
                    metrics["training_time"] = value
        
        # Training info nested structure
        if "training_info" in result_data:
            info = result_data["training_info"]
            # Map API keys to metric keys
            info_key_mapping = {
                "embedding_dim": "embed_dim",
            }
            for key in ["num_users", "num_products", "num_interactions", "num_training_samples",
                       "epochs", "batch_size", "embed_dim", "embedding_dim", "learning_rate"]:
                if key in info:
                    value = info[key]
                    target_key = info_key_mapping.get(key, key)
                    metrics[target_key] = str(value) if value is not None else "N/A"
        
        # Direct keys at root level (from /train API)
        # Map API keys to metric keys
        key_mapping = {
            "embedding_dim": "embed_dim",  # API returns embedding_dim, we need embed_dim
            "time": "training_time",
        }
        
        for key in ["num_users", "num_products", "num_interactions", "num_training_samples",
                   "epochs", "batch_size", "embed_dim", "embedding_dim", 
                   "learning_rate", "training_time", "time", "test_size"]:
            if key in result_data:
                value = result_data[key]
                # Use mapping if exists, otherwise use key as-is
                target_key = key_mapping.get(key, key)
                if isinstance(value, (int, float)):
                    metrics[target_key] = str(value)
                else:
                    metrics[target_key] = value if value is not None else "N/A"
        
        # Try nested structures (e.g., metrics.evaluation, stats, etc.)
        for nested_key in ["metrics", "evaluation", "stats", "results"]:
            if nested_key in result_data and isinstance(result_data[nested_key], dict):
                nested = result_data[nested_key]
                # Extract training time if available
                for key in ["training_time", "time"]:
                    if key in nested:
                        value = nested[key]
                        if isinstance(value, (int, float)):
                            metrics["training_time"] = str(value)
                        else:
                            metrics["training_time"] = value
        
        # Try to extract from summary or message
        if "summary" in result_data:
            summary = result_data["summary"]
            if isinstance(summary, dict):
                for key in summary:
                    if key in metrics:
                        value = summary[key]
                        metrics[key] = str(value) if isinstance(value, (int, float)) else value
    
    return metrics


def extract_recommend_metrics(result_data: Dict[str, Any], model_type: str) -> Dict[str, Any]:

    """Extract evaluation metrics from /recommend API response.
    
    The /recommend API returns evaluation_metrics with:
    - Recall@10, Recall@20, NDCG@10, NDCG@20, inference_time
    """
    metrics = {
        "recall_at_10": "N/A",
        "recall_at_20": "N/A",
        "ndcg_at_10": "N/A",
        "ndcg_at_20": "N/A",
        "inference_time": "N/A",
    }
    
    if not result_data or not isinstance(result_data, dict):
        return metrics
    
    # Extract from evaluation_metrics (from /recommend API)
    if "evaluation_metrics" in result_data:
        eval_metrics = result_data["evaluation_metrics"]
        if isinstance(eval_metrics, dict):
            for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20"]:
                if key in eval_metrics:
                    value = eval_metrics[key]
                    if isinstance(value, (int, float)):
                        metrics[key] = str(value)
                    else:
                        metrics[key] = value
            
            # Inference time (in milliseconds)
            if "inference_time" in eval_metrics:
                value = eval_metrics["inference_time"]
                metrics["inference_time"] = str(value) if isinstance(value, (int, float)) else value
            elif "time" in eval_metrics:
                value = eval_metrics["time"]
                # Convert seconds to milliseconds if needed
                if isinstance(value, (int, float)):
                    if value < 1000:  # Likely in seconds, convert to ms
                        metrics["inference_time"] = str(value * 1000)
                    else:
                        metrics["inference_time"] = str(value)
                else:
                    metrics["inference_time"] = value
    
    return metrics


def extract_evaluation_support(result_data: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """Extract evaluation support (tested user/product IDs or pairs) from API response.
    Normalizes to: { 'pairs': [{'user_id':..., 'current_product_id':...}, ...], 'user_ids': [...], 'product_ids': [...] }
    """
    if not isinstance(result_data, dict):
        return None

    def _normalize_pairs(pairs_list):
        norm = []
        for p in pairs_list or []:
            if not isinstance(p, dict):
                continue
            uid = p.get('user_id') or p.get('userId') or p.get('uid')
            pid = p.get('current_product_id') or p.get('product_id') or p.get('item_id') or p.get('pid')
            if uid is not None and pid is not None:
                norm.append({'user_id': str(uid), 'current_product_id': str(pid)})
        return norm

    # 1) Direct key
    if 'evaluation_support' in result_data:
        es = result_data.get('evaluation_support')
        pairs = []
        user_ids = None
        product_ids = None
        if isinstance(es, dict):
            # dict form
            if isinstance(es.get('pairs'), list):
                pairs = _normalize_pairs(es.get('pairs'))
            if isinstance(es.get('tested_pairs'), list):
                pairs = pairs or _normalize_pairs(es.get('tested_pairs'))
            if isinstance(es.get('test_pairs'), list):
                pairs = pairs or _normalize_pairs(es.get('test_pairs'))
            if isinstance(es.get('user_ids'), list):
                user_ids = [str(x) for x in es.get('user_ids')]
            if isinstance(es.get('product_ids'), list):
                product_ids = [str(x) for x in es.get('product_ids')]
        elif isinstance(es, list):
            pairs = _normalize_pairs(es)
        if pairs or user_ids or product_ids:
            return {'pairs': pairs, 'user_ids': user_ids, 'product_ids': product_ids}

    # 2) Alternate keys on root
    for key in ['tested_pairs', 'test_pairs', 'test_cases']:
        if isinstance(result_data.get(key), list):
            pairs = _normalize_pairs(result_data.get(key))
            if pairs:
                return {'pairs': pairs, 'user_ids': None, 'product_ids': None}

    # 3) Root arrays
    if isinstance(result_data.get('user_ids'), list) and isinstance(result_data.get('product_ids'), list):
        return {
            'pairs': None,
            'user_ids': [str(x) for x in result_data['user_ids']],
            'product_ids': [str(x) for x in result_data['product_ids']],
        }

    # 4) Nested common containers
    for container in ['data', 'metrics', 'evaluation', 'results']:
        sub = result_data.get(container)
        if isinstance(sub, dict):
            found = extract_evaluation_support(sub)
            if found:
                return found

    return None

def auto_fill_metrics_to_session_state(slug: str, metrics: Dict[str, Any]) -> None:
    """Auto-fill extracted metrics to session state for input fields."""
    # Map of metric keys to session state keys
    field_mapping = {
        "num_users": f"{slug}_num_users",
        "num_products": f"{slug}_num_products",
        "num_interactions": f"{slug}_num_interactions",
        "num_training_samples": f"{slug}_num_samples",
        "epochs": f"{slug}_epochs",
        "batch_size": f"{slug}_batch",
        "embed_dim": f"{slug}_embed",
        "learning_rate": f"{slug}_lr",
        "test_size": f"{slug}_test_size",
        "training_time": f"{slug}_training_time",
        "recall_at_10": f"{slug}_recall_at_10",
        "recall_at_20": f"{slug}_recall_at_20",
        "ndcg_at_10": f"{slug}_ndcg_at_10",
        "ndcg_at_20": f"{slug}_ndcg_at_20",
        "inference_time": f"{slug}_inference_time",
    }
    
    # Update session state with extracted metrics
    for metric_key, state_key in field_mapping.items():
        if metric_key in metrics and metrics[metric_key] != "N/A":
            value = metrics[metric_key]
            # Convert to appropriate type
            if metric_key == "test_size":
                try:
                    st.session_state[state_key] = float(value) if value != "N/A" else 0.2
                except (ValueError, TypeError):
                    st.session_state[state_key] = 0.2
            else:
                st.session_state[state_key] = str(value)


PRECISION_FORMAT_KEYS = ("recall_at_10", "recall_at_20", "training_time")


def format_metric_value(value: Any, decimals: int = 4) -> str:
    """Format numeric metrics with fixed decimal places without rounding up."""
    if value is None:
        return "N/A"
    value_str = str(value).strip()
    if not value_str or value_str.upper() == "N/A":
        return "N/A"
    match = re.match(r"^(-?\d+(?:\.\d+)?)(.*)$", value_str)
    suffix = ""
    number_part = value_str
    if match:
        number_part, suffix = match.groups()
    try:
        decimal_value = Decimal(number_part)
    except InvalidOperation:
        return value_str
    quant = Decimal("1").scaleb(-decimals)
    truncated = decimal_value.quantize(quant, rounding=ROUND_DOWN)
    formatted_number = f"{truncated:.{decimals}f}"
    return f"{formatted_number}{suffix}"


def apply_precision_formatting(metrics_dict: Dict[str, Any]) -> Dict[str, Any]:
    """Ensure key metrics respect the 4-decimal precision requirement."""
    for key in PRECISION_FORMAT_KEYS:
        metrics_dict[key] = format_metric_value(metrics_dict.get(key))
    return metrics_dict

# ----- Metric computation helpers (apply formulas) -----
from math import log2

def compute_recall_at_k(recommended_ids, ground_truth_ids, k=10) -> float:
    """Recall@K = |recs@K ‚à© GT| / |GT| (0..1)."""
    if not ground_truth_ids:
        return 0.0
    rec_topk = list(map(str, recommended_ids[:k]))
    gt = set(map(str, ground_truth_ids))
    hits = len([rid for rid in rec_topk if rid in gt])
    return hits / max(len(gt), 1)


def _dcg_at_k(binary_relevance, k=10) -> float:
    """DCG@K with binary gain: sum_{i=1..K} rel_i / log2(i+1)."""
    dcg = 0.0
    for i, rel in enumerate(binary_relevance[:k], start=1):
        if rel:
            dcg += 1.0 / log2(i + 1)
    return dcg


def compute_ndcg_at_k(recommended_ids, ground_truth_ids, k=10) -> float:
    """NDCG@K = DCG@K / IDCG@K with binary relevance from GT overlap."""
    if not ground_truth_ids:
        return 0.0
    rec_topk = list(map(str, recommended_ids[:k]))
    gt = set(map(str, ground_truth_ids))
    # Build binary relevance vector for the ranked list
    rel = [1 if rid in gt else 0 for rid in rec_topk]
    dcg = _dcg_at_k(rel, k)
    # Ideal relevance: top |GT| are 1s (capped at K)
    ideal_rel = [1] * min(len(gt), k)
    idcg = _dcg_at_k(ideal_rel, k)
    if idcg == 0:
        return 0.0
    return dcg / idcg


st.header("1. Upload & Preview CSV")

# T·∫°o 2 tabs cho s·∫£n ph·∫©m v√† ng∆∞·ªùi d√πng
tab_product, tab_user = st.tabs(["üì¶ D·ªØ li·ªáu S·∫£n ph·∫©m", "üë§ D·ªØ li·ªáu Ng∆∞·ªùi d√πng"])

# Tab 1: D·ªØ li·ªáu s·∫£n ph·∫©m
with tab_product:
    uploaded_file = st.file_uploader("T·∫£i file CSV s·∫£n ph·∫©m", type=["csv"], key="product_csv")

    df: Optional[pd.DataFrame] = None
    if uploaded_file is not None:
        with st.spinner("ƒêang ƒë·ªçc d·ªØ li·ªáu s·∫£n ph·∫©m..."):
            df = load_csv(uploaded_file)
        st.success(f"ƒê√£ t·∫£i {len(df):,} d√≤ng, {len(df.columns)} c·ªôt.")
        st.dataframe(df.head(100), use_container_width=True)

        st.subheader("Th·ªëng k√™ d·ªØ li·ªáu s·∫£n ph·∫©m")
        stats_df = describe_dataframe(df)
        st.dataframe(stats_df, use_container_width=True)

        st.subheader("Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (Missing Ratio)")
        plot_sparsity(df)

        st.subheader("Bi·ªÉu ƒë·ªì t·ª∑ l·ªá (Value Ratio)")
        ratio_col = st.selectbox(
            "Ch·ªçn c·ªôt ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì t·ª∑ l·ªá",
            options=df.columns.tolist(),
            key="product_ratio_col",
        )
        if ratio_col:
            plot_ratio(df, ratio_col)
    else:
        st.info("Vui l√≤ng t·∫£i l√™n file CSV s·∫£n ph·∫©m ƒë·ªÉ b·∫Øt ƒë·∫ßu.")

# Tab 2: D·ªØ li·ªáu ng∆∞·ªùi d√πng
with tab_user:
    uploaded_user_file = st.file_uploader("T·∫£i file CSV ng∆∞·ªùi d√πng", type=["csv"], key="user_csv")

    df_user: Optional[pd.DataFrame] = None
    if uploaded_user_file is not None:
        with st.spinner("ƒêang ƒë·ªçc d·ªØ li·ªáu ng∆∞·ªùi d√πng..."):
            df_user = load_csv(uploaded_user_file)
        st.success(f"ƒê√£ t·∫£i {len(df_user):,} ng∆∞·ªùi d√πng, {len(df_user.columns)} c·ªôt.")
        st.dataframe(df_user.head(100), use_container_width=True)

        st.subheader("Th·ªëng k√™ d·ªØ li·ªáu ng∆∞·ªùi d√πng")
        stats_user_df = describe_dataframe(df_user)
        st.dataframe(stats_user_df, use_container_width=True)

        # Ph√¢n t√≠ch ƒë·∫∑c bi·ªát cho d·ªØ li·ªáu ng∆∞·ªùi d√πng
        col1, col2 = st.columns(2)
        
        with col1:
            st.subheader("Ph√¢n b·ªë Gi·ªõi t√≠nh")
            if "Gender" in df_user.columns:
                gender_counts = df_user["Gender"].value_counts()
                st.bar_chart(gender_counts)
                
                # Hi·ªÉn th·ªã s·ªë li·ªáu
                for gender, count in gender_counts.items():
                    percentage = (count / len(df_user)) * 100
                    st.metric(
                        label=f"{gender}",
                        value=f"{count:,}",
                        delta=f"{percentage:.1f}%"
                    )
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y c·ªôt 'Gender' trong d·ªØ li·ªáu.")

        with col2:
            st.subheader("Ph√¢n b·ªë ƒê·ªô tu·ªïi")
            if "Age" in df_user.columns:
                # T·∫°o nh√≥m tu·ªïi
                df_user_copy = df_user.copy()
                df_user_copy["age_group"] = pd.cut(
                    df_user_copy["Age"],
                    bins=[0, 12, 18, 25, 35, 50, 100],
                    labels=["Kids (0-12)", "Teens (13-18)", "Young Adults (19-25)", 
                            "Adults (26-35)", "Middle Age (36-50)", "Senior (50+)"]
                )
                age_group_counts = df_user_copy["age_group"].value_counts().sort_index()
                st.bar_chart(age_group_counts)
                
                # Th·ªëng k√™ ƒë·ªô tu·ªïi
                st.write(f"**ƒê·ªô tu·ªïi trung b√¨nh:** {df_user['Age'].mean():.1f}")
                st.write(f"**ƒê·ªô tu·ªïi nh·ªè nh·∫•t:** {df_user['Age'].min()}")
                st.write(f"**ƒê·ªô tu·ªïi l·ªõn nh·∫•t:** {df_user['Age'].max()}")
            else:
                st.warning("Kh√¥ng t√¨m th·∫•y c·ªôt 'Age' trong d·ªØ li·ªáu.")

        st.subheader("Bi·ªÉu ƒë·ªì ƒë·ªô th∆∞a (Missing Ratio)")
        plot_sparsity(df_user)

        st.subheader("Bi·ªÉu ƒë·ªì t·ª∑ l·ªá (Value Ratio)")
        user_ratio_col = st.selectbox(
            "Ch·ªçn c·ªôt ƒë·ªÉ v·∫Ω bi·ªÉu ƒë·ªì t·ª∑ l·ªá",
            options=df_user.columns.tolist(),
            key="user_ratio_col",
        )
        if user_ratio_col:
            plot_ratio(df_user, user_ratio_col)
    else:
        st.info("Vui l√≤ng t·∫£i l√™n file CSV ng∆∞·ªùi d√πng ƒë·ªÉ ph√¢n t√≠ch.")


st.header("2. Hu·∫•n luy·ªán m√¥ h√¨nh")
models = {
    "GNN": "gnn",
    "Content-based (CBF)": "cbf",
    "Hybrid": "hybrid",
}

def poll_task_status(
    base_url: str, 
    endpoint: str, 
    task_id: str, 
    max_wait_time: int = 600,
    status_placeholder=None,
    progress_bar=None
) -> Dict[str, Any]:
    """Poll task status until completion or timeout."""
    start_time = time.time()
    poll_interval = 2  # Poll every 2 seconds
    last_progress = 0
    
    while time.time() - start_time < max_wait_time:
        result = call_api(base_url, endpoint, payload={"task_id": task_id}, method="post")
        
        if not result["success"]:
            return result
        
        data = result["data"]
        status = data.get("status", "unknown")
        
        if status == "success":
            # Success! Return the result with all metrics
            return result
        elif status == "failure":
            return {
                "success": False,
                "error": data.get("error", "Training failed"),
                "data": data,
            }
        elif status in ["pending", "running"]:
            # Update progress if available
            current_progress = data.get("progress", last_progress)
            if current_progress > last_progress:
                last_progress = current_progress
                # Progress from 30% to 90% during polling
                if progress_bar:
                    progress_bar.progress(30 + int(current_progress * 0.6))
                if status_placeholder:
                    message = data.get("message", f"Training in progress... {current_progress}%")
                    current_step = data.get("current_step", "")
                    if current_step:
                        message += f" - {current_step}"
                    status_placeholder.info(message)
            time.sleep(poll_interval)
            continue
        else:
            # Unknown status, wait and retry
            time.sleep(poll_interval)
            continue
    
    return {
        "success": False,
        "error": f"Training timeout after {max_wait_time} seconds",
        "data": {"status": "timeout", "task_id": task_id},
    }


train_cols = st.columns(len(models))
for col, (label, slug) in zip(train_cols, models.items()):
    with col:
        if st.button(f"Train {label}", key=f"train_{slug}"):
            status_placeholder = st.empty()
            progress = st.progress(0)
            status_placeholder.info("B·∫Øt ƒë·∫ßu g·ªçi API train...")
            progress.progress(10)
            start_time = time.time()
            
            # Use sync mode to get results immediately
            with st.spinner(f"ƒêang hu·∫•n luy·ªán {label}..."):
                # Try sync mode first (sends sync: true in payload)
                result = call_api(BASE_URL, f"{slug}/train", payload={"sync": True}, method="post")
                
                # If async response (has task_id), poll for results
                if result["success"] and isinstance(result["data"], dict):
                    data = result["data"]
                    if "task_id" in data and data.get("status") in ["pending", "running"]:
                        task_id = data["task_id"]
                        status_placeholder.info(f"Training ƒëang ch·∫°y (task_id: {task_id[:8]}...). ƒêang ch·ªù k·∫øt qu·∫£...")
                        progress.progress(30)
                        
                        # Poll for completion with progress updates
                        result = poll_task_status(
                            BASE_URL, 
                            f"{slug}/train", 
                            task_id, 
                            max_wait_time=600,
                            status_placeholder=status_placeholder,
                            progress_bar=progress
                        )
            
            elapsed_time = time.time() - start_time
            progress.progress(100)
            
            if result["success"]:
                status_placeholder.success(f"Train {label} ho√†n t·∫•t.")
                # Store result in session state for documentation
                result_data = result["data"]
                st.session_state.training_results[slug] = result_data
                # Extract and store evaluation_support from /train response (if provided)
                try:
                    support = extract_evaluation_support(result_data)
                    if support:
                        st.session_state.evaluation_support[slug] = support
                        cnt_pairs = len(support.get('pairs') or [])
                        cnt_u = len(support.get('user_ids') or [])
                        cnt_p = len(support.get('product_ids') or [])
                        st.info(f"üì¶ evaluation_support: pairs={cnt_pairs}, user_ids={cnt_u}, product_ids={cnt_p}")
                except Exception as _:
                    pass
                
                # Add training time if not present
                if isinstance(result_data, dict):
                    training_time_value = result_data.get("training_time")
                    legacy_time_value = result_data.get("time")
                    if training_time_value in (None, "", "N/A") and legacy_time_value in (None, "", "N/A"):
                        result_data["training_time"] = f"{elapsed_time:.2f}s"
                    
                    # Auto-fill metrics to session state for input fields
                    extracted_metrics = extract_training_metrics(result_data, slug)
                    auto_fill_metrics_to_session_state(slug, extracted_metrics)
                
                st.json(result_data)
                st.success(f"‚úÖ S·ªë li·ªáu ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông ƒëi·ªÅn v√†o ph·∫ßn t√†i li·ªáu!")
                
                # T·ª± ƒë·ªông g·ªçi API recommend ƒë·ªÉ l·∫•y evaluation metrics
                st.info("üîÑ ƒêang t·ª± ƒë·ªông g·ªçi API recommend ƒë·ªÉ l·∫•y evaluation metrics...")
                default_user_id = "690bf0f2d0c3753df0ecbdd6"
                
                # Try to get user's interaction history to test with multiple products
                product_ids_to_test = ["10068"]  # Default
                try:
                    user_url = f"{BASE_URL.rstrip('/')}/users/{default_user_id}"
                    user_response = requests.get(user_url, timeout=10)
                    if user_response.status_code == 200:
                        user_data = user_response.json()
                        if isinstance(user_data, dict) and "data" in user_data:
                            user_info = user_data["data"].get("user", {})
                            interaction_history = user_info.get("interaction_history", [])
                            if interaction_history:
                                # Get product IDs from interaction history
                                history_products = [str(interaction.get("product_id")) for interaction in interaction_history[:5] if interaction.get("product_id")]
                                if history_products:
                                    product_ids_to_test = history_products + ["10068"]  # Add default
                                    product_ids_to_test = list(dict.fromkeys(product_ids_to_test))  # Remove duplicates
                except:
                    pass
                
                # Test with multiple products and find the best result
                best_result = None
                best_metrics = None
                best_product_id = None
                recommended_products_to_try = []  # Collect recommended products to test
                
                progress_bar = st.progress(0)
                status_text = st.empty()
                total_tests = min(len(product_ids_to_test), 5)
                
                # First pass: Test with products from interaction history
                for idx, product_id in enumerate(product_ids_to_test[:5]):  # Test up to 5 products
                    status_text.info(f"ƒêang test v·ªõi product_id: {product_id} ({idx+1}/{total_tests})...")
                    progress_bar.progress((idx + 1) / (total_tests * 2))  # Reserve half for recommended products
                    
                    recommend_payload = {"user_id": default_user_id, "current_product_id": product_id}
                    recommend_result = call_api(BASE_URL, f"{slug}/recommend", payload=recommend_payload)
                    
                    if recommend_result["success"]:
                        data = recommend_result["data"]
                        eval_metrics = data.get("evaluation_metrics", {})
                        
                        # Collect recommended products for second pass
                        personalized = data.get("personalized", [])
                        for rec in personalized[:3]:  # Get first 3 recommendations
                            rec_product = rec.get("product", {})
                            if isinstance(rec_product, dict):
                                rec_id = rec_product.get("id")
                            else:
                                rec_id = rec.get("id") or rec.get("product_id")
                            if rec_id and str(rec_id) not in recommended_products_to_try and str(rec_id) not in product_ids_to_test:
                                recommended_products_to_try.append(str(rec_id))
                        
                        # Check if this result is better (has non-zero/non-null metrics)
                        if eval_metrics:
                            recall_at_10 = eval_metrics.get("recall_at_10", 0)
                            recall_at_20 = eval_metrics.get("recall_at_20", 0)
                            ndcg_at_10 = eval_metrics.get("ndcg_at_10", 0)
                            ndcg_at_20 = eval_metrics.get("ndcg_at_20", 0)
                            
                            # Check if this is a valid result (at least one metric is non-zero/non-null)
                            is_valid = (
                                recall_at_10 != 0 or recall_at_20 != 0 or 
                                ndcg_at_10 != 0 or ndcg_at_20 != 0
                            )
                            
                            if is_valid:
                                # Found valid metrics, use this result
                                best_result = recommend_result
                                best_metrics = eval_metrics
                                best_product_id = product_id
                                break
                            elif best_result is None:
                                # Keep first result as fallback
                                best_result = recommend_result
                                best_metrics = eval_metrics
                                best_product_id = product_id
                
                # Second pass: Test with recommended products if no valid metrics found
                if best_metrics and not any([
                    best_metrics.get("recall_at_10", 0) != 0,
                    best_metrics.get("recall_at_20", 0) != 0,
                    best_metrics.get("ndcg_at_10", 0) != 0,
                    best_metrics.get("ndcg_at_20", 0) != 0
                ]) and recommended_products_to_try:
                    status_text.info(f"Kh√¥ng t√¨m th·∫•y metrics h·ª£p l·ªá. ƒêang test v·ªõi {len(recommended_products_to_try[:5])} recommended products...")
                    
                    for idx, rec_product_id in enumerate(recommended_products_to_try[:5]):
                        status_text.info(f"ƒêang test v·ªõi recommended product_id: {rec_product_id} ({idx+1}/{min(len(recommended_products_to_try), 5)})...")
                        progress_bar.progress((total_tests + idx + 1) / (total_tests * 2))
                        
                        recommend_payload = {"user_id": default_user_id, "current_product_id": rec_product_id}
                        recommend_result = call_api(BASE_URL, f"{slug}/recommend", payload=recommend_payload)
                        
                        if recommend_result["success"]:
                            data = recommend_result["data"]
                            eval_metrics = data.get("evaluation_metrics", {})
                            
                            if eval_metrics:
                                recall_at_10 = eval_metrics.get("recall_at_10", 0)
                                recall_at_20 = eval_metrics.get("recall_at_20", 0)
                                ndcg_at_10 = eval_metrics.get("ndcg_at_10", 0)
                                ndcg_at_20 = eval_metrics.get("ndcg_at_20", 0)
                                
                                is_valid = (
                                    recall_at_10 != 0 or recall_at_20 != 0 or 
                                    ndcg_at_10 != 0 or ndcg_at_20 != 0
                                )
                                
                                if is_valid:
                                    # Found valid metrics, use this result
                                    best_result = recommend_result
                                    best_metrics = eval_metrics
                                    best_product_id = rec_product_id
                                    break
                
                progress_bar.progress(1.0)
                status_text.empty()
                
                if best_result and best_result["success"]:
                    has_valid_metrics = best_metrics and any([
                        best_metrics.get("recall_at_10", 0) != 0,
                        best_metrics.get("recall_at_20", 0) != 0,
                        best_metrics.get("ndcg_at_10", 0) != 0,
                        best_metrics.get("ndcg_at_20", 0) != 0
                    ])
                    
                    if has_valid_metrics:
                        st.success(f"‚úÖ ƒê√£ t√¨m th·∫•y evaluation metrics h·ª£p l·ªá v·ªõi product_id: {best_product_id}!")
                    else:
                        st.warning(f"‚ö†Ô∏è ƒê√£ test {total_tests + min(len(recommended_products_to_try), 5)} products nh∆∞ng metrics v·∫´n null/0.")
                        st.info(f"üìä S·ª≠ d·ª•ng k·∫øt qu·∫£ t·ª´ product_id: {best_product_id}")
                        
                        # Show debug info to help understand why
                        debug_info = best_metrics.get("_debug", {}) if best_metrics else {}
                        if debug_info:
                            with st.expander("üîç Debug Info - T·∫°i sao metrics = 0?"):
                                st.json(debug_info)
                                
                                # Show diagnosis if available
                                diagnosis = best_metrics.get("_diagnosis", {}) if best_metrics else {}
                                if diagnosis:
                                    st.markdown("#### üî¨ Ch·∫©n ƒëo√°n t·ª± ƒë·ªông:")
                                    issues = diagnosis.get("issues", [])
                                    if issues:
                                        for issue in issues:
                                            severity = issue.get("severity", "info")
                                            if severity == "error":
                                                st.error(f"‚ùå **{issue.get('issue')}**")
                                            elif severity == "warning":
                                                st.warning(f"‚ö†Ô∏è **{issue.get('issue')}**")
                                            else:
                                                st.info(f"‚ÑπÔ∏è **{issue.get('issue')}**")
                                            st.markdown(f"- **L√Ω do**: {issue.get('reason')}")
                                            st.markdown(f"- **C√°ch s·ª≠a**: {issue.get('fix')}")
                                    else:
                                        st.success("‚úÖ Kh√¥ng ph√°t hi·ªán v·∫•n ƒë·ªÅ trong logic t√≠nh to√°n")
                                
                                # Show overlap info
                                overlap_found = debug_info.get("overlap_found", False)
                                num_rec = debug_info.get("num_recommendations", 0)
                                num_gt = debug_info.get("num_ground_truth", 0)
                                
                                st.markdown("#### üìä T√≥m t·∫Øt:")
                                col1, col2, col3 = st.columns(3)
                                with col1:
                                    st.metric("Recommendations", num_rec)
                                with col2:
                                    st.metric("Ground Truth", num_gt)
                                with col3:
                                    st.metric("Overlap", "‚úÖ C√≥" if overlap_found else "‚ùå Kh√¥ng")
                                
                                if not overlap_found and num_rec > 0 and num_gt > 0:
                                    st.info("üí° **Gi·∫£i th√≠ch**: CBF ƒëang recommend c√°c s·∫£n ph·∫©m kh√°c v·ªõi interaction_history c·ªßa user. ƒê√¢y c√≥ th·ªÉ l√† h√†nh vi ƒë√∫ng (recommend s·∫£n ph·∫©m m·ªõi), nh∆∞ng ƒë·ªÉ t√≠nh metrics c·∫ßn c√≥ overlap.")
                    
                    # Store recommendation result
                    st.session_state.recommendation_results[slug] = best_result["data"]
                    
                    # Extract evaluation metrics from recommend API and update session state
                    if isinstance(best_result["data"], dict):
                        eval_metrics = extract_recommend_metrics(best_result["data"], slug)
                        # Update session state with evaluation metrics from recommend API
                        for key, value in eval_metrics.items():
                            if value != "N/A":
                                state_key = f"{slug}_{key}"
                                st.session_state[state_key] = value
                                # Also update training_results if exists
                                if st.session_state.training_results.get(slug):
                                    if isinstance(st.session_state.training_results[slug], dict):
                                        st.session_state.training_results[slug][key] = value
                    
                    st.json(best_result["data"].get("evaluation_metrics", {}))
                else:
                    st.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ t·ª± ƒë·ªông g·ªçi API recommend: {best_result.get('error', 'Unknown error') if best_result else 'No valid results found'}")
            else:
                status_placeholder.error(f"L·ªói train {label}.")
                st.error(result["error"])
                if result.get("data"):
                    st.json(result["data"])
                if result.get("response"):
                    st.code(result["response"])


st.header("3. Recommendation APIs")
default_user_id = "690bf0f2d0c3753df0ecbdd6"
default_product_id = "10068"

user_id = st.text_input("User ID", value=default_user_id)
product_id = st.text_input("Product ID", value=default_product_id)

recommend_cols = st.columns(len(models))
# API expects user_id and current_product_id (not userId and productId)
payload = {"user_id": user_id, "current_product_id": product_id}

for col, (label, slug) in zip(recommend_cols, models.items()):
    with col:
        if st.button(f"Recommend {label}", key=f"recommend_{slug}"):
            status_placeholder = st.empty()
            status_placeholder.info("ƒêang g·ªçi API recommend...")
            with st.spinner(f"ƒê·ª£i k·∫øt qu·∫£ {label}..."):
                result = call_api(BASE_URL, f"{slug}/recommend", payload=payload)
            if result["success"]:
                status_placeholder.success(f"K·∫øt qu·∫£ {label} s·∫µn s√†ng.")
                # Store recommendation result
                st.session_state.recommendation_results[slug] = result["data"]

                # Extract evaluation_support from recommend response (if provided)
                try:
                    support = extract_evaluation_support(result["data"])
                    if support:
                        st.session_state.evaluation_support[slug] = support
                        cnt_pairs = len(support.get('pairs') or [])
                        cnt_u = len(support.get('user_ids') or [])
                        cnt_p = len(support.get('product_ids') or [])
                        st.info(f"üì¶ evaluation_support: pairs={cnt_pairs}, user_ids={cnt_u}, product_ids={cnt_p}")
                except Exception:
                    pass
                
                # Extract evaluation metrics from recommend API and update session state
                if isinstance(result["data"], dict):
                    eval_metrics = extract_recommend_metrics(result["data"], slug)
                    # Update session state with evaluation metrics from recommend API
                    for key, value in eval_metrics.items():
                        if value != "N/A":
                            state_key = f"{slug}_{key}"
                            st.session_state[state_key] = value
                            # Also update training_results if exists
                            if st.session_state.training_results.get(slug):
                                if isinstance(st.session_state.training_results[slug], dict):
                                    st.session_state.training_results[slug][key] = value
                
                st.json(result["data"])
            else:
                status_placeholder.error(f"L·ªói recommend {label}.")
                st.error(result["error"])
                if result.get("response"):
                    st.code(result["response"])


def generate_gnn_documentation(metrics: Dict[str, Any]) -> str:
    """Generate GNN documentation markdown with metrics."""
    doc = f"""### 2.3.1. GNN (Graph Neural Network - LightGCN)

- **Quy tr√¨nh th·ª±c hi·ªán**:
  - *Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi Surprise*:  
    S·ª≠ d·ª•ng `surprise.Dataset.load_from_df(...)` v√† `train_test_split(test_size={metrics['test_size']})` ƒë·ªÉ chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠.  
    - Test size: **{metrics['test_size']}** (t·ª∑ l·ªá d·ªØ li·ªáu d√πng ƒë·ªÉ ki·ªÉm th·ª≠, ph·∫ßn c√≤n l·∫°i d√πng ƒë·ªÉ hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng train: **{metrics['num_users']}** (s·ªë ng∆∞·ªùi d√πng trong t·∫≠p hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng s·∫£n ph·∫©m train: **{metrics['num_products']}** (s·ªë s·∫£n ph·∫©m trong t·∫≠p hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng t∆∞∆°ng t√°c (interactions): **{metrics['num_interactions']}** (t·ªïng s·ªë l∆∞·ª£t t∆∞∆°ng t√°c gi·ªØa ng∆∞·ªùi d√πng v√† s·∫£n ph·∫©m)
    - S·ªë l∆∞·ª£ng training samples (BPR): **{metrics['num_training_samples']}** (s·ªë m·∫´u hu·∫•n luy·ªán sau khi t·∫°o negative samples cho BPR)
  - *Pipeline 5 b∆∞·ªõc*:
    1. **Hu·∫•n luy·ªán m√¥ h√¨nh**: LightGCN v·ªõi ki·∫øn tr√∫c Graph Convolutional Network.
       - Thu·∫≠t to√°n: LightGCN (Light Graph Convolution Network) - m√¥ h√¨nh h·ªçc bi·ªÉu di·ªÖn ng∆∞·ªùi d√πng v√† s·∫£n ph·∫©m d·ª±a tr√™n ƒë·ªì th·ªã t∆∞∆°ng t√°c
       - Framework: PyTorch + PyTorch Geometric
       - Loss function: BPR (Bayesian Personalized Ranking) - t·ªëi ∆∞u h√≥a th·ª© h·∫°ng s·∫£n ph·∫©m cho t·ª´ng ng∆∞·ªùi d√πng
       - Negative sampling: 4 negative samples per positive interaction (t·∫°o 4 m·∫´u √¢m cho m·ªói t∆∞∆°ng t√°c t√≠ch c·ª±c ƒë·ªÉ h·ªçc ph√¢n bi·ªát)
       - Epochs: **{metrics['epochs']}** (s·ªë l·∫ßn duy·ªát to√†n b·ªô d·ªØ li·ªáu training)
       - Batch size: **{metrics['batch_size']}** (s·ªë l∆∞·ª£ng m·∫´u x·ª≠ l√Ω c√πng l√∫c trong m·ªói b∆∞·ªõc c·∫≠p nh·∫≠t)
       - Embedding dimension: **{metrics['embed_dim']}** (k√≠ch th∆∞·ªõc vector ƒë·∫°i di·ªán cho ng∆∞·ªùi d√πng/s·∫£n ph·∫©m, c√†ng l·ªõn c√†ng bi·ªÉu di·ªÖn chi ti·∫øt h∆°n)
       - Learning rate: **{metrics['learning_rate']}** (t·ªëc ƒë·ªô h·ªçc, ƒëi·ªÅu ch·ªânh ƒë·ªô l·ªõn b∆∞·ªõc c·∫≠p nh·∫≠t tham s·ªë)
       - Optimizer: Adam (thu·∫≠t to√°n t·ªëi ∆∞u h√≥a t·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh learning rate)
       - Model file: `models/gnn_lightgcn.pkl`
    2. **Chu·∫©n b·ªã d·ªØ li·ªáu graph**: 
       - X√¢y d·ª±ng bipartite graph (ƒë·ªì th·ªã hai ph√≠a) t·ª´ `UserInteraction` collection, m·ªói c·∫°nh n·ªëi m·ªôt ng∆∞·ªùi d√πng v·ªõi m·ªôt s·∫£n ph·∫©m
       - √Åp d·ª•ng tr·ªçng s·ªë t∆∞∆°ng t√°c theo `INTERACTION_WEIGHTS` ƒë·ªÉ ph√¢n bi·ªát m·ª©c ƒë·ªô quan tr·ªçng:
         ```python
         INTERACTION_WEIGHTS = {{
             'view': 1.0,        # Xem s·∫£n ph·∫©m (quan t√¢m th·∫•p)
             'add_to_cart': 2.0, # Th√™m v√†o gi·ªè (quan t√¢m trung b√¨nh)
             'purchase': 3.0,    # Mua h√†ng (quan t√¢m cao nh·∫•t)
             'wishlist': 1.5,    # Y√™u th√≠ch (quan t√¢m trung b√¨nh-th·∫•p)
             'rating': 2.5       # ƒê√°nh gi√° (quan t√¢m cao)
         }}
         ```
       - T·∫°o edge index (danh s√°ch c·∫∑p user-product) v√† edge weights (tr·ªçng s·ªë t∆∞∆°ng ·ª©ng)
    3. **T·∫°o ma tr·∫≠n User-Item Interaction**: 
       - S·ª≠ d·ª•ng sparse matrix (ma tr·∫≠n th∆∞a) ƒë·ªÉ bi·ªÉu di·ªÖn t∆∞∆°ng t√°c user-product m·ªôt c√°ch hi·ªáu qu·∫£
       - T√≠nh to√°n sparsity (ƒë·ªô th∆∞a): `sparsity = 1 - ({metrics['num_interactions']} / ({metrics['num_users']} * {metrics['num_products']}))` - t·ª∑ l·ªá ph·∫ßn trƒÉm c√°c t∆∞∆°ng t√°c kh√¥ng x·∫£y ra
    4. **T√≠nh cosine similarity** gi·ªØa user embeddings v√† product embeddings.  
       - Sau khi training, LightGCN sinh ra:
         - User embeddings: `[{metrics['num_users']}, {metrics['embed_dim']}]` - {metrics['num_users']} vector, m·ªói vector {metrics['embed_dim']} chi·ªÅu
         - Product embeddings: `[{metrics['num_products']}, {metrics['embed_dim']}]` - {metrics['num_products']} vector, m·ªói vector {metrics['embed_dim']} chi·ªÅu
       - Recommendation score = dot product (t√≠ch v√¥ h∆∞·ªõng) gi·ªØa user embedding v√† product embedding, gi√° tr·ªã c√†ng cao th√¨ s·∫£n ph·∫©m c√†ng ph√π h·ª£p v·ªõi ng∆∞·ªùi d√πng
    5. **T√≠nh to√°n ch·ªâ s·ªë ƒë√°nh gi√°**: Recall@10, Recall@20, NDCG@10, NDCG@20, th·ªùi gian train, th·ªùi gian inference.
       - *Recall@10*: Trong 10 m√≥n b·∫°n g·ª£i √Ω, c√≥ bao nhi√™u m√≥n user th·ª±c s·ª± th√≠ch (trong test set)? C√†ng cao c√†ng t·ªët (0-1)
       - *Recall@20*: T∆∞∆°ng t·ª± nh∆∞ng top 20. C√†ng cao c√†ng t·ªët (0-1)
       - *NDCG@10*: Top 10 c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë√∫ng m√† c√≤n s·∫Øp x·∫øp ƒë√∫ng th·ª© t·ª± (m√≥n user th√≠ch nh·∫•t ƒë·ª©ng cao). C√†ng cao c√†ng t·ªët (0-1)
       - *NDCG@20*: T∆∞∆°ng t·ª± top 20. C√†ng cao c√†ng t·ªët (0-1)
       - *Th·ªùi gian train*: M·∫•t bao l√¢u ƒë·ªÉ train xong 1 l·∫ßn ({metrics.get('training_time', 'N/A')}) - c√†ng th·∫•p c√†ng t·ªët
       - *Th·ªùi gian inference/user*: M·∫•t bao l√¢u ƒë·ªÉ tr·∫£ v·ªÅ g·ª£i √Ω cho 1 user ({metrics.get('inference_time', 'N/A')} ms) - c√†ng th·∫•p c√†ng t·ªët (r·∫•t quan tr·ªçng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Th·ªùi gian train | Th·ªùi gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| GNN (LightGCN) | {metrics.get('recall_at_10', 'N/A')} | {metrics.get('recall_at_20', 'N/A')} | {metrics.get('ndcg_at_10', 'N/A')} | {metrics.get('ndcg_at_20', 'N/A')} | {metrics.get('training_time', 'N/A')} | {metrics.get('inference_time', 'N/A')} ms |
"""
    return doc


def generate_cbf_documentation(metrics: Dict[str, Any]) -> str:
    """Generate Content-based Filtering documentation markdown with metrics."""
    doc = f"""### 2.3.2. Content-based Filtering

- **Quy tr√¨nh th·ª±c hi·ªán**:
  - *Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi Surprise*:  
    S·ª≠ d·ª•ng `surprise.Dataset.load_from_df(...)` v√† `train_test_split(test_size={metrics['test_size']})` ƒë·ªÉ chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠.  
    - Test size: **{metrics['test_size']}** (t·ª∑ l·ªá d·ªØ li·ªáu d√πng ƒë·ªÉ ki·ªÉm th·ª≠, ph·∫ßn c√≤n l·∫°i d√πng ƒë·ªÉ hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng s·∫£n ph·∫©m train: **{metrics['num_products']}** (s·ªë s·∫£n ph·∫©m trong t·∫≠p hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng test: **{metrics['num_users']}** (s·ªë ng∆∞·ªùi d√πng trong t·∫≠p ki·ªÉm th·ª≠)
  - *Pipeline 5 b∆∞·ªõc*:
    1. **Hu·∫•n luy·ªán m√¥ h√¨nh**: Sentence-BERT embedding + FAISS index.
       - Model: Sentence-BERT (SBERT) - m√¥ h√¨nh chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n th√†nh vector s·ªë, hi·ªÉu ƒë∆∞·ª£c ng·ªØ nghƒ©a c·ªßa m√¥ t·∫£ s·∫£n ph·∫©m
       - Index: FAISS (Facebook AI Similarity Search) - th∆∞ vi·ªán t√¨m ki·∫øm t∆∞∆°ng t·ª± nhanh, cho ph√©p t√¨m s·∫£n ph·∫©m t∆∞∆°ng t·ª± trong th·ªùi gian ng·∫Øn
       - Embedding dimension: **{metrics['embed_dim']}** (k√≠ch th∆∞·ªõc vector ƒë·∫°i di·ªán cho m·ªói s·∫£n ph·∫©m, c√†ng l·ªõn c√†ng bi·ªÉu di·ªÖn chi ti·∫øt h∆°n)
    2. **Chu·∫©n b·ªã d·ªØ li·ªáu vƒÉn b·∫£n**: gh√©p c√°c thu·ªôc t√≠nh `category`, `gender`, `color`, `style_tags`, `productDisplayName` th√†nh m·ªôt chu·ªói vƒÉn b·∫£n m√¥ t·∫£ ƒë·∫ßy ƒë·ªß s·∫£n ph·∫©m
    3. **T·∫°o ma tr·∫≠n TF-IDF**: s·ª≠ d·ª•ng `TfidfVectorizer` ƒë·ªÉ t·∫°o ma tr·∫≠n TF-IDF (Term Frequency-Inverse Document Frequency) - ƒë√°nh gi√° t·∫ßm quan tr·ªçng c·ªßa t·ª´ trong m√¥ t·∫£ s·∫£n ph·∫©m
    4. **T√≠nh cosine similarity** gi·ªØa c√°c s·∫£n ph·∫©m (SBERT embeddings).  
       - Recommendation score = cosine similarity (ƒë·ªô t∆∞∆°ng t·ª± cosine) gi·ªØa product embeddings, gi√° tr·ªã t·ª´ 0-1, c√†ng g·∫ßn 1 th√¨ s·∫£n ph·∫©m c√†ng gi·ªëng nhau v·ªÅ ƒë·∫∑c ƒëi·ªÉm
    5. **T√≠nh to√°n ch·ªâ s·ªë ƒë√°nh gi√°**: Recall@10, Recall@20, NDCG@10, NDCG@20, th·ªùi gian train, th·ªùi gian inference.
       - *Recall@10*: Trong 10 m√≥n b·∫°n g·ª£i √Ω, c√≥ bao nhi√™u m√≥n user th·ª±c s·ª± th√≠ch (trong test set)? C√†ng cao c√†ng t·ªët (0-1)
       - *Recall@20*: T∆∞∆°ng t·ª± nh∆∞ng top 20. C√†ng cao c√†ng t·ªët (0-1)
       - *NDCG@10*: Top 10 c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë√∫ng m√† c√≤n s·∫Øp x·∫øp ƒë√∫ng th·ª© t·ª± (m√≥n user th√≠ch nh·∫•t ƒë·ª©ng cao). C√†ng cao c√†ng t·ªët (0-1)
       - *NDCG@20*: T∆∞∆°ng t·ª± top 20. C√†ng cao c√†ng t·ªët (0-1)
       - *Th·ªùi gian train*: M·∫•t bao l√¢u ƒë·ªÉ train xong 1 l·∫ßn ({metrics.get('training_time', 'N/A')}) - c√†ng th·∫•p c√†ng t·ªët
       - *Th·ªùi gian inference/user*: M·∫•t bao l√¢u ƒë·ªÉ tr·∫£ v·ªÅ g·ª£i √Ω cho 1 user ({metrics.get('inference_time', 'N/A')} ms) - c√†ng th·∫•p c√†ng t·ªët (r·∫•t quan tr·ªçng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Th·ªùi gian train | Th·ªùi gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| Content-based Filtering | {metrics.get('recall_at_10', 'N/A')} | {metrics.get('recall_at_20', 'N/A')} | {metrics.get('ndcg_at_10', 'N/A')} | {metrics.get('ndcg_at_20', 'N/A')} | {metrics.get('training_time', 'N/A')} | {metrics.get('inference_time', 'N/A')} ms |
"""
    return doc


def generate_hybrid_documentation(metrics: Dict[str, Any], alpha: float = 0.7) -> str:
    """Generate Hybrid documentation markdown with metrics."""
    doc = f"""### 2.3.3. Hybrid GNN (LightGCN) & Content-based Filtering

- **Quy tr√¨nh th·ª±c hi·ªán**:
  - *Chu·∫©n h√≥a d·ªØ li·ªáu v·ªõi Surprise*:  
    S·ª≠ d·ª•ng `surprise.Dataset.load_from_df(...)` v√† `train_test_split(test_size={metrics['test_size']})` ƒë·ªÉ chia d·ªØ li·ªáu th√†nh t·∫≠p hu·∫•n luy·ªán v√† t·∫≠p ki·ªÉm th·ª≠.  
    - Test size: **{metrics['test_size']}** (t·ª∑ l·ªá d·ªØ li·ªáu d√πng ƒë·ªÉ ki·ªÉm th·ª≠, ph·∫ßn c√≤n l·∫°i d√πng ƒë·ªÉ hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng train: **{metrics['num_users']}** (s·ªë ng∆∞·ªùi d√πng trong t·∫≠p hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng s·∫£n ph·∫©m train: **{metrics['num_products']}** (s·ªë s·∫£n ph·∫©m trong t·∫≠p hu·∫•n luy·ªán)
    - S·ªë l∆∞·ª£ng t∆∞∆°ng t√°c (interactions): **{metrics['num_interactions']}** (t·ªïng s·ªë l∆∞·ª£t t∆∞∆°ng t√°c gi·ªØa ng∆∞·ªùi d√πng v√† s·∫£n ph·∫©m)
  - *Pipeline 5 b∆∞·ªõc*:
    1. **Hu·∫•n luy·ªán m√¥ h√¨nh**: K·∫øt h·ª£p GNN (LightGCN) + CBF (Sentence-BERT + FAISS).
       - GNN component: LightGCN v·ªõi embedding dimension **{metrics['embed_dim']}** - h·ªçc t·ª´ h√†nh vi t∆∞∆°ng t√°c c·ªßa ng∆∞·ªùi d√πng th√¥ng qua Graph Neural Network
       - CBF component: Sentence-BERT + FAISS index - h·ªçc t·ª´ ƒë·∫∑c ƒëi·ªÉm n·ªôi dung s·∫£n ph·∫©m th√¥ng qua semantic embeddings
       - Tr·ªçng s·ªë k·∫øt h·ª£p: `alpha = {alpha}` (GNN weight = {alpha}, CBF weight = {1-alpha:.1f}) - alpha c√†ng cao th√¨ c√†ng ∆∞u ti√™n h√†nh vi ng∆∞·ªùi d√πng (GNN), c√†ng th·∫•p th√¨ c√†ng ∆∞u ti√™n ƒë·∫∑c ƒëi·ªÉm s·∫£n ph·∫©m (CBF)
    2. **Chu·∫©n b·ªã d·ªØ li·ªáu**: 
       - K·∫øt h·ª£p embedding t·ª´ GNN (LightGCN) v√† Content-based Filtering (Sentence-BERT + FAISS)
       - User embeddings t·ª´ GNN (LightGCN): `[{metrics['num_users']}, {metrics['embed_dim']}]` - {metrics['num_users']} vector ng∆∞·ªùi d√πng, m·ªói vector {metrics['embed_dim']} chi·ªÅu, h·ªçc t·ª´ ƒë·ªì th·ªã t∆∞∆°ng t√°c
       - Product embeddings t·ª´ CBF (Sentence-BERT): `[{metrics['num_products']}, {metrics['embed_dim']}]` - {metrics['num_products']} vector s·∫£n ph·∫©m, m·ªói vector {metrics['embed_dim']} chi·ªÅu, h·ªçc t·ª´ m√¥ t·∫£ s·∫£n ph·∫©m
    3. **T√≠nh to√°n similarity**: 
       - GNN similarity: cosine similarity gi·ªØa user embedding (LightGCN) v√† product embedding (LightGCN) - d·ª±a tr√™n h√†nh vi ng∆∞·ªùi d√πng t∆∞∆°ng t·ª± trong ƒë·ªì th·ªã t∆∞∆°ng t√°c
       - CBF similarity: cosine similarity gi·ªØa product embeddings (Sentence-BERT) - d·ª±a tr√™n ƒë·∫∑c ƒëi·ªÉm s·∫£n ph·∫©m t∆∞∆°ng t·ª± v·ªÅ ng·ªØ nghƒ©a
       - Final score = `{alpha} * GNN_score + {1-alpha:.1f} * CBF_score` - k·∫øt h·ª£p hai ngu·ªìn th√¥ng tin v·ªõi tr·ªçng s·ªë
    4. **K·∫øt h·ª£p tr·ªçng s·ªë**: 
       - B·∫£ng similarity t·ª´ CBF (Sentence-BERT + FAISS) ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng t·ª± n·ªôi dung, c·ªông th√™m tr·ªçng s·ªë GNN (LightGCN) ƒë√°nh gi√° ƒë·ªô t∆∞∆°ng t·ª± h√†nh vi trong ƒë·ªì th·ªã
    5. **T√≠nh to√°n ch·ªâ s·ªë ƒë√°nh gi√°**: Recall@10, Recall@20, NDCG@10, NDCG@20, th·ªùi gian train, th·ªùi gian inference.
       - *Recall@10*: Trong 10 m√≥n b·∫°n g·ª£i √Ω, c√≥ bao nhi√™u m√≥n user th·ª±c s·ª± th√≠ch (trong test set)? C√†ng cao c√†ng t·ªët (0-1)
       - *Recall@20*: T∆∞∆°ng t·ª± nh∆∞ng top 20. C√†ng cao c√†ng t·ªët (0-1)
       - *NDCG@10*: Top 10 c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë√∫ng m√† c√≤n s·∫Øp x·∫øp ƒë√∫ng th·ª© t·ª± (m√≥n user th√≠ch nh·∫•t ƒë·ª©ng cao). C√†ng cao c√†ng t·ªët (0-1)
       - *NDCG@20*: T∆∞∆°ng t·ª± top 20. C√†ng cao c√†ng t·ªët (0-1)
       - *Th·ªùi gian train*: M·∫•t bao l√¢u ƒë·ªÉ train xong 1 l·∫ßn ({metrics.get('training_time', 'N/A')}) - c√†ng th·∫•p c√†ng t·ªët
       - *Th·ªùi gian inference/user*: M·∫•t bao l√¢u ƒë·ªÉ tr·∫£ v·ªÅ g·ª£i √Ω cho 1 user ({metrics.get('inference_time', 'N/A')} ms) - c√†ng th·∫•p c√†ng t·ªët (r·∫•t quan tr·ªçng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Th·ªùi gian train | Th·ªùi gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| Hybrid GNN+CBF | {metrics.get('recall_at_10', 'N/A')} | {metrics.get('recall_at_20', 'N/A')} | {metrics.get('ndcg_at_10', 'N/A')} | {metrics.get('ndcg_at_20', 'N/A')} | {metrics.get('training_time', 'N/A')} | {metrics.get('inference_time', 'N/A')} ms |
"""
    return doc


def generate_comparison_table(
    gnn_metrics: Dict[str, Any],
    cbf_metrics: Dict[str, Any],
    hybrid_metrics: Dict[str, Any],
    analysis_text: str,
) -> str:
    """Generate comparison table for all 3 models."""
    doc = """
**Gi·∫£i th√≠ch c√°c ch·ªâ s·ªë:**
- **Recall@10** (0-1): Trong 10 m√≥n b·∫°n g·ª£i √Ω, c√≥ bao nhi√™u m√≥n user th·ª±c s·ª± th√≠ch (trong test set)? C√†ng cao c√†ng t·ªët
- **Recall@20** (0-1): T∆∞∆°ng t·ª± nh∆∞ng top 20. C√†ng cao c√†ng t·ªët
- **NDCG@10** (0-1): Top 10 c·ªßa b·∫°n kh√¥ng ch·ªâ ƒë√∫ng m√† c√≤n s·∫Øp x·∫øp ƒë√∫ng th·ª© t·ª± (m√≥n user th√≠ch nh·∫•t ƒë·ª©ng cao). C√†ng cao c√†ng t·ªët
- **NDCG@20** (0-1): T∆∞∆°ng t·ª± top 20. C√†ng cao c√†ng t·ªët
- **Th·ªùi gian train**: M·∫•t bao l√¢u ƒë·ªÉ train xong 1 l·∫ßn (th∆∞·ªùng t√≠nh b·∫±ng ph√∫t/gi·ªù) - c√†ng th·∫•p c√†ng t·ªët
- **Th·ªùi gian inference/user**: M·∫•t bao l√¢u ƒë·ªÉ tr·∫£ v·ªÅ g·ª£i √Ω cho 1 user (th∆∞·ªùng t√≠nh b·∫±ng ms) - c√†ng th·∫•p c√†ng t·ªët (r·∫•t quan tr·ªçng trong production)

| Model | Recall@10 | Recall@20 | NDCG@10 | NDCG@20 | Th·ªùi gian train | Th·ªùi gian inference/user |
|-------|-----------|-----------|---------|---------|----------------|------------------------|
| GNN (LightGCN) | {gnn_recall_10} | {gnn_recall_20} | {gnn_ndcg_10} | {gnn_ndcg_20} | {gnn_train_time} | {gnn_inference_time} |
| Content-based Filtering | {cbf_recall_10} | {cbf_recall_20} | {cbf_ndcg_10} | {cbf_ndcg_20} | {cbf_train_time} | {cbf_inference_time} |
| Hybrid GNN+CBF | {hybrid_recall_10} | {hybrid_recall_20} | {hybrid_ndcg_10} | {hybrid_ndcg_20} | {hybrid_train_time} | {hybrid_inference_time} |

{analysis_section}
""".format(
        gnn_recall_10=gnn_metrics.get('recall_at_10', 'N/A'),
        gnn_recall_20=gnn_metrics.get('recall_at_20', 'N/A'),
        gnn_ndcg_10=gnn_metrics.get('ndcg_at_10', 'N/A'),
        gnn_ndcg_20=gnn_metrics.get('ndcg_at_20', 'N/A'),
        gnn_train_time=gnn_metrics.get('training_time', 'N/A'),
        gnn_inference_time=f"{gnn_metrics.get('inference_time', 'N/A')} ms" if gnn_metrics.get('inference_time', 'N/A') != 'N/A' else 'N/A',
        cbf_recall_10=cbf_metrics.get('recall_at_10', 'N/A'),
        cbf_recall_20=cbf_metrics.get('recall_at_20', 'N/A'),
        cbf_ndcg_10=cbf_metrics.get('ndcg_at_10', 'N/A'),
        cbf_ndcg_20=cbf_metrics.get('ndcg_at_20', 'N/A'),
        cbf_train_time=cbf_metrics.get('training_time', 'N/A'),
        cbf_inference_time=f"{cbf_metrics.get('inference_time', 'N/A')} ms" if cbf_metrics.get('inference_time', 'N/A') != 'N/A' else 'N/A',
        hybrid_recall_10=hybrid_metrics.get('recall_at_10', 'N/A'),
        hybrid_recall_20=hybrid_metrics.get('recall_at_20', 'N/A'),
        hybrid_ndcg_10=hybrid_metrics.get('ndcg_at_10', 'N/A'),
        hybrid_ndcg_20=hybrid_metrics.get('ndcg_at_20', 'N/A'),
        hybrid_train_time=hybrid_metrics.get('training_time', 'N/A'),
        hybrid_inference_time=f"{hybrid_metrics.get('inference_time', 'N/A')} ms" if hybrid_metrics.get('inference_time', 'N/A') != 'N/A' else 'N/A',
        analysis_section=analysis_text.replace("{", "{{").replace("}", "}}"),
    )
    return doc


# 3.1 Apply formulas locally to compute metrics
st.header("3.1 √Åp d·ª•ng c√¥ng th·ª©c (t√≠nh c·ª•c b·ªô)")
st.caption("T√≠nh Recall@K, NDCG@K d·ª±a tr√™n danh s√°ch g·ª£i √Ω tr·∫£ v·ªÅ v√† Ground Truth l·∫•y t·ª´ l·ªãch s·ª≠ t∆∞∆°ng t√°c c·ªßa user. D√πng ch√≠nh c√¥ng th·ª©c ƒë√£ tr√¨nh b√†y ƒë·ªÉ ki·ªÉm ch·ª©ng.")

with st.expander("üî¨ T√≠nh Recall/NDCG c·ª•c b·ªô t·ª´ k·∫øt qu·∫£ recommend"):
    uid_local = st.text_input("User ID (local)", value=user_id, key="local_user_id")
    pid_local = st.text_input("Current Product ID (local)", value=product_id, key="local_product_id")
    k_values = st.multiselect("Ch·ªçn K ƒë·ªÉ t√≠nh", options=[5, 10, 20, 50], default=[10, 20])
    model_choices = st.multiselect("Ch·ªçn m√¥ h√¨nh", options=[("GNN","gnn"), ("CBF","cbf"), ("Hybrid","hybrid")], format_func=lambda x: x[0], default=[("GNN","gnn"), ("CBF","cbf"), ("Hybrid","hybrid")])

    def _extract_rec_ids(recommend_data: Dict[str, Any]) -> list:
        recs = recommend_data.get("personalized") or recommend_data.get("recommendations") or []
        rec_ids = []
        for rec in recs:
            rid = None
            if isinstance(rec, dict):
                # nested product object or flat id
                prod = rec.get("product")
                if isinstance(prod, dict):
                    rid = prod.get("id") or prod.get("product_id")
                rid = rid or rec.get("id") or rec.get("product_id")
            else:
                rid = rec
            if rid is not None:
                rec_ids.append(str(rid))
        # unique and keep order
        seen = set()
        ordered = []
        for rid in rec_ids:
            if rid not in seen:
                seen.add(rid)
                ordered.append(rid)
        return ordered

    def _fetch_ground_truth_ids(base_url: str, uid: str, exclude_pid: str) -> list:
        try:
            resp = requests.get(f"{base_url.rstrip('/')}/users/{uid}", timeout=15)
            if resp.status_code == 200:
                payload = resp.json()
                user_info = (payload.get("data") or {}).get("user") or {}
                history = user_info.get("interaction_history") or []
                gt_ids = []
                for it in history:
                    pid = it.get("product_id")
                    if pid is None:
                        continue
                    pid = str(pid)
                    if exclude_pid and pid == str(exclude_pid):
                        continue
                    gt_ids.append(pid)
                # unique
                gt_ids = list(dict.fromkeys(gt_ids))
                return gt_ids
        except Exception:
            pass
        return []

    if st.button("‚ñ∂Ô∏è T√≠nh to√°n c·ª•c b·ªô", key="btn_compute_local"):
        if not uid_local:
            st.warning("Vui l√≤ng nh·∫≠p User ID")
        else:
            gt_ids = _fetch_ground_truth_ids(BASE_URL, uid_local, pid_local)
            if not gt_ids:
                st.warning("Kh√¥ng l·∫•y ƒë∆∞·ª£c Ground Truth t·ª´ interaction_history c·ªßa user. H√£y ƒë·∫£m b·∫£o backend tr·∫£ v·ªÅ /users/{id} c√≥ interaction_history.")
            else:
                st.success(f"ƒê√£ l·∫•y {len(gt_ids)} Ground Truth items t·ª´ l·ªãch s·ª≠ user")
                cols = st.columns(len(model_choices) or 1)
                for col, (label, slug) in zip(cols, model_choices):
                    with col:
                        st.markdown(f"#### {label}")
                        payload_local = {"user_id": uid_local, "current_product_id": pid_local}
                        t0 = time.perf_counter()
                        res = call_api(BASE_URL, f"{slug}/recommend", payload=payload_local)
                        t1 = time.perf_counter()
                        if not res["success"]:
                            st.error(res.get("error", "Recommend API l·ªói"))
                            continue
                        data = res["data"] if isinstance(res["data"], dict) else {}
                        rec_ids = _extract_rec_ids(data)
                        if not rec_ids:
                            st.warning("Kh√¥ng c√≥ danh s√°ch g·ª£i √Ω ƒë·ªÉ t√≠nh to√°n.")
                            continue

                        # Compute metrics locally
                        for k in k_values:
                            recall_k = compute_recall_at_k(rec_ids, gt_ids, k=k)
                            ndcg_k = compute_ndcg_at_k(rec_ids, gt_ids, k=k)
                            st.metric(f"Recall@{k} (local)", f"{recall_k:.4f}")
                            st.metric(f"NDCG@{k} (local)", f"{ndcg_k:.4f}")
                        # Compare to API's evaluation_metrics if present
                        api_eval = data.get("evaluation_metrics", {}) if isinstance(data, dict) else {}
                        if api_eval:
                            with st.expander("So s√°nh v·ªõi evaluation_metrics API"):
                                st.json(api_eval)
                        inf_ms = (t1 - t0) * 1000.0
                        st.metric("Inference time (local)", f"{inf_ms:.2f} ms")

# 3.2 Batch evaluation using API-provided test cases
st.header("3.2 ƒê√°nh gi√° theo b·ªô test (t·ª´ API)")
st.caption("S·ª≠ d·ª•ng danh s√°ch user_id/product_id m√† API tr·∫£ v·ªÅ trong evaluation_support ƒë·ªÉ ch·∫°y recommend theo l√¥, √°p d·ª•ng c√¥ng th·ª©c Recall@K v√† NDCG@K, r·ªìi t·ªïng h·ª£p k·∫øt qu·∫£.")

with st.expander("üß™ Ch·∫°y ƒë√°nh gi√° theo evaluation_support"):
    # Show availability per model
    col_av1, col_av2, col_av3 = st.columns(3)
    for c, slug, label in zip([col_av1, col_av2, col_av3], ["gnn", "cbf", "hybrid"], ["GNN", "CBF", "Hybrid"]):
        with c:
            es = st.session_state.evaluation_support.get(slug)
            if es:
                num_pairs = len(es.get("pairs") or [])
                num_u = len(es.get("user_ids") or [])
                num_p = len(es.get("product_ids") or [])
                st.success(f"{label}: pairs={num_pairs}, user_ids={num_u}, product_ids={num_p}")
            else:
                st.warning(f"{label}: Ch∆∞a c√≥ evaluation_support t·ª´ API")

    # Controls
    model_opts = st.multiselect(
        "Ch·ªçn m√¥ h√¨nh ƒë·ªÉ ƒë√°nh gi√°",
        options=[("GNN", "gnn"), ("CBF", "cbf"), ("Hybrid", "hybrid")],
        format_func=lambda x: x[0],
        default=[("GNN", "gnn"), ("CBF", "cbf"), ("Hybrid", "hybrid")]
    )
    ks = st.multiselect("Ch·ªçn K", options=[5, 10, 20, 50], default=[10, 20])
    max_pairs = st.number_input("Gi·ªõi h·∫°n s·ªë c·∫∑p test/pairs", min_value=1, max_value=1000, value=50, step=5)

    def _get_eval_pairs(slug: str, limit: int) -> list:
        es = st.session_state.evaluation_support.get(slug) or {}
        pairs = es.get("pairs") or []
        if not pairs:
            # fallback: build pairs from user_ids x product_ids (c·∫Øt m·∫´u ƒë·ªÉ tr√°nh n·ªï t·ªï h·ª£p)
            uids = es.get("user_ids") or []
            pids = es.get("product_ids") or []
            built = []
            for i, uid in enumerate(uids):
                if len(built) >= limit:
                    break
                for j, pid in enumerate(pids):
                    built.append({"user_id": str(uid), "current_product_id": str(pid)})
                    if len(built) >= limit:
                        break
            pairs = built
        return pairs[:limit]

    def _extract_rec_ids(recommend_data: Dict[str, Any]) -> list:
        recs = recommend_data.get("personalized") or recommend_data.get("recommendations") or []
        rec_ids = []
        for rec in recs:
            rid = None
            if isinstance(rec, dict):
                prod = rec.get("product")
                if isinstance(prod, dict):
                    rid = prod.get("id") or prod.get("product_id")
                rid = rid or rec.get("id") or rec.get("product_id")
            else:
                rid = rec
            if rid is not None:
                rec_ids.append(str(rid))
        # unique ordered
        seen, ordered = set(), []
        for rid in rec_ids:
            if rid not in seen:
                seen.add(rid)
                ordered.append(rid)
        return ordered

    GT_CACHE: Dict[str, list] = {}

    def _get_gt(uid: str, exclude_pid: Optional[str]) -> list:
        if uid in GT_CACHE:
            gt = GT_CACHE[uid]
        else:
            gt = []
            try:
                resp = requests.get(f"{BASE_URL.rstrip('/')}/users/{uid}", timeout=15)
                if resp.status_code == 200:
                    payload = resp.json()
                    user_info = (payload.get("data") or {}).get("user") or {}
                    history = user_info.get("interaction_history") or []
                    for it in history:
                        pid = it.get("product_id")
                        if pid is None:
                            continue
                        gt.append(str(pid))
                    gt = list(dict.fromkeys(gt))
            except Exception:
                pass
            GT_CACHE[uid] = gt
        if exclude_pid:
            return [x for x in gt if x != str(exclude_pid)]
        return gt

    if st.button("‚ñ∂Ô∏è Ch·∫°y ƒë√°nh gi√° theo b·ªô test", key="btn_run_eval_support"):
        if not model_opts:
            st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt m√¥ h√¨nh")
        elif not ks:
            st.warning("Vui l√≤ng ch·ªçn √≠t nh·∫•t m·ªôt K")
        else:
            for label, slug in model_opts:
                st.markdown(f"#### K·∫øt qu·∫£ - {label}")
                pairs = _get_eval_pairs(slug, int(max_pairs))
                if not pairs:
                    st.warning("Kh√¥ng c√≥ c·∫∑p test t·ª´ evaluation_support.")
                    continue
                prog = st.progress(0)
                rows = []
                sum_recalls = {k: 0.0 for k in ks}
                sum_ndcgs = {k: 0.0 for k in ks}
                total = len(pairs)
                total_time_ms = 0.0
                for idx, pair in enumerate(pairs, start=1):
                    uid = pair.get("user_id")
                    pid = pair.get("current_product_id")
                    if not uid:
                        continue
                    gt_ids = _get_gt(uid, pid)
                    t0 = time.perf_counter()
                    res = call_api(BASE_URL, f"{slug}/recommend", payload=pair)
                    t1 = time.perf_counter()
                    if not res["success"]:
                        rows.append({"user_id": uid, "product_id": pid, "ok": False, "error": res.get("error")})
                        prog.progress(min(idx/total, 1.0))
                        continue
                    data = res["data"] if isinstance(res["data"], dict) else {}
                    rec_ids = _extract_rec_ids(data)
                    pair_row = {"user_id": uid, "product_id": pid, "ok": True}
                    for k in ks:
                        r = compute_recall_at_k(rec_ids, gt_ids, k=k)
                        n = compute_ndcg_at_k(rec_ids, gt_ids, k=k)
                        sum_recalls[k] += r
                        sum_ndcgs[k] += n
                        pair_row[f"recall@{k}"] = round(r, 4)
                        pair_row[f"ndcg@{k}"] = round(n, 4)
                    inf_ms = (t1 - t0) * 1000.0
                    total_time_ms += inf_ms
                    pair_row["inference_ms"] = round(inf_ms, 2)
                    rows.append(pair_row)
                    prog.progress(min(idx/total, 1.0))

                # Aggregate
                agg_cols = st.columns(len(ks) * 2 + 1)
                cidx = 0
                for k in ks:
                    with agg_cols[cidx]:
                        st.metric(f"Recall@{k} (avg)", f"{(sum_recalls[k]/total):.4f}")
                    cidx += 1
                    with agg_cols[cidx]:
                        st.metric(f"NDCG@{k} (avg)", f"{(sum_ndcgs[k]/total):.4f}")
                    cidx += 1
                with agg_cols[cidx]:
                    st.metric("Inference (avg)", f"{(total_time_ms/max(total,1)):.2f} ms")
                st.dataframe(pd.DataFrame(rows), use_container_width=True)

st.header("4. T√†i li·ªáu m√¥ h√¨nh (Documentation)")

st.markdown("""
**üìå Ngu·ªìn d·ªØ li·ªáu cho t√†i li·ªáu:**

- **T·ª´ API `/train`**: Th√¥ng s·ªë hu·∫•n luy·ªán (num_users, num_products, epochs, batch_size, embed_dim, learning_rate, etc.)
- **T·ª´ API `/recommend`**: Ch·ªâ s·ªë ƒë√°nh gi√° (MAPE, RMSE, Precision, Recall, F1, execution_time) trong `evaluation_metrics`

**üí° L∆∞u √Ω**: ƒê·ªÉ c√≥ ƒë·∫ßy ƒë·ªß s·ªë li·ªáu, b·∫°n c·∫ßn:
1. Train m√¥ h√¨nh qua API `/train` ‚Üí L·∫•y th√¥ng s·ªë hu·∫•n luy·ªán
2. G·ªçi API `/recommend` ‚Üí L·∫•y evaluation metrics
""")

GROQ_MODEL_NAME = "llama-3.3-70b-versatile"
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"


def call_groq_api(prompt: str, system_message: str = "", max_tokens: int = 2000, temperature: float = 0.3) -> str:
    """Call Groq API with given prompt."""
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        return (
            "**‚ö†Ô∏è Groq ch∆∞a s·∫µn s√†ng**: Vui l√≤ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng `GROQ_API_KEY` "
            "ƒë·ªÉ b·∫≠t ph√¢n t√≠ch t·ª± ƒë·ªông."
        )
    
    default_system = "You are a helpful data scientist specializing in recommender systems. Always respond in Markdown and Vietnamese."
    
    payload = {
        "model": GROQ_MODEL_NAME,
        "messages": [
            {
                "role": "system",
                "content": system_message or default_system,
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    
    try:
        response = requests.post(
            GROQ_API_URL,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json=payload,
            timeout=120,
        )
        response.raise_for_status()
        data = response.json()
        content = (
            data.get("choices", [{}])[0]
            .get("message", {})
            .get("content", "")
            .strip()
        )
        if not content:
            raise ValueError("Groq response empty.")
        return content
    except (requests.RequestException, ValueError, KeyError) as exc:
        return f"**‚ö†Ô∏è Groq l·ªói**: {exc}"


def analyze_metrics_detailed(
    gnn_metrics: Dict[str, Any],
    cbf_metrics: Dict[str, Any],
    hybrid_metrics: Dict[str, Any],
) -> str:
    """Use Groq to provide detailed explanation of metrics and model selection."""
    metrics_snapshot = {
        "GNN (LightGCN)": gnn_metrics,
        "Content-based Filtering": cbf_metrics,
        "Hybrid GNN+CBF": hybrid_metrics,
    }
    
    prompt = f"""B·∫°n l√† chuy√™n gia v·ªÅ h·ªá th·ªëng g·ª£i √Ω (Recommender Systems). 
D·ª±a v√†o s·ªë li·ªáu th·ª±c nghi·ªám d∆∞·ªõi ƒë√¢y, h√£y:

1. **Gi·∫£i th√≠ch chi ti·∫øt t·ª´ng ch·ªâ s·ªë:**
   - Recall@10, Recall@20: √ù nghƒ©a l√† g√¨? Gi√° tr·ªã bao nhi√™u l√† t·ªët?
   - NDCG@10, NDCG@20: Kh√°c g√¨ v·ªõi Recall? T·∫°i sao c·∫ßn c·∫£ hai?
   - Th·ªùi gian train vs inference: T·∫°i sao c·∫£ hai ƒë·ªÅu quan tr·ªçng?

2. **So s√°nh 3 m√¥ h√¨nh:**
   - M√¥ h√¨nh n√†o c√≥ Recall/NDCG cao nh·∫•t?
   - M√¥ h√¨nh n√†o train nhanh nh·∫•t?
   - M√¥ h√¨nh n√†o inference nhanh nh·∫•t (quan tr·ªçng cho production)?
   - M√¥ h√¨nh n√†o c√¢n b·∫±ng t·ªët nh·∫•t gi·ªØa ƒë·ªô ch√≠nh x√°c v√† t·ªëc ƒë·ªô?

3. **Khuy·∫øn ngh·ªã:**
   - Ch·ªçn m√¥ h√¨nh n√†o ƒë·ªÉ tri·ªÉn khai production? T·∫°i sao?
   - Trong tr∆∞·ªùng h·ª£p n√†o n√™n d√πng m√¥ h√¨nh kh√°c?
   - C√≥ c√°ch n√†o c·∫£i thi·ªán m√¥ h√¨nh ƒë∆∞·ª£c ch·ªçn kh√¥ng?

**S·ªë li·ªáu th·ª±c nghi·ªám:**
{json.dumps(metrics_snapshot, ensure_ascii=False, indent=2)}

Vi·∫øt chi ti·∫øt, d·ªÖ hi·ªÉu, c√≥ v√≠ d·ª• c·ª• th·ªÉ. S·ª≠ d·ª•ng ti·∫øng Vi·ªát."""

    return call_groq_api(prompt, max_tokens=3000, temperature=0.2)


def explain_algorithms_detailed(
    gnn_metrics: Dict[str, Any],
    cbf_metrics: Dict[str, Any],
    hybrid_metrics: Dict[str, Any],
) -> str:
    """Use Groq to explain algorithms in detail with formulas and step-by-step process."""
    metrics_snapshot = {
        "GNN": gnn_metrics,
        "CBF": cbf_metrics,
        "Hybrid": hybrid_metrics,
    }
    
    prompt = f"""B·∫°n l√† chuy√™n gia Machine Learning v√† Recommender Systems.
H√£y tr√¨nh b√†y chi ti·∫øt thu·∫≠t to√°n c·ªßa 3 m√¥ h√¨nh sau v·ªõi:

1. **GNN (LightGCN)**
   - C√¥ng th·ª©c to√°n h·ªçc t·ª´ng b∆∞·ªõc (d√πng k√Ω hi·ªáu to√°n h·ªçc chu·∫©n)
   - Gi·∫£i th√≠ch √Ω nghƒ©a c·ªßa t·ª´ng bi·∫øn
   - Qu√° tr√¨nh t√≠nh to√°n: User embedding ‚Üí Product embedding ‚Üí Similarity score ‚Üí Ranking
   - T·∫°i sao d√πng Graph Neural Network?
   - ∆Øu ƒëi·ªÉm: H·ªçc ƒë∆∞·ª£c m·ªëi quan h·ªá gi·ªØa users v√† items t·ª´ ƒë·ªì th·ªã t∆∞∆°ng t√°c
   - Nh∆∞·ª£c ƒëi·ªÉm: C·∫ßn d·ªØ li·ªáu t∆∞∆°ng t√°c ƒë·ªß l·ªõn

2. **Content-based Filtering (CBF)**
   - C√¥ng th·ª©c to√°n h·ªçc t·ª´ng b∆∞·ªõc
   - Gi·∫£i th√≠ch Sentence-BERT embeddings
   - C√¥ng th·ª©c t√≠nh cosine similarity
   - Qu√° tr√¨nh: Text ‚Üí SBERT embedding ‚Üí Similarity matrix ‚Üí Ranking
   - T·∫°i sao d√πng Content-based?
   - ∆Øu ƒëi·ªÉm: Kh√¥ng c·∫ßn d·ªØ li·ªáu t∆∞∆°ng t√°c, c√≥ th·ªÉ recommend s·∫£n ph·∫©m m·ªõi
   - Nh∆∞·ª£c ƒëi·ªÉm: Kh√¥ng h·ªçc ƒë∆∞·ª£c preference c·ªßa user

3. **Hybrid GNN+CBF**
   - C√¥ng th·ª©c k·∫øt h·ª£p: Score = Œ± √ó GNN_score + (1-Œ±) √ó CBF_score
   - T·∫°i sao k·∫øt h·ª£p hai m√¥ h√¨nh?
   - ∆Øu ƒëi·ªÉm: K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa c·∫£ hai
   - Nh∆∞·ª£c ƒëi·ªÉm: Ph·ª©c t·∫°p h∆°n, c·∫ßn tune Œ±

**Th√¥ng s·ªë t·ª´ th·ª±c nghi·ªám:**
{json.dumps(metrics_snapshot, ensure_ascii=False, indent=2)}

Vi·∫øt r·∫•t chi ti·∫øt, c√≥ c√¥ng th·ª©c to√°n h·ªçc r√µ r√†ng, d·ªÖ hi·ªÉu. S·ª≠ d·ª•ng ti·∫øng Vi·ªát."""

    return call_groq_api(prompt, max_tokens=4000, temperature=0.2)


def explain_personalized_vs_outfit(
    gnn_metrics: Dict[str, Any],
    cbf_metrics: Dict[str, Any],
    hybrid_metrics: Dict[str, Any],
) -> str:
    """Use Groq to explain Personalized vs Outfit recommendation methodologies."""
    metrics_snapshot = {
        "GNN": gnn_metrics,
        "CBF": cbf_metrics,
        "Hybrid": hybrid_metrics,
    }
    
    prompt = f"""B·∫°n l√† chuy√™n gia v·ªÅ Personalized Recommendation v√† Outfit Recommendation.
H√£y tr√¨nh b√†y chi ti·∫øt hai ph∆∞∆°ng ph√°p n√†y:

1. **PERSONALIZED RECOMMENDATION (G·ª£i √Ω c√° nh√¢n h√≥a)**
   - ƒê·ªãnh nghƒ©a: G·ª£i √Ω d·ª±a tr√™n h√†nh vi v√† s·ªü th√≠ch c√° nh√¢n c·ªßa t·ª´ng user
   - T·ªï ch·ª©c d·ªØ li·ªáu:
     * User-Item interaction matrix: [num_users √ó num_items]
     * M·ªói ph·∫ßn t·ª≠ = rating/weight c·ªßa user ƒë·ªëi v·ªõi item
     * V√≠ d·ª•: User 1 mua √°o s∆° mi ‚Üí weight = 3.0
   - Qu√° tr√¨nh t√≠nh to√°n:
     * B∆∞·ªõc 1: X√¢y d·ª±ng user embedding t·ª´ interaction history
     * B∆∞·ªõc 2: T√≠nh similarity gi·ªØa user embedding v√† item embeddings
     * B∆∞·ªõc 3: Rank items theo similarity score
     * B∆∞·ªõc 4: Tr·∫£ v·ªÅ top-K items cao nh·∫•t
   - C√¥ng th·ª©c: Score(user_i, item_j) = similarity(user_embedding_i, item_embedding_j)
   - ·ª®ng d·ª•ng: Amazon, Netflix, Spotify (m·ªói user c√≥ g·ª£i √Ω kh√°c nhau)

2. **OUTFIT RECOMMENDATION (G·ª£i √Ω trang ph·ª•c/b·ªô s∆∞u t·∫≠p)**
   - ƒê·ªãnh nghƒ©a: G·ª£i √Ω c√°c s·∫£n ph·∫©m ph·ªëi h·ª£p t·ªët v·ªõi nhau (√°o + qu·∫ßn + gi√†y)
   - T·ªï ch·ª©c d·ªØ li·ªáu:
     * Item-Item similarity matrix: [num_items √ó num_items]
     * M·ªói ph·∫ßn t·ª≠ = ƒë·ªô t∆∞∆°ng t·ª± gi·ªØa hai items
     * V√≠ d·ª•: √Åo s∆° mi xanh + Qu·∫ßn jeans xanh ‚Üí similarity = 0.85
   - Qu√° tr√¨nh t√≠nh to√°n:
     * B∆∞·ªõc 1: T√≠nh item embeddings t·ª´ content (m√†u, ki·ªÉu, ch·∫•t li·ªáu)
     * B∆∞·ªõc 2: T√≠nh similarity gi·ªØa current_item v√† t·∫•t c·∫£ items kh√°c
     * B∆∞·ªõc 3: Filter items ph√π h·ª£p (c√πng style, m√†u, size)
     * B∆∞·ªõc 4: Rank theo similarity score
     * B∆∞·ªõc 5: Tr·∫£ v·ªÅ top-K items ƒë·ªÉ ph·ªëi h·ª£p
   - C√¥ng th·ª©c: Score(item_i, item_j) = similarity(item_embedding_i, item_embedding_j)
   - ·ª®ng d·ª•ng: Zalora, Tiki, H&M (g·ª£i √Ω s·∫£n ph·∫©m ph·ªëi h·ª£p)

3. **SO S√ÅNH:**
   | Ti√™u ch√≠ | Personalized | Outfit |
   |----------|-------------|--------|
   | D·ªØ li·ªáu input | User ID + Interaction history | Current item ID |
   | D·ªØ li·ªáu t√≠nh to√°n | User-Item matrix | Item-Item similarity matrix |
   | Output | S·∫£n ph·∫©m user th√≠ch | S·∫£n ph·∫©m ph·ªëi h·ª£p t·ªët |
   | ·ª®ng d·ª•ng | Trang ch·ªß, Email | Chi ti·∫øt s·∫£n ph·∫©m, Gi·ªè h√†ng |

4. **TRI·ªÇN KHAI TRONG H·ªÜ TH·ªêNG:**
   - Personalized: D√πng GNN ho·∫∑c Hybrid (h·ªçc t·ª´ user behavior)
   - Outfit: D√πng CBF (h·ªçc t·ª´ item content/features)
   - K·∫øt h·ª£p: Personalized tr√™n trang ch·ªß, Outfit ·ªü chi ti·∫øt s·∫£n ph·∫©m

**Th√¥ng s·ªë t·ª´ th·ª±c nghi·ªám:**
{json.dumps(metrics_snapshot, ensure_ascii=False, indent=2)}

Vi·∫øt r·∫•t chi ti·∫øt, c√≥ v√≠ d·ª• c·ª• th·ªÉ, c√¥ng th·ª©c r√µ r√†ng. S·ª≠ d·ª•ng ti·∫øng Vi·ªát."""

    return call_groq_api(prompt, max_tokens=4000, temperature=0.2)


def analyze_models_with_groq(
    gnn_metrics: Dict[str, Any],
    cbf_metrics: Dict[str, Any],
    hybrid_metrics: Dict[str, Any],
) -> str:
    """Use Groq's Llama model to analyze metrics and produce recommendations."""
    api_key = os.getenv("GROQ_API_KEY")
    if not api_key:
        return (
            "**‚ö†Ô∏è Groq ch∆∞a s·∫µn s√†ng**: Vui l√≤ng ƒë·∫∑t bi·∫øn m√¥i tr∆∞·ªùng `GROQ_API_KEY` "
            "ƒë·ªÉ b·∫≠t ph√¢n t√≠ch t·ª± ƒë·ªông."
        )
    
    metrics_snapshot = {
        "GNN": gnn_metrics,
        "Content-based": cbf_metrics,
        "Hybrid": hybrid_metrics,
    }
    prompt = (
        "B·∫°n l√† chuy√™n gia h·ªá th·ªëng g·ª£i √Ω. D·ª±a v√†o s·ªë li·ªáu Recall@K, NDCG@K, th·ªùi gian train "
        "v√† inference c·ªßa ba m√¥ h√¨nh (GNN, Content-based, Hybrid), h√£y ƒë√°nh gi√° ∆∞u/nh∆∞·ª£c ƒëi·ªÉm "
        "v√† ƒë·ªÅ xu·∫•t m√¥ h√¨nh n√™n tri·ªÉn khai production.\n\n"
        "Y√™u c·∫ßu ƒë·ªãnh d·∫°ng:\n"
        "- B·∫Øt ƒë·∫ßu b·∫±ng ti√™u ƒë·ªÅ in ƒë·∫≠m `Ph√¢n t√≠ch & l·ª±a ch·ªçn`.\n"
        "- Vi·∫øt m·ªói m√¥ h√¨nh m·ªôt g·∫°ch ƒë·∫ßu d√≤ng n√™u r√µ b·ªëi c·∫£nh ph√π h·ª£p v√† ƒëi·ªÉm c·∫ßn ch√∫ √Ω.\n"
        "- K·∫øt th√∫c b·∫±ng m·ªôt g·∫°ch ƒë·∫ßu d√≤ng **K·∫øt lu·∫≠n** n√™u l·ª±a ch·ªçn cu·ªëi c√πng.\n"
        "- Vi·∫øt b·∫±ng ti·∫øng Vi·ªát s√∫c t√≠ch (t·ªëi ƒëa 4 g·∫°ch ƒë·∫ßu d√≤ng cho ph·∫ßn m√¥ h√¨nh + 1 k·∫øt lu·∫≠n).\n\n"
        f"D·ªØ li·ªáu:\n{json.dumps(metrics_snapshot, ensure_ascii=False, indent=2)}"
    )
    
    payload = {
        "model": GROQ_MODEL_NAME,
        "messages": [
            {
                "role": "system",
                "content": "You are a helpful data scientist specializing in recommender systems. Always respond in Markdown.",
            },
            {"role": "user", "content": prompt},
        ],
        "temperature": 0.3,
        "max_tokens": 600,
    }
    
    try:
        response = requests.post(
            GROQ_API_URL,
            headers={
                "Authorization": f"Bearer {api_key}",
                "Content-Type": "application/json",
            },
            json=payload,
            timeout=60,
        )
        response.raise_for_status()
        data = response.json()
        content = (
            data.get("choices", [{}])[0]
            .get("message", {})
            .get("content", "")
            .strip()
        )
        if not content:
            raise ValueError("Groq response empty.")
        return content
    except (requests.RequestException, ValueError, KeyError) as exc:
        return f"**‚ö†Ô∏è Groq l·ªói**: {exc}"


# Test API section
with st.expander("üîç Test API & Xem Response", expanded=False):
    st.subheader("Test API Responses")
    
    test_tabs = st.tabs(["Train API", "Recommend API"])
    
    # Tab 1: Test Train API
    with test_tabs[0]:
        st.markdown("### Test `/train` API Response")
        test_train_cols = st.columns(len(models))
        for col, (label, slug) in zip(test_train_cols, models.items()):
            with col:
                if st.button(f"Test {label} Train", key=f"test_train_{slug}"):
                    with st.spinner(f"ƒêang g·ªçi {label} /train API..."):
                        result = call_api(BASE_URL, f"{slug}/train", payload={"sync": True}, method="post")
                    
                    if result["success"]:
                        st.success(f"‚úÖ {label} Train API Response:")
                        st.json(result["data"])
                        
                        # Store for analysis
                        st.session_state[f"test_train_{slug}"] = result["data"]
                    else:
                        st.error(f"‚ùå L·ªói: {result.get('error', 'Unknown error')}")
                        if result.get("data"):
                            st.json(result["data"])
    
    # Tab 2: Test Recommend API
    with test_tabs[1]:
        st.markdown("### Test `/recommend` API Response")
        test_user_id = st.text_input("User ID (test)", value="690bf0f2d0c3753df0ecbdd6", key="test_user_id")
        test_product_id = st.text_input("Product ID (test)", value="10068", key="test_product_id")
        
        test_recommend_cols = st.columns(len(models))
        for col, (label, slug) in zip(test_recommend_cols, models.items()):
            with col:
                if st.button(f"Test {label} Recommend", key=f"test_recommend_{slug}"):
                    # API expects user_id and current_product_id (not userId and productId)
                    payload = {"user_id": test_user_id, "current_product_id": test_product_id}
                    with st.spinner(f"ƒêang g·ªçi {label} /recommend API..."):
                        result = call_api(BASE_URL, f"{slug}/recommend", payload=payload, method="post")
                    
                    if result["success"]:
                        st.success(f"‚úÖ {label} Recommend API Response:")
                        data = result["data"]
                        
                        # Show evaluation_metrics if available
                        if "evaluation_metrics" in data:
                            st.markdown("**üìä Evaluation Metrics:**")
                            st.json(data["evaluation_metrics"])
                            st.markdown("---")
                            st.markdown("**üì¶ Full Response:**")
                        
                        st.json(data)
                        
                        # Store evaluation metrics for documentation
                        if "evaluation_metrics" in data:
                            eval_metrics = data["evaluation_metrics"]
                            # Update session state with evaluation metrics
                            for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20"]:
                                if key in eval_metrics:
                                    st.session_state[f"{slug}_{key}"] = str(eval_metrics[key])
                            if "inference_time" in eval_metrics:
                                st.session_state[f"{slug}_inference_time"] = str(eval_metrics["inference_time"])
                            elif "execution_time" in eval_metrics:
                                # Convert seconds to milliseconds
                                exec_time = eval_metrics["execution_time"]
                                if isinstance(exec_time, (int, float)):
                                    st.session_state[f"{slug}_inference_time"] = str(exec_time * 1000)
                                else:
                                    st.session_state[f"{slug}_inference_time"] = str(exec_time)
                            st.success(f"‚úÖ ƒê√£ c·∫≠p nh·∫≠t evaluation metrics t·ª´ {label} recommend API!")
                    else:
                        st.error(f"‚ùå L·ªói: {result.get('error', 'Unknown error')}")
                        if result.get("data"):
                            st.json(result["data"])

st.markdown("---")

# Create tabs for each model
doc_tabs = st.tabs([
    "üìä GNN (LightGCN)", 
    "üìù Content-based Filtering", 
    "üîÄ Hybrid GNN+CBF", 
    "üìà So s√°nh 3 m√¥ h√¨nh",
    "üîç Ph√¢n t√≠ch Chi ti·∫øt Metrics",
    "üßÆ Gi·∫£i th√≠ch Thu·∫≠t to√°n",
    "üëî Personalized vs Outfit"
])

# Tab 1: GNN Documentation
with doc_tabs[0]:
    st.markdown("### 2.3.1. GNN (Graph Neural Network - LightGCN)")
    
    # Get metrics from training results or session state
    gnn_metrics = extract_training_metrics(
        st.session_state.training_results.get("gnn"), 
        "gnn"
    )
    
    # Get values from session state if available (auto-filled from API)
    def get_value(key: str, default: str) -> str:
        session_key = f"gnn_{key}"
        if session_key in st.session_state:
            return str(st.session_state[session_key])
        return default
    
    def get_test_size() -> float:
        if "gnn_test_size" in st.session_state:
            return st.session_state["gnn_test_size"]
        return gnn_metrics['test_size']
    
    # Display metrics (read-only display, auto-filled from API)
    st.subheader("Th√¥ng s·ªë hu·∫•n luy·ªán (t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ API)")
    
    # Show status if data is available
    if st.session_state.training_results.get("gnn"):
        st.info("‚úÖ S·ªë li·ªáu ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ k·∫øt qu·∫£ training API")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ API. Vui l√≤ng train m√¥ h√¨nh GNN tr∆∞·ªõc.")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        num_users = get_value("num_users", str(gnn_metrics['num_users']))
        num_products = get_value("num_products", str(gnn_metrics['num_products']))
        st.metric("S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng train", num_users)
        st.metric("S·ªë l∆∞·ª£ng s·∫£n ph·∫©m train", num_products)
    with col2:
        num_interactions = get_value("num_interactions", str(gnn_metrics['num_interactions']))
        num_training_samples = get_value("num_samples", str(gnn_metrics['num_training_samples']))
        st.metric("S·ªë l∆∞·ª£ng t∆∞∆°ng t√°c", num_interactions)
        st.metric("S·ªë l∆∞·ª£ng training samples (BPR)", num_training_samples)
    with col3:
        epochs = get_value("epochs", str(gnn_metrics['epochs']))
        batch_size = get_value("batch", str(gnn_metrics['batch_size']))
        st.metric("Epochs", epochs)
        st.metric("Batch size", batch_size)
    
    col4, col5 = st.columns(2)
    with col4:
        embed_dim = get_value("embed", str(gnn_metrics['embed_dim']))
        learning_rate = get_value("lr", str(gnn_metrics['learning_rate']))
        st.metric("Embedding dimension", embed_dim)
        st.metric("Learning rate", learning_rate)
    with col5:
        test_size = get_test_size()
        st.metric("Test size", test_size)
    
    st.subheader("Ch·ªâ s·ªë ƒë√°nh gi√° (t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ API /recommend)")
    st.caption("üí° **L∆∞u √Ω**: C√°c ch·ªâ s·ªë n√†y l·∫•y t·ª´ `evaluation_metrics` trong response c·ªßa API `/recommend`. Vui l√≤ng g·ªçi API recommend ƒë·ªÉ c√≥ s·ªë li·ªáu ƒë√°nh gi√°.")
    
    # Check if we have recommendation results
    has_recommend_data = st.session_state.recommendation_results.get("gnn") is not None
    if has_recommend_data:
        st.info("‚úÖ ƒê√£ c√≥ d·ªØ li·ªáu t·ª´ API /recommend")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ API /recommend. Vui l√≤ng g·ªçi API recommend ·ªü section 3 ƒë·ªÉ l·∫•y evaluation metrics.")
    
    eval_col1, eval_col2, eval_col3 = st.columns(3)
    with eval_col1:
        recall_at_10 = format_metric_value(get_value("recall_at_10", "N/A"))
        recall_at_20 = format_metric_value(get_value("recall_at_20", "N/A"))
        st.metric("Recall@10", recall_at_10)
        st.metric("Recall@20", recall_at_20)
    with eval_col2:
        ndcg_at_10 = get_value("ndcg_at_10", "N/A")
        ndcg_at_20 = get_value("ndcg_at_20", "N/A")
        st.metric("NDCG@10", ndcg_at_10)
        st.metric("NDCG@20", ndcg_at_20)
    with eval_col3:
        training_time = format_metric_value(get_value("training_time", "N/A"))
        inference_time = get_value("inference_time", "N/A")
        st.metric("Th·ªùi gian train", training_time)
        st.metric("Th·ªùi gian inference/user", f"{inference_time} ms" if inference_time != "N/A" else "N/A")
    
    # Update metrics with current input values
    gnn_metrics_updated = {
        'num_users': num_users,
        'num_products': num_products,
        'num_interactions': num_interactions,
        'num_training_samples': num_training_samples,
        'epochs': epochs,
        'batch_size': batch_size,
        'embed_dim': embed_dim,
        'learning_rate': learning_rate,
        'test_size': test_size,
        'recall_at_10': recall_at_10,
        'recall_at_20': recall_at_20,
        'ndcg_at_10': ndcg_at_10,
        'ndcg_at_20': ndcg_at_20,
        'training_time': training_time,
        'inference_time': inference_time,
    }
    gnn_metrics_updated = apply_precision_formatting(gnn_metrics_updated)
    
    # Generate and display documentation
    gnn_doc = generate_gnn_documentation(gnn_metrics_updated)
    
    st.markdown("---")
    st.subheader("üìÑ N·ªôi dung t√†i li·ªáu (c√≥ th·ªÉ copy)")
    st.markdown(gnn_doc)
    
    # Copy button
    st.code(gnn_doc, language="markdown")

# Tab 2: CBF Documentation
with doc_tabs[1]:
    st.markdown("### 2.3.2. Content-based Filtering")
    
    # Get metrics from training results or session state
    cbf_metrics = extract_training_metrics(
        st.session_state.training_results.get("cbf"), 
        "cbf"
    )
    
    # Get values from session state if available (auto-filled from API)
    def get_value(key: str, default: str) -> str:
        session_key = f"cbf_{key}"
        if session_key in st.session_state:
            return str(st.session_state[session_key])
        return default
    
    def get_test_size() -> float:
        if "cbf_test_size" in st.session_state:
            return st.session_state["cbf_test_size"]
        return cbf_metrics['test_size']
    
    # Display metrics (read-only display, auto-filled from API)
    st.subheader("Th√¥ng s·ªë hu·∫•n luy·ªán (t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ API)")
    
    # Show status if data is available
    if st.session_state.training_results.get("cbf"):
        st.info("‚úÖ S·ªë li·ªáu ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ k·∫øt qu·∫£ training API")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ API. Vui l√≤ng train m√¥ h√¨nh CBF tr∆∞·ªõc.")
    
    col1, col2 = st.columns(2)
    with col1:
        num_products = get_value("num_products", str(cbf_metrics['num_products']))
        num_users = get_value("num_users", str(cbf_metrics['num_users']))
        st.metric("S·ªë l∆∞·ª£ng s·∫£n ph·∫©m train", num_products)
        st.metric("S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng test", num_users)
    with col2:
        embed_dim = get_value("embed", str(cbf_metrics['embed_dim']))
        test_size = get_test_size()
        st.metric("Embedding dimension", embed_dim)
        st.metric("Test size", test_size)
    
    st.subheader("Ch·ªâ s·ªë ƒë√°nh gi√° (t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ API /recommend)")
    st.caption("üí° **L∆∞u √Ω**: C√°c ch·ªâ s·ªë n√†y l·∫•y t·ª´ `evaluation_metrics` trong response c·ªßa API `/recommend`. Vui l√≤ng g·ªçi API recommend ƒë·ªÉ c√≥ s·ªë li·ªáu ƒë√°nh gi√°.")
    
    # Check if we have recommendation results
    has_recommend_data = st.session_state.recommendation_results.get("cbf") is not None
    if has_recommend_data:
        st.info("‚úÖ ƒê√£ c√≥ d·ªØ li·ªáu t·ª´ API /recommend")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ API /recommend. Vui l√≤ng g·ªçi API recommend ·ªü section 3 ƒë·ªÉ l·∫•y evaluation metrics.")
    
    eval_col1, eval_col2, eval_col3 = st.columns(3)
    with eval_col1:
        recall_at_10 = format_metric_value(get_value("recall_at_10", "N/A"))
        recall_at_20 = format_metric_value(get_value("recall_at_20", "N/A"))
        st.metric("Recall@10", recall_at_10)
        st.metric("Recall@20", recall_at_20)
    with eval_col2:
        ndcg_at_10 = get_value("ndcg_at_10", "N/A")
        ndcg_at_20 = get_value("ndcg_at_20", "N/A")
        st.metric("NDCG@10", ndcg_at_10)
        st.metric("NDCG@20", ndcg_at_20)
    with eval_col3:
        training_time = format_metric_value(get_value("training_time", "N/A"))
        inference_time = get_value("inference_time", "N/A")
        st.metric("Th·ªùi gian train", training_time)
        st.metric("Th·ªùi gian inference/user", f"{inference_time} ms" if inference_time != "N/A" else "N/A")
    
    # Update metrics with current input values
    cbf_metrics_updated = {
        'num_products': num_products,
        'num_users': num_users,
        'embed_dim': embed_dim,
        'test_size': test_size,
        'recall_at_10': recall_at_10,
        'recall_at_20': recall_at_20,
        'ndcg_at_10': ndcg_at_10,
        'ndcg_at_20': ndcg_at_20,
        'training_time': training_time,
        'inference_time': inference_time,
    }
    cbf_metrics_updated = apply_precision_formatting(cbf_metrics_updated)
    
    # Generate and display documentation
    cbf_doc = generate_cbf_documentation(cbf_metrics_updated)
    
    st.markdown("---")
    st.subheader("üìÑ N·ªôi dung t√†i li·ªáu (c√≥ th·ªÉ copy)")
    st.markdown(cbf_doc)
    
    # Copy button
    st.code(cbf_doc, language="markdown")

# Tab 3: Hybrid Documentation
with doc_tabs[2]:
    st.markdown("### 2.3.3. Hybrid GNN (LightGCN) & Content-based Filtering")
    
    # Get metrics from training results or session state
    hybrid_metrics = extract_training_metrics(
        st.session_state.training_results.get("hybrid"), 
        "hybrid"
    )
    
    # Get values from session state if available (auto-filled from API)
    def get_value(key: str, default: str) -> str:
        session_key = f"hybrid_{key}"
        if session_key in st.session_state:
            return str(st.session_state[session_key])
        return default
    
    def get_test_size() -> float:
        if "hybrid_test_size" in st.session_state:
            return st.session_state["hybrid_test_size"]
        return hybrid_metrics['test_size']
    
    # Alpha parameter (can be from API or default)
    if "hybrid_alpha" in st.session_state:
        default_alpha = st.session_state["hybrid_alpha"]
    else:
        default_alpha = 0.7
    alpha = st.slider("Tr·ªçng s·ªë alpha (GNN weight)", min_value=0.0, max_value=1.0, value=default_alpha, step=0.1, key="hybrid_alpha")
    
    # Display metrics (read-only display, auto-filled from API)
    st.subheader("Th√¥ng s·ªë hu·∫•n luy·ªán (t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ API)")
    
    # Show status if data is available
    if st.session_state.training_results.get("hybrid"):
        st.info("‚úÖ S·ªë li·ªáu ƒë√£ ƒë∆∞·ª£c t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ k·∫øt qu·∫£ training API")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ API. Vui l√≤ng train m√¥ h√¨nh Hybrid tr∆∞·ªõc.")
    
    col1, col2 = st.columns(2)
    with col1:
        num_users = get_value("num_users", str(hybrid_metrics['num_users']))
        num_products = get_value("num_products", str(hybrid_metrics['num_products']))
        st.metric("S·ªë l∆∞·ª£ng ng∆∞·ªùi d√πng train", num_users)
        st.metric("S·ªë l∆∞·ª£ng s·∫£n ph·∫©m train", num_products)
    with col2:
        num_interactions = get_value("num_interactions", str(hybrid_metrics['num_interactions']))
        embed_dim = get_value("embed", str(hybrid_metrics['embed_dim']))
        st.metric("S·ªë l∆∞·ª£ng t∆∞∆°ng t√°c", num_interactions)
        st.metric("Embedding dimension", embed_dim)
    
    test_size = get_test_size()
    st.metric("Test size", test_size)
    
    st.subheader("Ch·ªâ s·ªë ƒë√°nh gi√° (t·ª± ƒë·ªông ƒëi·ªÅn t·ª´ API /recommend)")
    st.caption("üí° **L∆∞u √Ω**: C√°c ch·ªâ s·ªë n√†y l·∫•y t·ª´ `evaluation_metrics` trong response c·ªßa API `/recommend`. Vui l√≤ng g·ªçi API recommend ƒë·ªÉ c√≥ s·ªë li·ªáu ƒë√°nh gi√°.")
    
    # Check if we have recommendation results
    has_recommend_data = st.session_state.recommendation_results.get("hybrid") is not None
    if has_recommend_data:
        st.info("‚úÖ ƒê√£ c√≥ d·ªØ li·ªáu t·ª´ API /recommend")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ d·ªØ li·ªáu t·ª´ API /recommend. Vui l√≤ng g·ªçi API recommend ·ªü section 3 ƒë·ªÉ l·∫•y evaluation metrics.")
    
    eval_col1, eval_col2, eval_col3 = st.columns(3)
    with eval_col1:
        recall_at_10 = format_metric_value(get_value("recall_at_10", "N/A"))
        recall_at_20 = format_metric_value(get_value("recall_at_20", "N/A"))
        st.metric("Recall@10", recall_at_10)
        st.metric("Recall@20", recall_at_20)
    with eval_col2:
        ndcg_at_10 = get_value("ndcg_at_10", "N/A")
        ndcg_at_20 = get_value("ndcg_at_20", "N/A")
        st.metric("NDCG@10", ndcg_at_10)
        st.metric("NDCG@20", ndcg_at_20)
    with eval_col3:
        training_time = format_metric_value(get_value("training_time", "N/A"))
        inference_time = get_value("inference_time", "N/A")
        st.metric("Th·ªùi gian train", training_time)
        st.metric("Th·ªùi gian inference/user", f"{inference_time} ms" if inference_time != "N/A" else "N/A")
    
    # Update metrics with current input values
    hybrid_metrics_updated = {
        'num_users': num_users,
        'num_products': num_products,
        'num_interactions': num_interactions,
        'embed_dim': embed_dim,
        'test_size': test_size,
        'recall_at_10': recall_at_10,
        'recall_at_20': recall_at_20,
        'ndcg_at_10': ndcg_at_10,
        'ndcg_at_20': ndcg_at_20,
        'training_time': training_time,
        'inference_time': inference_time,
    }
    hybrid_metrics_updated = apply_precision_formatting(hybrid_metrics_updated)
    
    # Generate and display documentation
    hybrid_doc = generate_hybrid_documentation(hybrid_metrics_updated, alpha)
    
    st.markdown("---")
    st.subheader("üìÑ N·ªôi dung t√†i li·ªáu (c√≥ th·ªÉ copy)")
    st.markdown(hybrid_doc)
    
    # Copy button
    st.code(hybrid_doc, language="markdown")

# Tab 4: Comparison
with doc_tabs[3]:
    st.markdown("### So s√°nh 3 m√¥ h√¨nh")
    
    st.info("üí° **L∆∞u √Ω**: S·ªë li·ªáu s·∫Ω t·ª± ƒë·ªông ƒë∆∞·ª£c ƒëi·ªÅn sau khi train c√°c m√¥ h√¨nh qua API. Vui l√≤ng train c√°c m√¥ h√¨nh tr∆∞·ªõc khi xem b·∫£ng so s√°nh.")
    
    # Get all metrics from session state (will be updated by the input fields in other tabs)
    gnn_metrics_final = extract_training_metrics(st.session_state.training_results.get("gnn"), "gnn")
    cbf_metrics_final = extract_training_metrics(st.session_state.training_results.get("cbf"), "cbf")
    hybrid_metrics_final = extract_training_metrics(st.session_state.training_results.get("hybrid"), "hybrid")
    
    # Get values from session state (auto-filled from API)
    def update_metrics_from_session(metrics_dict: Dict[str, Any], prefix: str) -> None:
        """Update metrics from session state with proper key mapping."""
        for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20", 
                   "training_time", "inference_time",
                   "num_users", "num_products", "num_interactions", 
                   "epochs", "embed_dim", "learning_rate"]:
            session_key = f"{prefix}_{key}"
            if session_key in st.session_state:
                metrics_dict[key] = st.session_state[session_key]
        
        # Handle special mappings
        if f"{prefix}_num_samples" in st.session_state:
            metrics_dict["num_training_samples"] = st.session_state[f"{prefix}_num_samples"]
        if f"{prefix}_batch" in st.session_state:
            metrics_dict["batch_size"] = st.session_state[f"{prefix}_batch"]
        if f"{prefix}_embed" in st.session_state:
            metrics_dict["embed_dim"] = st.session_state[f"{prefix}_embed"]
        if f"{prefix}_lr" in st.session_state:
            metrics_dict["learning_rate"] = st.session_state[f"{prefix}_lr"]
    
    update_metrics_from_session(gnn_metrics_final, "gnn")
    update_metrics_from_session(cbf_metrics_final, "cbf")
    update_metrics_from_session(hybrid_metrics_final, "hybrid")
    gnn_metrics_final = apply_precision_formatting(gnn_metrics_final)
    cbf_metrics_final = apply_precision_formatting(cbf_metrics_final)
    hybrid_metrics_final = apply_precision_formatting(hybrid_metrics_final)
    
    # Also get alpha for hybrid
    if "hybrid_alpha" in st.session_state:
        alpha_final = st.session_state["hybrid_alpha"]
    else:
        alpha_final = 0.7
    
    # Generate Groq-backed analysis text
    with st.spinner("ü§ñ ƒêang nh·ªù Groq ph√¢n t√≠ch s·ªë li·ªáu..."):
        groq_analysis_text = analyze_models_with_groq(
            gnn_metrics_final,
            cbf_metrics_final,
            hybrid_metrics_final,
        )
    
    # Generate comparison table
    comparison_doc = generate_comparison_table(
        gnn_metrics_final,
        cbf_metrics_final,
        hybrid_metrics_final,
        groq_analysis_text or "**‚ö†Ô∏è Groq kh√¥ng tr·∫£ v·ªÅ d·ªØ li·ªáu ƒë·ªÉ ph√¢n t√≠ch.**",
    )
    st.markdown(comparison_doc)
    
    # Copy button
    st.code(comparison_doc, language="markdown")
    
    st.subheader("ü§ñ Ph√¢n t√≠ch & l·ª±a ch·ªçn (Groq)")
    st.markdown(groq_analysis_text)

# Tab 5: Detailed Metrics Analysis
with doc_tabs[4]:
    st.markdown("### üîç Ph√¢n t√≠ch Chi ti·∫øt Metrics")
    st.info("Ph·∫ßn n√†y s·ª≠ d·ª•ng Groq AI ƒë·ªÉ gi·∫£i th√≠ch r·∫•t chi ti·∫øt c√°c ch·ªâ s·ªë Recall, NDCG, th·ªùi gian train/inference v√† ƒë∆∞a ra khuy·∫øn ngh·ªã ch·ªçn m√¥ h√¨nh t·ªët nh·∫•t d·ª±a tr√™n s·ªë li·ªáu th·ª±c nghi·ªám.")

    # Gather metrics for analysis
    gnn_metrics_analysis = extract_training_metrics(st.session_state.training_results.get("gnn"), "gnn")
    cbf_metrics_analysis = extract_training_metrics(st.session_state.training_results.get("cbf"), "cbf")
    hybrid_metrics_analysis = extract_training_metrics(st.session_state.training_results.get("hybrid"), "hybrid")

    def _update_from_session(metrics_dict: Dict[str, Any], prefix: str) -> None:
        for key in ["recall_at_10", "recall_at_20", "ndcg_at_10", "ndcg_at_20", "training_time", "inference_time",
                    "num_users", "num_products", "num_interactions", "epochs", "embed_dim", "learning_rate"]:
            session_key = f"{prefix}_{key}"
            if session_key in st.session_state:
                metrics_dict[key] = st.session_state[session_key]
        if f"{prefix}_num_samples" in st.session_state:
            metrics_dict["num_training_samples"] = st.session_state[f"{prefix}_num_samples"]
        if f"{prefix}_batch" in st.session_state:
            metrics_dict["batch_size"] = st.session_state[f"{prefix}_batch"]
        if f"{prefix}_embed" in st.session_state:
            metrics_dict["embed_dim"] = st.session_state[f"{prefix}_embed"]
        if f"{prefix}_lr" in st.session_state:
            metrics_dict["learning_rate"] = st.session_state[f"{prefix}_lr"]

    _update_from_session(gnn_metrics_analysis, "gnn")
    _update_from_session(cbf_metrics_analysis, "cbf")
    _update_from_session(hybrid_metrics_analysis, "hybrid")

    if st.button("üöÄ Ph√¢n t√≠ch Chi ti·∫øt v·ªõi Groq", key="btn_detailed_metrics"):
        with st.spinner("‚è≥ ƒêang g·ªçi Groq ƒë·ªÉ ph√¢n t√≠ch chi ti·∫øt..."):
            detailed_text = analyze_metrics_detailed(
                gnn_metrics_analysis,
                cbf_metrics_analysis,
                hybrid_metrics_analysis,
            )
        st.markdown("---")
        st.markdown(detailed_text)
        st.code(detailed_text, language="markdown")

# Tab 6: Algorithm Explanation
with doc_tabs[5]:
    st.markdown("### üßÆ Gi·∫£i th√≠ch Thu·∫≠t to√°n (c√≥ c√¥ng th·ª©c)")
    st.info("Ph·∫ßn n√†y s·ª≠ d·ª•ng Groq AI ƒë·ªÉ tr√¨nh b√†y thu·∫≠t to√°n GNN, CBF v√† Hybrid v·ªõi c√¥ng th·ª©c chi ti·∫øt, gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc t√≠nh to√°n.")

    with st.expander("Thi·∫øt l·∫≠p th∆∞ vi·ªán c√¥ng th·ª©c to√°n h·ªçc (t√πy ch·ªçn)"):
        st.markdown("- Streamlit h·ªó tr·ª£ hi·ªÉn th·ªã c√¥ng th·ª©c LaTeX qua st.markdown/st.latex, kh√¥ng c·∫ßn c√†i th√™m.")
        st.markdown("- N·∫øu mu·ªën t√≠nh to√°n bi·ªÉu th·ª©c v√† render c√¥ng th·ª©c t·ª± ƒë·ªông, c√≥ th·ªÉ d√πng SymPy:")
        st.code("""
# K√≠ch ho·∫°t m√¥i tr∆∞·ªùng ·∫£o (ch·ªçn m·ªôt trong c√°c l·ªánh ph√π h·ª£p h·ªá ƒëi·ªÅu h√†nh)
# macOS/Linux (bash/zsh)
source .venv/bin/activate
# Windows PowerShell
.venv\\Scripts\\Activate.ps1

# C√†i ƒë·∫∑t th∆∞ vi·ªán
pip install sympy
""", language="bash")
        st.markdown("V√≠ d·ª• d√πng SymPy ƒë·ªÉ t√≠nh v√† render c√¥ng th·ª©c:")
        st.code("""
import sympy as sp
x, y = sp.symbols('x y')
expr = (x + y)**3
expanded = sp.expand(expr)
latex_str = sp.latex(expanded)  # Chuy·ªÉn sang LaTeX ƒë·ªÉ hi·ªÉn th·ªã
st.latex(latex_str)
""", language="python")

    gnn_metrics_algo = extract_training_metrics(st.session_state.training_results.get("gnn"), "gnn")
    cbf_metrics_algo = extract_training_metrics(st.session_state.training_results.get("cbf"), "cbf")
    hybrid_metrics_algo = extract_training_metrics(st.session_state.training_results.get("hybrid"), "hybrid")

    _update_from_session(gnn_metrics_algo, "gnn")
    _update_from_session(cbf_metrics_algo, "cbf")
    _update_from_session(hybrid_metrics_algo, "hybrid")

    if st.button("üöÄ Gi·∫£i th√≠ch Thu·∫≠t to√°n v·ªõi Groq", key="btn_algo_explain"):
        with st.spinner("‚è≥ ƒêang g·ªçi Groq ƒë·ªÉ gi·∫£i th√≠ch thu·∫≠t to√°n..."):
            algo_text = explain_algorithms_detailed(
                gnn_metrics_algo,
                cbf_metrics_algo,
                hybrid_metrics_algo,
            )
        st.markdown("---")
        st.markdown(algo_text)
        st.code(algo_text, language="markdown")

# Tab 7: Personalized vs Outfit
with doc_tabs[6]:
    st.markdown("### üëî Personalized vs Outfit Recommendation")
    st.info("Gi·∫£i th√≠ch ti√™u chu·∫©n Personalized (c√° nh√¢n h√≥a) v√† Outfit (ph·ªëi ƒë·ªì), c√°ch t·ªï ch·ª©c d·ªØ li·ªáu v√† c√¥ng th·ª©c t√≠nh ƒëi·ªÉm g·ª£i √Ω.")

    gnn_metrics_pf = extract_training_metrics(st.session_state.training_results.get("gnn"), "gnn")
    cbf_metrics_pf = extract_training_metrics(st.session_state.training_results.get("cbf"), "cbf")
    hybrid_metrics_pf = extract_training_metrics(st.session_state.training_results.get("hybrid"), "hybrid")

    _update_from_session(gnn_metrics_pf, "gnn")
    _update_from_session(cbf_metrics_pf, "cbf")
    _update_from_session(hybrid_metrics_pf, "hybrid")

    if st.button("üöÄ Ph√¢n t√≠ch Personalized vs Outfit (Groq)", key="btn_pf_outfit"):
        with st.spinner("‚è≥ ƒêang g·ªçi Groq ƒë·ªÉ ph√¢n t√≠ch Personalized vs Outfit..."):
            pf_text = explain_personalized_vs_outfit(
                gnn_metrics_pf,
                cbf_metrics_pf,
                hybrid_metrics_pf,
            )
        st.markdown("---")
        st.markdown(pf_text)
        st.code(pf_text, language="markdown")